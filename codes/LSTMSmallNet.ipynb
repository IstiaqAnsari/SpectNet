{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "# import tensorflow as tf\n",
    "# from keras.backend.tensorflow_backend import set_session\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "# set_session(tf.Session(config=config))\n",
    "# from clr_callback import CyclicLR\n",
    "# import dill\n",
    "from BalancedDannAudioDataGenerator import BalancedAudioDataGenerator\n",
    "import os,time\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "import math\n",
    "import pandas as pd\n",
    "import tables,h5py\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "from keras.callbacks import TensorBoard, Callback, ReduceLROnPlateau\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, CSVLogger\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "from Heartnet import heartnet,getAttentionModel\n",
    "from utils import log_macc, results_log\n",
    "from dataLoader import reshape_folds\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import Evaluator\n",
    "import dataLoader\n",
    "from custom_layers import Attention\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set()\n",
    "import tensorflow as tf\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_layers import Conv1D_zerophase_linear, Conv1D_linearphase, Conv1D_zerophase,\\\n",
    "    DCT1D, Conv1D_gammatone, Conv1D_linearphaseType, Attention\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Dense, Dropout,\\\n",
    "        Reshape,Flatten, Activation, AveragePooling1D,Add,LSTM\n",
    "from keras import initializers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Lambda\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import max_norm\n",
    "from keras.optimizers import Adam, SGD # Nadam, Adamax\n",
    "import numpy as np\n",
    "import tables,h5py\n",
    "from Gradient_Reverse_Layer import GradientReversal\n",
    "from ResultAnalyser import Result\n",
    "from utils import Confused_Crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def branch(input_tensor,num_filt,kernel_size,random_seed,padding,bias,maxnorm,l2_reg,\n",
    "           eps,bn_momentum,activation_function,dropout_rate,subsam,trainable):\n",
    "\n",
    "    num_filt1, num_filt2 = num_filt\n",
    "    t = Conv1D(num_filt1, kernel_size=kernel_size,\n",
    "                kernel_initializer=initializers.he_normal(seed=random_seed),\n",
    "                padding=padding,\n",
    "                use_bias=bias,\n",
    "                kernel_constraint=max_norm(maxnorm),\n",
    "                trainable=trainable,\n",
    "                kernel_regularizer=l2(l2_reg))(input_tensor)\n",
    "    t = BatchNormalization(epsilon=eps, momentum=bn_momentum, axis=-1)(t)\n",
    "    t = Activation(activation_function)(t)\n",
    "    t = Dropout(rate=dropout_rate, seed=random_seed)(t)\n",
    "    t = Conv1D(16, kernel_size=kernel_size,\n",
    "               kernel_initializer=initializers.he_normal(seed=random_seed),\n",
    "               padding=padding,\n",
    "               use_bias=bias,\n",
    "               trainable=trainable,\n",
    "               kernel_constraint=max_norm(maxnorm),\n",
    "               kernel_regularizer=l2(l2_reg))(t)\n",
    "    t = BatchNormalization(epsilon=eps, momentum=bn_momentum, axis=-1)(t)\n",
    "    t = Activation(activation_function)(t)\n",
    "    t = Dropout(rate=dropout_rate, seed=random_seed)(t)\n",
    "    # t = Flatten()(t)\n",
    "    return t\n",
    "def zeropad(x):\n",
    "    y = K.zeros_like(x)\n",
    "    return K.concatenate([x, y], axis=2)\n",
    "\n",
    "def zeropad_output_shape(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3\n",
    "    shape[2] *= 2\n",
    "    return tuple(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filt = (8, 4)\n",
    "num_dense = 20\n",
    "lr = 0.01\n",
    "bn_momentum = 0.99\n",
    "eps = 1.1e-5\n",
    "bias = False\n",
    "l2_reg = 0.04864911065093751\n",
    "l2_reg_dense = 0.\n",
    "kernel_size = 5\n",
    "maxnorm = 10000.\n",
    "dropout_rate = 0.5\n",
    "dropout_rate_dense = 0.\n",
    "padding = 'valid'\n",
    "activation_function = 'relu'\n",
    "subsam = 2\n",
    "FIR_train= True\n",
    "trainable = True\n",
    "hp_lambda = np.float32(0)\n",
    "lr_decay =0.0001132885\n",
    "random_seed = 1\n",
    "num_class =2\n",
    "num_class_domain = 18\n",
    "tipe= 1\n",
    "decision = 'majority' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_block(input_tensor,num_filt,kernel_size,stride,padding,random_seed,bias,maxnorm,l2_reg,\n",
    "           eps,bn_momentum,activation_function,dropout_rate,subsam,trainable,cat=True):\n",
    "\n",
    "    t = Conv1D(num_filt, kernel_size=kernel_size,\n",
    "                kernel_initializer=initializers.he_normal(seed=random_seed),\n",
    "                padding=padding,\n",
    "                strides=stride,\n",
    "                use_bias=bias,\n",
    "                kernel_constraint=max_norm(maxnorm),\n",
    "                trainable=trainable,\n",
    "                kernel_regularizer=l2(l2_reg))(input_tensor)\n",
    "    t = BatchNormalization(epsilon=eps, momentum=bn_momentum, axis=-1)(t)\n",
    "    t = Activation(activation_function)(t)\n",
    "    t = Dropout(rate=dropout_rate, seed=random_seed)(t)\n",
    "    t = Conv1D(num_filt, kernel_size=kernel_size,\n",
    "                kernel_initializer=initializers.he_normal(seed=random_seed),\n",
    "                padding=padding,\n",
    "                strides=1,\n",
    "                use_bias=bias,\n",
    "                kernel_constraint=max_norm(maxnorm),\n",
    "                trainable=trainable,\n",
    "                kernel_regularizer=l2(l2_reg))(t)\n",
    "    t = BatchNormalization(epsilon=eps, momentum=bn_momentum, axis=-1)(t)\n",
    "    t = Activation(activation_function)(t)\n",
    "    t = Dropout(rate=dropout_rate, seed=random_seed)(t)\n",
    "    \n",
    "    p = MaxPooling1D(pool_size=stride)(input_tensor)\n",
    "    if(stride>1):\n",
    "        if(cat):\n",
    "            p = Lambda(zeropad, output_shape=zeropad_output_shape)(p)\n",
    "    \n",
    "    t = Add()([t,p])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heartnet(load_path,activation_function='relu', bn_momentum=0.99, bias=False, dropout_rate=0.5, dropout_rate_dense=0.0,\n",
    "             eps=1.1e-5, kernel_size=5, l2_reg=0.0, l2_reg_dense=0.0,lr=0.0012843784, lr_decay=0.0001132885, maxnorm=10000.,\n",
    "             padding='valid', random_seed=1, subsam=2, num_filt=(8, 4), num_dense=20,FIR_train=False,trainable=True,type=1,\n",
    "             num_class=2, num_class_domain=1,hp_lambda=0,batch_size=1024,optim='SGD'):\n",
    "    \n",
    "    #num_dense = 20 default \n",
    "    input = Input(shape=(2500, 1))\n",
    "\n",
    "    \n",
    "    xx = branch(input,num_filt,kernel_size,random_seed,padding,bias,maxnorm,l2_reg,\n",
    "           eps,bn_momentum,activation_function,dropout_rate,subsam,trainable)\n",
    "    xx = branch(input,num_filt,kernel_size,random_seed,padding,bias,maxnorm,l2_reg,\n",
    "           eps,bn_momentum,activation_function,dropout_rate,subsam,trainable)\n",
    "    xx = res_block(xx,32,kernel_size,2,'same',random_seed,bias,maxnorm,l2_reg,\n",
    "           eps,bn_momentum,activation_function,dropout_rate,subsam,trainable)\n",
    "    xx = res_block(xx,64,kernel_size,2,'same',random_seed,bias,maxnorm,l2_reg,\n",
    "           eps,bn_momentum,activation_function,dropout_rate,subsam,trainable)\n",
    "    \n",
    "    xx = MaxPooling1D(pool_size=2)(xx)\n",
    "    \n",
    "#     xx = res_block(xx,128,kernel_size,2,'same',random_seed,bias,maxnorm,l2_reg,\n",
    "#            eps,bn_momentum,activation_function,dropout_rate,subsam,trainable)\n",
    "#     xx = res_block(xx,128,kernel_size,1,'same',random_seed,bias,maxnorm,l2_reg,\n",
    "#            eps,bn_momentum,activation_function,dropout_rate,subsam,trainable)\n",
    "    \n",
    "#     xx = res_block(xx,128,kernel_size,2,'same',random_seed,bias,maxnorm,l2_reg,\n",
    "#            eps,bn_momentum,activation_function,dropout_rate,subsam,trainable,cat=False)\n",
    "#     xx = res_block(xx,128,kernel_size,2,'same',random_seed,bias,maxnorm,l2_reg,\n",
    "#            eps,bn_momentum,activation_function,dropout_rate,subsam,trainable,cat=False)\n",
    "    \n",
    "    xx = Conv1D(128, kernel_size=kernel_size,\n",
    "                kernel_initializer=initializers.he_normal(seed=random_seed),\n",
    "                padding=padding,\n",
    "                strides=2,\n",
    "                use_bias=bias,\n",
    "                kernel_constraint=max_norm(maxnorm),\n",
    "                trainable=trainable,\n",
    "                kernel_regularizer=l2(l2_reg))(xx)\n",
    "    xx = MaxPooling1D(pool_size=2)(xx)\n",
    "    merged = Flatten()(xx)\n",
    "    \n",
    "    dann_in = GradientReversal(hp_lambda=hp_lambda,name='grl')(merged)\n",
    "    \n",
    "    merged = Dropout(rate=dropout_rate, seed=random_seed)(merged)\n",
    "    merged = Reshape((9856, 1))(merged)\n",
    "    merged = LSTM(20, kernel_regularizer=l2(l2_reg),\n",
    "                  activation='relu',\n",
    "                  dropout=dropout_rate_dense,\n",
    "                  kernel_initializer=initializers.he_normal(seed=random_seed),\n",
    "                  kernel_constraint=max_norm(maxnorm),\n",
    "                  return_sequences=False)(merged)\n",
    "#     merged = LSTM(20,kernel_regularizer=l2(l2_reg),\n",
    "#                   dropout=dropout_rate_dense,\n",
    "#                   kernel_initializer=initializers.he_normal(seed=random_seed),\n",
    "#                   kernel_constraint=max_norm(maxnorm),\n",
    "#                   return_sequences=True)(merged)\n",
    "    merged = Dense(2, activation='softmax', name = \"class\")(merged)\n",
    "    \n",
    "    \n",
    "    dsc = Dense(80,\n",
    "                   activation=activation_function,\n",
    "                   kernel_initializer=initializers.he_normal(seed=random_seed),\n",
    "                   use_bias=bias,\n",
    "                   kernel_constraint=max_norm(maxnorm),\n",
    "                   kernel_regularizer=l2(l2_reg_dense),\n",
    "                   name = 'domain_dense')(dann_in)   \n",
    "    dsc = Dense(num_class_domain, activation='softmax', name = \"domain\")(dsc) \n",
    "    \n",
    "    model = Model(inputs=input, outputs=[merged,dsc])\n",
    "    \n",
    "    if load_path:\n",
    "        model.load_weights(filepath=load_path, by_name=False)\n",
    "    \n",
    "    #if load_path:  # If path for loading model was specified\n",
    "    #model.load_weights(filepath='../../models_dbt_dann/fold_a_gt 2019-09-09 16:53:52.063276/weights.0041-0.6907.hdf5', by_name=True)\n",
    "    # models/fold_a_gt 2019-09-04 17:36:52.860817/weights.0200-0.7135.hdf5\n",
    "    \n",
    "    if optim=='Adam':\n",
    "        opt = Adam(lr=lr, decay=lr_decay)\n",
    "    else:  \n",
    "        opt = SGD(lr=lr,decay=lr_decay)\n",
    "    if(num_class_domain>1):\n",
    "        domain_loss_function = 'categorical_crossentropy'\n",
    "    else:\n",
    "        domain_loss_function = 'binary_crossentropy'\n",
    "    model.compile(optimizer=opt, loss={'class':'categorical_crossentropy','domain':domain_loss_function}, loss_weights=[1,0], metrics=['accuracy'])\n",
    "    #model.compile(optimizer=opt, loss={'class':'categorical_crossentropy','domain':'categorical_crossentropy'}, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = heartnet(None,activation_function, bn_momentum, bias, dropout_rate, dropout_rate_dense,\n",
    "                             eps, kernel_size, l2_reg, l2_reg_dense, lr, lr_decay, maxnorm,\n",
    "                             padding, random_seed, subsam, num_filt, num_dense, FIR_train, trainable, tipe,\n",
    "                             num_class=num_class,num_class_domain=9,hp_lambda=hp_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 2500, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 2496, 8)      40          input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 2496, 8)      32          conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 2496, 8)      0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 2496, 8)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 2492, 16)     640         dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 2492, 16)     64          conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 2492, 16)     0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 2492, 16)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 1246, 32)     2560        dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 1246, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 1246, 32)     0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 1246, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1246, 32)     5120        dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 1246, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1246, 32)     0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 1246, 16)     0           dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 1246, 32)     0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 1246, 32)     0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 1246, 32)     0           dropout_42[0][0]                 \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 623, 64)      10240       add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 623, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 623, 64)      0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 623, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 623, 64)      20480       dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 623, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 623, 64)      0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 623, 32)      0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 623, 64)      0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 623, 64)      0           max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 623, 64)      0           dropout_44[0][0]                 \n",
      "                                                                 lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 311, 64)      0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 154, 128)     40960       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 77, 128)      0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 9856)         0           max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 9856)         0           flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 9856, 1)      0           dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "grl (GradientReversal)          (None, 9856)         0           flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 20)           1760        reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "domain_dense (Dense)            (None, 80)           788480      grl[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "class (Dense)                   (None, 2)            42          lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "domain (Dense)                  (None, 9)            729         domain_dense[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 871,915\n",
      "Trainable params: 871,483\n",
      "Non-trainable params: 432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='LSTMsmallNet.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
