{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "# import tensorflow as tf\n",
    "# from keras.backend.tensorflow_backend import set_session\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "# set_session(tf.Session(config=config))\n",
    "# from clr_callback import CyclicLR\n",
    "# import dill\n",
    "from BalancedDannAudioDataGenerator import BalancedAudioDataGenerator, AudioDataGenerator\n",
    "import os,time\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "import math\n",
    "import pandas as pd\n",
    "import tables,h5py\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "from keras.utils import plot_model\n",
    "# from Heartnet import heartnet,getAttentionModel\n",
    "from collections import Counter\n",
    "from torchviz import make_dot\n",
    "def to_numpy(x):\n",
    "    return x.cpu().detach().numpy()\n",
    "def plotf(x):\n",
    "    plt.plot(to_numpy(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import log_macc, results_log\n",
    "from dataLoader import reshape_folds\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import Evaluator\n",
    "import dataLoader\n",
    "# from custom_layers import Attention\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wow():\n",
    "    def __init__(self):\n",
    "        self.dann = False\n",
    "        self.self = False\n",
    "        self.reduce = None\n",
    "        self.shuffle = 1\n",
    "        self.mfcc = False\n",
    "args = wow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train normal -  60862 - 6466  Abnormal\n",
      "                90  -  9 %\n",
      "Test normal -  4303 - 9864  Abnormal\n",
      "               30  -  69 %\n"
     ]
    }
   ],
   "source": [
    "test_domains = 'a'\n",
    "train_domains = 'cdebf'\n",
    "source_domain = train_domains\n",
    "target_domain = test_domains\n",
    "\n",
    "test_split = 0\n",
    "fold_dir = '../data/all_folds_wav_name/'\n",
    "\n",
    "if(args.self):\n",
    "    print(\"Self training activated\")\n",
    "    x_train, y_train, y_domain, train_parts, x_val, y_val, val_domain, val_parts,val_wav_files = dataLoader.getData(fold_dir,'',test_domains,0.9,shuffle=args.shuffle)\n",
    "    print(x_train.shape, x_val.shape)\n",
    "else:\n",
    "    x_train, y_train, y_domain, train_parts,x_val, y_val, val_domain, val_parts, val_wav_files = dataLoader.getData(fold_dir,train_domains,test_domains,test_split,shuffle = args.shuffle)\n",
    "\n",
    "if(args.reduce):\n",
    "    print(\"Reduction \", args.reduce)\n",
    "    x_train,_,y_train,_,y_domain,_ = train_test_split(x_train.transpose(),y_train,y_domain,stratify=y_train,test_size = args.reduce)\n",
    "    x_train = x_train.transpose()\n",
    "\n",
    "    #x_val,_,y_val,_,val_domain,_ = train_test_split(x_val.transpose(),y_val,val_domain,stratify=y_val,test_size = args.reduce)\n",
    "    #x_val = x_val.transpose()\n",
    "\n",
    "val_files = val_domain\n",
    "#Create meta labels and domain labels\n",
    "\n",
    "if(test_split>0):\n",
    "    source_domain = \"\".join(set(source_domain).union(set(target_domain)))\n",
    "    #domains = domains + test_domains\n",
    "\n",
    "if(args.self):\n",
    "    print(\"self training\")\n",
    "    source_domain = test_domains\n",
    "\n",
    "domains = set(source_domain + target_domain)\n",
    "#num_class_domain = len(set(train_domains + test_domains))\n",
    "num_class_domain = len(domains)\n",
    "num_class = 2\n",
    "\n",
    "domainClass_source = [(cls,dfc) for cls in range(2) for dfc in source_domain]\n",
    "domainClass_target = [(cls,dfc) for cls in range(2) for dfc in target_domain]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load old fold0_noFIR.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "path = '../data/fold0_noFIR.mat'\n",
    "data = h5py.File(path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trainX',\n",
       " 'trainY',\n",
       " 'train_files',\n",
       " 'train_parts',\n",
       " 'valX',\n",
       " 'valY',\n",
       " 'val_files',\n",
       " 'val_parts']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data['trainX'][:].astype('float32')\n",
    "x_train = np.expand_dims(x_train.transpose(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = data['valX'][:].astype('float32')\n",
    "x_val = np.expand_dims(x_val.transpose(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data['trainY'][:].astype('int32')\n",
    "y_train = y_train.transpose()\n",
    "y_train = y_train[:,0]\n",
    "y_train[y_train<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = data['valY'][:].astype('int32')\n",
    "y_val = y_val.transpose()\n",
    "y_val = y_val[:,0]\n",
    "y_val[y_val<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_parts = data['val_parts'][:].astype('int32').squeeze(0)\n",
    "val_files = data['val_files'][:].astype('int32').squeeze(0)\n",
    "train_files = data['train_files'][:].astype('int32').squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = list(Counter(train_files).keys())\n",
    "domainClass = [(cls,dfc) for cls in range(2) for dfc in domains]\n",
    "meta_labels = [domainClass.index((cl,df)) for (cl,df) in zip((y_train),train_files)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mfcc extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Convert to MFCC\n",
    "import python_speech_features as psf\n",
    "from matplotlib import cm\n",
    "if(args.mfcc):\n",
    "    train_mfcc = np.array([(psf.base.mfcc(x,samplerate=1000,winlen=0.05,winstep=0.01)) for x in x_train.transpose()])\n",
    "    val_mfcc = np.array([(psf.base.mfcc(x,samplerate=1000,winlen=0.05,winstep=0.01)) for x in x_val.transpose()])\n",
    "    \n",
    "    train_mfcc = (train_mfcc-np.mean(train_mfcc))/np.std(train_mfcc)\n",
    "    val_mfcc = (val_mfcc-np.mean(val_mfcc))/np.std(val_mfcc)\n",
    "    #train_mfcc = train_mfcc/np.max(np.abs(train_mfcc))\n",
    "    #val_mfcc = val_mfcc/np.max(np.abs(val_mfcc))\n",
    "    \n",
    "    del x_train, x_val\n",
    "    x_train = train_mfcc.copy()\n",
    "    x_val = val_mfcc.copy()\n",
    "    print(x_train.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaped x  (67328, 2500, 1)\n",
      "reshaped x  (14167, 2500, 1)\n",
      "reshaped Y  (67328, 1)\n",
      "reshaped Y  (67328, 1)\n",
      "reshaped Y  (14167, 1)\n",
      "Y domain  Counter({3: 57642, 2: 4119, 5: 2985, 1: 1781, 4: 801})\n",
      "Val domain  Counter({'a': 14167})\n",
      "Source Meta labels  Counter({2: 54790, 4: 3012, 7: 2852, 3: 2396, 5: 1425, 9: 1107, 8: 589, 6: 493, 0: 356, 1: 308})\n",
      "Target Meta labels  Counter()\n",
      "Train files  (67328, 2)   Domain  (67328, 1)\n",
      "Test files  (14167, 2)   Domain  (14167, 6)\n"
     ]
    }
   ],
   "source": [
    "meta_labels_source = [domainClass_source.index((cl,df)) for (cl,df) in zip(y_train,y_domain)]\n",
    "meta_labels_target = None\n",
    "if(args.dann):\n",
    "    meta_labels_target = [domainClass_target.index((cl,df)) for (cl,df) in zip((y_val),(val_domain))]\n",
    "    \n",
    "\n",
    "domains = \"\".join(set(source_domain).union(set(target_domain)))\n",
    "\n",
    "y_domain_source = np.array([list(domains).index(lab) for lab in y_domain])\n",
    "\n",
    "y_domain_target = np.array([list(domains).index(lab) for lab in val_domain])\n",
    "\n",
    "################### Reshaping ############\n",
    "\n",
    "if(args.mfcc):\n",
    "    [], [y_train,y_domain,y_val] = reshape_folds([],[y_train,y_domain_source,y_val])\n",
    "else:\n",
    "    [x_train,x_val], [y_train,y_domain,y_val] = reshape_folds([x_train,x_val],[y_train,y_domain_source,y_val])\n",
    "y_train = to_categorical(y_train, num_classes=num_class)\n",
    "\n",
    "print(\"Y domain \", Counter([x[0] for x in y_domain]))\n",
    "print(\"Val domain \", Counter(val_domain))\n",
    "print(\"Source Meta labels \", Counter(meta_labels_source))\n",
    "print(\"Target Meta labels \", Counter(meta_labels_target))\n",
    "y_domain_source = to_categorical(y_domain_source,num_classes=num_class_domain)\n",
    "\n",
    "y_val = to_categorical(y_val, num_classes=num_class)\n",
    "y_domain_target = to_categorical(y_domain_target,num_classes=num_class_domain)\n",
    "\n",
    "\n",
    "val_domain = y_domain_target\n",
    "print(\"Train files \", y_train.shape, \"  Domain \", y_domain.shape)\n",
    "print(\"Test files \", y_val.shape, \"  Domain \", val_domain.shape)\n",
    "\n",
    "### Batch Size limmiter \n",
    "batch_size = 1000\n",
    "if(batch_size > max(y_train.shape)):\n",
    "    print(\"Batch size if given greater than train files size. limiting batch size\")\n",
    "    batch_size = max(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((67328, 2500, 1),\n",
       " (14167, 2500, 1),\n",
       " (67328, 2),\n",
       " (14167, 2),\n",
       " (67328, 1),\n",
       " (14167, 6),\n",
       " Counter({5: 1425,\n",
       "          0: 356,\n",
       "          1: 308,\n",
       "          6: 493,\n",
       "          2: 54790,\n",
       "          7: 2852,\n",
       "          3: 2396,\n",
       "          8: 589,\n",
       "          4: 3012,\n",
       "          9: 1107}),\n",
       " Counter())"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,x_val.shape, y_train.shape, y_val.shape, y_domain.shape,val_domain.shape,Counter(meta_labels_source),Counter(meta_labels_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.argmax(y_train,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = np.argmax(y_val,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((67328, 1, 2500), (67328,), (14167, 1, 2500), (14167,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,y_train.shape,x_val.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## change 2500 axis for pytorch \n",
    "x_train = x_train.transpose((0,2,1))\n",
    "x_val = x_val.transpose((0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((67328, 1, 2500), (67328, 2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:2100,:,:]\n",
    "y_train = y_train[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_labels = meta_labels_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  356   308 54790  2396  3012  1425   493  2852   589  1107]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "Chunk size selected as 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mhealthra2/Data/heart_sound/Adversarial-Heart-Sound-Classification/codes/BalancedDannAudioDataGenerator.py:841: UserWarning: `meta_labels` specified, will use meta_labels instead of target_label\n",
      "  warnings.warn('`meta_labels` specified, will use meta_labels instead of target_label')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 24\n",
    "datagen_source = BalancedAudioDataGenerator(shift=.1,data_format = 'channels_first')\n",
    "flow_source = datagen_source.flow(x_train, y_train,\n",
    "                meta_label=meta_labels,\n",
    "                batch_size=batch_size, shuffle=True,\n",
    "                seed=1)\n",
    "# datagen_source_balanced = AudioDataGenerator(shift=.1,data_format = 'channels_first')\n",
    "# flow_source = datagen_source_balanced.flow(x_train, y_train,\n",
    "#                 batch_size=batch_size, shuffle=True,\n",
    "#                 seed=1)\n",
    "# datagen_val = BalancedAudioDataGenerator(shift=.1,data_format = 'channels_first')\n",
    "# flow_val = datagen_val.flow(x_val, y_val,\n",
    "#                 meta_label=y_val,\n",
    "#                 batch_size=batch_size, shuffle=True,\n",
    "#                 seed=1)\n",
    "try:\n",
    "    flow_source.steps_per_epoch = len(flow_source)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1, 2500])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 1, 64, 240])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = flow_source.next()\n",
    "x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
    "x = x.type(torch.FloatTensor).cuda()\n",
    "holdx = x\n",
    "print(x.shape)\n",
    "x = mfcc_gen(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1, 64, 1, 240])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hold = x\n",
    "x = x.transpose(2,1)\n",
    "x = x.unsqueeze(1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from torchvision import datasets, transforms\n",
    "from torchsummary import summary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import HeartCepTorch,importlib\n",
    "HeartCepTorch = importlib.reload(HeartCepTorch)\n",
    "from HeartCepTorch import MFCC_Gen,Network,MFCC_Gen_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "from HeartCepTorch import Conv_Gammatone_coeff\n",
    "class Branch(nn.Module):\n",
    "    def __init__(self,c_in, c_out, kernel_size=5,stride=1,dropout = 0.5):\n",
    "        super(Branch, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(c_in, c_out*2, kernel_size=kernel_size,stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(c_out*2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.conv2 = nn.Conv1d(c_out*2, c_out, kernel_size=kernel_size,stride=stride)\n",
    "        self.bn2 = nn.BatchNorm1d(c_out)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "    def forward(self, x):\n",
    "        x = self.drop(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(x)\n",
    "        x = self.drop(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,filters=64,kernel_size=81,fs=1000):\n",
    "        super(Net,self).__init__()\n",
    "        wow = Conv_Gammatone_coeff(1,filters,kernel_size,fs)\n",
    "        self.filterbank = nn.ModuleList()\n",
    "        for i in range(filters):\n",
    "            conv = nn.Conv1d(1,1,kernel_size=kernel_size)\n",
    "            conv.weight = Parameter(wow.weight[i:i+1])\n",
    "            self.filterbank.append(conv)\n",
    "        self.branches = nn.ModuleList()\n",
    "        for i in range(filters):\n",
    "            branch = Branch(1,4)\n",
    "            self.branches.append(branch)\n",
    "        self.dense = nn.Linear(4*602*filters,20)\n",
    "        self.cls = nn.Linear(20,2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.soft = nn.Softmax()\n",
    "    def forward(self,x):\n",
    "        x = [c(x) for c in self.filterbank]\n",
    "        x = [c(xx) for (xx,c) in zip(x,self.branches)]\n",
    "        x = torch.cat(x,dim=1)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.relu(self.dense(x))\n",
    "        x = self.soft(self.cls(x))\n",
    "        return x\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network(2,0)\n",
    "mfcc_gen = MFCC_Gen_coeff(fs=1000,filters=64,momentum=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_gen.gamma.weight = Parameter(torch.from_numpy(bank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/media/mhealthra2/Data/heart_sound/Heartnet_Results/logs/gammatone_torch_layer/fold0_noFIR dbt 64 HNET_2020-05-17 17.56.06.569274/weights/'\n",
    "weights = os.listdir(path)\n",
    "wow = dict(torch.load(path+weights[53]))\n",
    "gamma_weight = torch.cat([wow['filterbank.{}.weight'.format(i)] for i in range(64)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "mfcc_gen.gamma.weight = Parameter(gamma_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7eff518c85d0>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(to_numpy(mfcc_gen.gamma.weight[35]).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 16, 244, 65]             128\n",
      "       BatchNorm2d-2          [-1, 16, 244, 65]              48\n",
      "            Conv2d-3          [-1, 32, 242, 63]           4,672\n",
      "       BatchNorm2d-4          [-1, 32, 242, 63]              96\n",
      "         Dropout2d-5          [-1, 32, 242, 63]               0\n",
      "            Conv2d-6          [-1, 32, 244, 65]           9,280\n",
      "       BatchNorm2d-7          [-1, 32, 244, 65]              96\n",
      "         Dropout2d-8          [-1, 32, 244, 65]               0\n",
      "            Conv2d-9          [-1, 64, 122, 32]          18,560\n",
      "      BatchNorm2d-10          [-1, 64, 122, 32]             192\n",
      "        Dropout2d-11          [-1, 64, 122, 32]               0\n",
      "           Conv2d-12           [-1, 64, 61, 16]          36,992\n",
      "      BatchNorm2d-13           [-1, 64, 61, 16]             192\n",
      "        Dropout2d-14           [-1, 64, 61, 16]               0\n",
      "           Conv2d-15          [-1, 128, 61, 16]          73,984\n",
      "      BatchNorm2d-16          [-1, 128, 61, 16]             384\n",
      "        Dropout2d-17          [-1, 128, 61, 16]               0\n",
      "           Conv2d-18           [-1, 128, 30, 8]         147,712\n",
      "      BatchNorm2d-19           [-1, 128, 30, 8]             384\n",
      "        Dropout2d-20           [-1, 128, 30, 8]               0\n",
      "           Conv2d-21           [-1, 256, 30, 8]         295,424\n",
      "      BatchNorm2d-22           [-1, 256, 30, 8]             768\n",
      "        Dropout2d-23           [-1, 256, 30, 8]               0\n",
      "           Conv2d-24           [-1, 256, 15, 4]         590,336\n",
      "      BatchNorm2d-25           [-1, 256, 15, 4]             768\n",
      "        Dropout2d-26           [-1, 256, 15, 4]               0\n",
      "        Extractor-27                 [-1, 7168]       1,178,064\n",
      "           Linear-28                  [-1, 100]         717,000\n",
      "             ReLU-29                  [-1, 100]               0\n",
      "           Linear-30                    [-1, 2]             204\n",
      "          Softmax-31                    [-1, 2]               0\n",
      " Class_classifier-32                    [-1, 2]         717,102\n",
      "================================================================\n",
      "Total params: 3,792,386\n",
      "Trainable params: 3,792,386\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 39.18\n",
      "Params size (MB): 14.47\n",
      "Estimated Total Size (MB): 53.71\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model.cuda(),(1,240,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "    Conv_Gammatone-1             [-1, 64, 2420]             192\n",
      "       BatchNorm1d-2             [-1, 64, 2420]             192\n",
      "            Conv1d-3              [-1, 64, 240]         102,400\n",
      "       BatchNorm2d-4           [-1, 1, 64, 240]               3\n",
      "================================================================\n",
      "Total params: 102,787\n",
      "Trainable params: 195\n",
      "Non-trainable params: 102,592\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 2.60\n",
      "Params size (MB): 0.39\n",
      "Estimated Total Size (MB): 3.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(mfcc_gen.cuda(),(1,2500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WHOLE_MODEL(nn.Module):\n",
    "    def __init__(self,kernel_size = 81,filters = 26,fs=1000,winlen=0.025,winstep=0.01,dimension=1):\n",
    "        super(WHOLE_MODEL,self).__init__()\n",
    "        self.mfcc = MFCC_Gen_coeff(fs=1000,filters=64,momentum=0.99)\n",
    "        self.classifier = Network(2,0)\n",
    "        for x in self.mfcc.gamma.named_parameters():\n",
    "            x[1].requires_grad = False\n",
    "    def forward(self,x):\n",
    "        x = self.mfcc(x)\n",
    "#         mfcc = x\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.transpose(3,2)\n",
    "        x = self.classifier(x)\n",
    "        return x#,mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WHOLE_MODEL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 81])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.mfcc.gamma.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1             [-1, 64, 2420]           5,312\n",
      "       BatchNorm1d-2             [-1, 64, 2420]             192\n",
      "            Conv1d-3              [-1, 64, 240]         102,400\n",
      "       BatchNorm1d-4              [-1, 64, 240]             192\n",
      "    MFCC_Gen_coeff-5              [-1, 64, 240]         107,906\n",
      "            Conv2d-6          [-1, 16, 244, 65]             128\n",
      "       BatchNorm2d-7          [-1, 16, 244, 65]              48\n",
      "            Conv2d-8          [-1, 32, 242, 63]           4,672\n",
      "       BatchNorm2d-9          [-1, 32, 242, 63]              96\n",
      "        Dropout2d-10          [-1, 32, 242, 63]               0\n",
      "           Conv2d-11          [-1, 32, 244, 65]           9,280\n",
      "      BatchNorm2d-12          [-1, 32, 244, 65]              96\n",
      "        Dropout2d-13          [-1, 32, 244, 65]               0\n",
      "           Conv2d-14          [-1, 64, 122, 32]          18,560\n",
      "      BatchNorm2d-15          [-1, 64, 122, 32]             192\n",
      "        Dropout2d-16          [-1, 64, 122, 32]               0\n",
      "           Conv2d-17           [-1, 64, 61, 16]          36,992\n",
      "      BatchNorm2d-18           [-1, 64, 61, 16]             192\n",
      "        Dropout2d-19           [-1, 64, 61, 16]               0\n",
      "           Conv2d-20          [-1, 128, 61, 16]          73,984\n",
      "      BatchNorm2d-21          [-1, 128, 61, 16]             384\n",
      "        Dropout2d-22          [-1, 128, 61, 16]               0\n",
      "           Conv2d-23           [-1, 128, 30, 8]         147,712\n",
      "      BatchNorm2d-24           [-1, 128, 30, 8]             384\n",
      "        Dropout2d-25           [-1, 128, 30, 8]               0\n",
      "           Conv2d-26           [-1, 256, 30, 8]         295,424\n",
      "      BatchNorm2d-27           [-1, 256, 30, 8]             768\n",
      "        Dropout2d-28           [-1, 256, 30, 8]               0\n",
      "           Conv2d-29           [-1, 256, 15, 4]         590,336\n",
      "      BatchNorm2d-30           [-1, 256, 15, 4]             768\n",
      "        Dropout2d-31           [-1, 256, 15, 4]               0\n",
      "        Extractor-32                 [-1, 7168]       1,178,064\n",
      "           Linear-33                  [-1, 100]         717,000\n",
      "             ReLU-34                  [-1, 100]               0\n",
      "           Linear-35                    [-1, 2]             204\n",
      "          Softmax-36                    [-1, 2]               0\n",
      " Class_classifier-37                    [-1, 2]         717,102\n",
      "          Network-38                    [-1, 2]       1,895,166\n",
      "================================================================\n",
      "Total params: 5,903,554\n",
      "Trainable params: 5,795,842\n",
      "Non-trainable params: 107,712\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 41.89\n",
      "Params size (MB): 22.52\n",
      "Estimated Total Size (MB): 64.42\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model.cuda(),(1,2500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr= .00001)\n",
    "class_criterion = nn.CrossEntropyLoss()\n",
    "domain_criterion = nn.CrossEntropyLoss()\n",
    "# class_criterion = nn.BCELoss()\n",
    "start_epoch = 0\n",
    "lr_schedule = optim.lr_scheduler.ExponentialLR(optimizer,1)\n",
    "# wow= []\n",
    "# for i in range(400):\n",
    "#     optimizer.step()\n",
    "#     wow.append(optimizer.param_groups[0]['lr'])\n",
    "#     sh.step()\n",
    "# plt.plot(wow)\n",
    "# print(wow[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 16, 244, 65]             128\n",
      "       BatchNorm2d-2          [-1, 16, 244, 65]              48\n",
      "            Conv2d-3          [-1, 32, 242, 63]           4,672\n",
      "       BatchNorm2d-4          [-1, 32, 242, 63]              96\n",
      "         Dropout2d-5          [-1, 32, 242, 63]               0\n",
      "            Conv2d-6          [-1, 32, 244, 65]           9,280\n",
      "       BatchNorm2d-7          [-1, 32, 244, 65]              96\n",
      "         Dropout2d-8          [-1, 32, 244, 65]               0\n",
      "            Conv2d-9          [-1, 64, 122, 32]          18,560\n",
      "      BatchNorm2d-10          [-1, 64, 122, 32]             192\n",
      "        Dropout2d-11          [-1, 64, 122, 32]               0\n",
      "           Conv2d-12           [-1, 64, 61, 16]          36,992\n",
      "      BatchNorm2d-13           [-1, 64, 61, 16]             192\n",
      "        Dropout2d-14           [-1, 64, 61, 16]               0\n",
      "           Conv2d-15          [-1, 128, 61, 16]          73,984\n",
      "      BatchNorm2d-16          [-1, 128, 61, 16]             384\n",
      "        Dropout2d-17          [-1, 128, 61, 16]               0\n",
      "           Conv2d-18           [-1, 128, 30, 8]         147,712\n",
      "      BatchNorm2d-19           [-1, 128, 30, 8]             384\n",
      "        Dropout2d-20           [-1, 128, 30, 8]               0\n",
      "           Conv2d-21           [-1, 256, 30, 8]         295,424\n",
      "      BatchNorm2d-22           [-1, 256, 30, 8]             768\n",
      "        Dropout2d-23           [-1, 256, 30, 8]               0\n",
      "           Conv2d-24           [-1, 256, 15, 4]         590,336\n",
      "      BatchNorm2d-25           [-1, 256, 15, 4]             768\n",
      "        Dropout2d-26           [-1, 256, 15, 4]               0\n",
      "        Extractor-27                 [-1, 7168]       1,178,064\n",
      "           Linear-28                  [-1, 100]         717,000\n",
      "             ReLU-29                  [-1, 100]               0\n",
      "           Linear-30                    [-1, 2]             204\n",
      "          Softmax-31                    [-1, 2]               0\n",
      " Class_classifier-32                    [-1, 2]         717,102\n",
      "================================================================\n",
      "Total params: 3,792,386\n",
      "Trainable params: 3,792,386\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 39.18\n",
      "Params size (MB): 14.47\n",
      "Estimated Total Size (MB): 53.71\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model.cuda(),(1,240,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(torch.randn(1,1,240,64))\n",
    "make_dot(out,params = dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "eps = 0.0000001\n",
    "def log_macc(y_pred, y_val,val_parts):\n",
    "    y_pred = y_pred.cpu().detach().numpy()\n",
    "#     y_pred_domain = y_pred_domain.cpu().detach().numpy()\n",
    "    y_val = y_val.cpu().detach().numpy()\n",
    "    true = []\n",
    "    pred = []\n",
    "    files = []\n",
    "    start_idx = 0\n",
    "\n",
    "#     y_pred = np.argmax(y_pred, axis=-1)\n",
    "#     y_val = np.transpose(np.argmax(y_val, axis=-1))\n",
    "\n",
    "    for j,s in enumerate(val_parts):\n",
    "\n",
    "        if not s:  ## for e00032 in validation0 there was no cardiac cycle\n",
    "            continue\n",
    "        # ~ print \"part {} start {} stop {}\".format(s,start_idx,start_idx+int(s)-1)\n",
    "        \n",
    "        temp_ = y_val[start_idx:start_idx + int(s)]\n",
    "        temp = y_pred[start_idx:start_idx + int(s)]\n",
    "\n",
    "        if (sum(temp == 0) > sum(temp == 1)):\n",
    "            pred.append(0)\n",
    "        else:\n",
    "            pred.append(1)\n",
    "\n",
    "        if (sum(temp_ == 0) > sum(temp_ == 1)):\n",
    "            true.append(0)\n",
    "        else:\n",
    "            true.append(1)\n",
    "\n",
    "#         if val_files is not None:\n",
    "#             files.append(val_files[start_idx])\n",
    "\n",
    "        start_idx = start_idx + int(s)\n",
    "    TN, FP, FN, TP = confusion_matrix(true, pred, labels=[0,1]).ravel()\n",
    "    # TN = float(TN)\n",
    "    # TP = float(TP)\n",
    "    # FP = float(FP)\n",
    "    # FN = float(FN)\n",
    "    sensitivity = TP / (TP + FN + eps)\n",
    "    specificity = TN / (TN + FP + eps)\n",
    "    precision = TP / (TP + FP + eps)\n",
    "    F1 = 2 * (precision * sensitivity) / (precision + sensitivity + eps)\n",
    "    Macc = (sensitivity + specificity) / 2\n",
    "    \n",
    "    print(\"TN:\",TN,\"FP:\",FP,\"FN:\",FN,\"TP:\",TP)\n",
    "    print(\"Sensitivity:\",\"%.2f\"%sensitivity,\"Specificity:\",\"%.2f\"%specificity,\"Precision:\",\"%.2f\"%precision,end=' ')\n",
    "    print(\"F1:\", \"%.2f\"%F1,\"MACC\", \"%.2f\"%Macc)\n",
    "    return Macc,sensitivity,specificity,precision,F1\n",
    "def trainLog(y_true,y_pred):\n",
    "    eps = 0.0000001\n",
    "    y_pred = y_pred.cpu().detach().numpy()\n",
    "    y_true = y_true.cpu().detach().numpy()\n",
    "    TN, FP, FN, TP = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    sensitivity = TP / (TP + FN + eps)\n",
    "    specificity = TN / (TN + FP + eps)\n",
    "    precision = TP / (TP + FP + eps)\n",
    "    F1 = 2 * (precision * sensitivity) / (precision + sensitivity + eps)\n",
    "    Macc = (sensitivity + specificity) / 2\n",
    "    print(\"TN:\",TN,\"FP:\",FP,\"FN:\",FN,\"TP:\",TP)\n",
    "    print(\"Sensitivity:\",\"%.2f\"%sensitivity,\"Specificity:\",\"%.2f\"%specificity,\"Precision:\",\"%.2f\"%precision,end=' ')\n",
    "    print(\"F1:\", \"%.2f\"%F1,\"MACC\", \"%.2f\"%Macc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from datetime import datetime\n",
    "import CSVLogger\n",
    "CSVLogger = importlib.reload(CSVLogger)\n",
    "from CSVLogger import CSVLogger\n",
    "fold = \"a_bcdef 64 mfcc .00001\"\n",
    "path = \"../../Heartnet_Results/logs/gammatone_torch_layer/\"\n",
    "fold = fold+'_'+str(datetime.now()).replace(':','.')\n",
    "path = path + fold\n",
    "if not os.path.isdir(path):\n",
    "    os.mkdir(path)\n",
    "    os.mkdir(os.path.join(path,'weights'))\n",
    "logger = CSVLogger(path+'/'+'training.csv')\n",
    "checkpoint_name = os.path.join(path,'weights') + \"/\" + 'weights.{epoch:04d}-acc_{val_acc:.4f}-macc_{macc:.4f}.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps  27395\n",
      "EPOCH    1\n",
      "learning rate  1e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###   MFCC\n",
    "\n",
    "model.cuda()\n",
    "mfcc_gen.cuda()\n",
    "mfcc_gen.eval()\n",
    "epochs = 1500\n",
    "# torch.save(model,os.path.join(path,'model.pt'))\n",
    "logger.on_train_begin()\n",
    "print(\"steps \", flow_source.steps_per_epoch)\n",
    "\n",
    "# with open(path+fold+'/model1.json', 'w') as outfile:\n",
    "#     outfile.write(str(mod))\n",
    "for e in range(epochs):\n",
    "    print(\"EPOCH   \",e+1)\n",
    "    print(\"learning rate \",optimizer.param_groups[0]['lr'])\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    acc = 0\n",
    "    y_pred = None\n",
    "    y_true = None\n",
    "    N = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    for i in range(flow_source.steps_per_epoch+1):\n",
    "        \n",
    "        \n",
    "        x,y = flow_source.next()\n",
    "        if(str(class_criterion) in ['MSELoss()','BCELoss()']):\n",
    "            y = to_categorical(y,2)\n",
    "            y = y.astype(np.float32)\n",
    "        x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
    "        x = x.type(torch.FloatTensor).cuda()\n",
    "        holdx = x\n",
    "        x = mfcc_gen(x)\n",
    "#         if(len(x)>1):\n",
    "#             x = x[0]\n",
    "        hold = x\n",
    "        x = x.transpose(2,1)\n",
    "        x = x.unsqueeze(1)\n",
    "        # print(x.shape)\n",
    "        x,y = Variable(x),Variable(y)\n",
    "        \n",
    "#         y = y.cuda()\n",
    "        y = y.long().cuda()        \n",
    "        cls = model(x)\n",
    "        # class_loss = class_criterion(cls,torch.argmax(y,axis=1))        \n",
    "        loss = class_criterion(cls,y)\n",
    "        loss.backward()\n",
    "        epoch_loss = epoch_loss + loss\n",
    "        if(i%50==0 or i==flow_source.steps_per_epoch):            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "#         if(str(class_criterion)in ['MSELoss()','BCELoss()']):\n",
    "#             y = torch.argmax(y,axis=1)\n",
    "        if(y_pred is None):\n",
    "            y_pred = torch.argmax(cls,axis=1)\n",
    "            y_true = y\n",
    "        else:\n",
    "            y_pred = torch.cat((y_pred,torch.argmax(cls,axis=1)))\n",
    "            y_true = torch.cat((y_true,y))\n",
    "    \n",
    "        acc = acc + torch.sum(y==torch.argmax(cls,axis=1))\n",
    "        N = N+len(y)\n",
    "    epoch_loss_print = (epoch_loss.item()) if (type(epoch_loss)==torch.Tensor) else epoch_loss\n",
    "    acc_print = (acc.item()) if (type(acc)==torch.Tensor) else acc\n",
    "    print(\"Training loss\", \"%.2f\"%(epoch_loss_print/flow_source.steps_per_epoch),end=' ')\n",
    "    print(\"Training Acc \", \"%.2f\"%(acc_print/N),end=' ')\n",
    "    trainLog(y_true,y_pred)\n",
    "    logger.logs['train_loss'] = (epoch_loss_print/flow_source.steps_per_epoch)\n",
    "    logger.logs['train_acc'] = (acc_print/N)\n",
    "    # Validate \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    acc = 0\n",
    "    N = 0\n",
    "    y_pred = None\n",
    "    y_true = None\n",
    "    with torch.no_grad():\n",
    "        start_idx = 0\n",
    "        for i,s in enumerate(val_parts):\n",
    "            if(s==0):\n",
    "                continue\n",
    "            x,y = x_val[start_idx:start_idx+s],y_val[start_idx:start_idx+s]\n",
    "            start_idx = start_idx+s\n",
    "            \n",
    "            if(str(class_criterion) in ['MSELoss()','BCELoss()']):\n",
    "                y = to_categorical(y,2)\n",
    "                y = y.astype(np.float32)\n",
    "            \n",
    "            x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
    "            x = x.type(torch.FloatTensor).cuda()\n",
    "            holdvalx = x\n",
    "            x = mfcc_gen(x)\n",
    "#             if(len(x)>1):\n",
    "#                 x = x[0]\n",
    "            holdval = x\n",
    "            x = x.transpose(2,1)\n",
    "            x = x.unsqueeze(1)\n",
    "            x,y = Variable(x),Variable(y)\n",
    "            #x = x.reshape(x.shape[0],1,x.shape[1],x.shape[2])\n",
    "            \n",
    "#             y = y.cuda()\n",
    "            y = y.long().cuda()\n",
    "            cls= model(x)\n",
    "            # val_class_loss = class_criterion(cls,torch.argmax(y,axis=1))\n",
    "            val_class_loss = class_criterion(cls,y)\n",
    "            \n",
    "            if(str(class_criterion) in ['MSELoss()','BCELoss()']):\n",
    "                y = torch.argmax(y,axis=1)\n",
    "            \n",
    "            acc = acc + torch.sum(y==torch.argmax(cls,axis=1))\n",
    "            N = N+len(y)\n",
    "            epoch_loss = epoch_loss + val_class_loss\n",
    "            if(y_pred is None):\n",
    "                y_pred = torch.argmax(cls,axis=1)\n",
    "                y_true = y\n",
    "            else:\n",
    "                y_pred = torch.cat((y_pred,torch.argmax(cls,axis=1)))\n",
    "                y_true = torch.cat((y_true,y))\n",
    "        Macc,sensitivity,specificity,precision,F1 = log_macc(y_pred,y_true,val_parts)\n",
    "        \n",
    "        print(\"Validation loss\", \"%.2f\"%(epoch_loss.item()/len(val_parts)),end=' ')\n",
    "        print(\"Validation Acc \", \"%.2f\"%(acc.item()/N))\n",
    "        logger.logs['val_loss'] = (epoch_loss.item()/len(val_parts))\n",
    "        logger.logs['val_acc'] = (acc.item()/N)\n",
    "        acc = (acc.item()/N)\n",
    "        logger.logs['val_macc'] = Macc\n",
    "        logger.logs['precision'] = precision\n",
    "        logger.logs['sensitivity'] = sensitivity\n",
    "        logger.logs['specificity'] = specificity\n",
    "        logger.logs['F1'] = F1\n",
    "    lr_schedule.step()\n",
    "    torch.save(model.state_dict(),checkpoint_name.format(epoch=e,val_acc=acc,macc=Macc))\n",
    "    logger.on_epoch_end(e)\n",
    "    flow_source.reset()\n",
    "logger.on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps  27673\n",
      "EPOCH    1\n",
      "learning rate  0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhealthra2/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.56 Training Acc  0.73 TN: 243233 FP: 88855 FN: 90757 TP: 241331\n",
      "Sensitivity: 0.73 Specificity: 0.73 Precision: 0.73 F1: 0.73 MACC 0.73\n",
      "TN: 117 FP: 29 FN: 47 TP: 91\n",
      "Sensitivity: 0.66 Specificity: 0.80 Precision: 0.76 F1: 0.71 MACC 0.73\n",
      "Validation loss 0.56 Validation Acc  0.72\n",
      "EPOCH    2\n",
      "learning rate  0.005\n",
      "Training loss 0.49 Training Acc  0.82 TN: 269224 FP: 62864 FN: 58315 TP: 273773\n",
      "Sensitivity: 0.82 Specificity: 0.81 Precision: 0.81 F1: 0.82 MACC 0.82\n",
      "TN: 116 FP: 30 FN: 44 TP: 94\n",
      "Sensitivity: 0.68 Specificity: 0.79 Precision: 0.76 F1: 0.72 MACC 0.74\n",
      "Validation loss 0.55 Validation Acc  0.72\n",
      "EPOCH    3\n",
      "learning rate  0.005\n",
      "Training loss 0.46 Training Acc  0.85 TN: 280975 FP: 51113 FN: 46194 TP: 285894\n",
      "Sensitivity: 0.86 Specificity: 0.85 Precision: 0.85 F1: 0.85 MACC 0.85\n",
      "TN: 114 FP: 32 FN: 40 TP: 98\n",
      "Sensitivity: 0.71 Specificity: 0.78 Precision: 0.75 F1: 0.73 MACC 0.75\n",
      "Validation loss 0.56 Validation Acc  0.71\n",
      "EPOCH    4\n",
      "learning rate  0.005\n",
      "Training loss 0.44 Training Acc  0.88 TN: 288521 FP: 43567 FN: 39252 TP: 292836\n",
      "Sensitivity: 0.88 Specificity: 0.87 Precision: 0.87 F1: 0.88 MACC 0.88\n",
      "TN: 115 FP: 31 FN: 36 TP: 102\n",
      "Sensitivity: 0.74 Specificity: 0.79 Precision: 0.77 F1: 0.75 MACC 0.76\n",
      "Validation loss 0.55 Validation Acc  0.73\n",
      "EPOCH    5\n",
      "learning rate  0.005\n",
      "Training loss 0.42 Training Acc  0.89 TN: 293822 FP: 38266 FN: 34035 TP: 298053\n",
      "Sensitivity: 0.90 Specificity: 0.88 Precision: 0.89 F1: 0.89 MACC 0.89\n",
      "TN: 111 FP: 35 FN: 33 TP: 105\n",
      "Sensitivity: 0.76 Specificity: 0.76 Precision: 0.75 F1: 0.76 MACC 0.76\n",
      "Validation loss 0.55 Validation Acc  0.73\n",
      "EPOCH    6\n",
      "learning rate  0.005\n",
      "Training loss 0.41 Training Acc  0.90 TN: 297010 FP: 35078 FN: 30963 TP: 301125\n",
      "Sensitivity: 0.91 Specificity: 0.89 Precision: 0.90 F1: 0.90 MACC 0.90\n",
      "TN: 122 FP: 24 FN: 44 TP: 94\n",
      "Sensitivity: 0.68 Specificity: 0.84 Precision: 0.80 F1: 0.73 MACC 0.76\n",
      "Validation loss 0.56 Validation Acc  0.72\n",
      "EPOCH    7\n",
      "learning rate  0.005\n",
      "Training loss 0.40 Training Acc  0.91 TN: 300343 FP: 31745 FN: 28297 TP: 303791\n",
      "Sensitivity: 0.91 Specificity: 0.90 Precision: 0.91 F1: 0.91 MACC 0.91\n",
      "TN: 117 FP: 29 FN: 32 TP: 106\n",
      "Sensitivity: 0.77 Specificity: 0.80 Precision: 0.79 F1: 0.78 MACC 0.78\n",
      "Validation loss 0.56 Validation Acc  0.73\n",
      "EPOCH    8\n",
      "learning rate  0.005\n",
      "Training loss 0.40 Training Acc  0.92 TN: 302683 FP: 29405 FN: 26356 TP: 305732\n",
      "Sensitivity: 0.92 Specificity: 0.91 Precision: 0.91 F1: 0.92 MACC 0.92\n",
      "TN: 124 FP: 22 FN: 43 TP: 95\n",
      "Sensitivity: 0.69 Specificity: 0.85 Precision: 0.81 F1: 0.75 MACC 0.77\n",
      "Validation loss 0.56 Validation Acc  0.72\n",
      "EPOCH    9\n",
      "learning rate  0.005\n",
      "Training loss 0.39 Training Acc  0.92 TN: 305007 FP: 27081 FN: 24455 TP: 307633\n",
      "Sensitivity: 0.93 Specificity: 0.92 Precision: 0.92 F1: 0.92 MACC 0.92\n",
      "TN: 133 FP: 13 FN: 58 TP: 80\n",
      "Sensitivity: 0.58 Specificity: 0.91 Precision: 0.86 F1: 0.69 MACC 0.75\n",
      "Validation loss 0.58 Validation Acc  0.71\n",
      "EPOCH    10\n",
      "learning rate  0.005\n",
      "Training loss 0.39 Training Acc  0.93 TN: 306561 FP: 25527 FN: 23048 TP: 309040\n",
      "Sensitivity: 0.93 Specificity: 0.92 Precision: 0.92 F1: 0.93 MACC 0.93\n",
      "TN: 129 FP: 17 FN: 52 TP: 86\n",
      "Sensitivity: 0.62 Specificity: 0.88 Precision: 0.83 F1: 0.71 MACC 0.75\n",
      "Validation loss 0.58 Validation Acc  0.71\n",
      "EPOCH    11\n",
      "learning rate  0.005\n",
      "Training loss 0.38 Training Acc  0.93 TN: 307861 FP: 24227 FN: 22315 TP: 309773\n",
      "Sensitivity: 0.93 Specificity: 0.93 Precision: 0.93 F1: 0.93 MACC 0.93\n",
      "TN: 122 FP: 24 FN: 41 TP: 97\n",
      "Sensitivity: 0.70 Specificity: 0.84 Precision: 0.80 F1: 0.75 MACC 0.77\n",
      "Validation loss 0.57 Validation Acc  0.72\n",
      "EPOCH    12\n",
      "learning rate  0.005\n",
      "Training loss 0.38 Training Acc  0.93 TN: 309161 FP: 22927 FN: 21404 TP: 310684\n",
      "Sensitivity: 0.94 Specificity: 0.93 Precision: 0.93 F1: 0.93 MACC 0.93\n",
      "TN: 129 FP: 17 FN: 58 TP: 80\n",
      "Sensitivity: 0.58 Specificity: 0.88 Precision: 0.82 F1: 0.68 MACC 0.73\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    13\n",
      "learning rate  0.005\n",
      "Training loss 0.38 Training Acc  0.94 TN: 310015 FP: 22073 FN: 20355 TP: 311733\n",
      "Sensitivity: 0.94 Specificity: 0.93 Precision: 0.93 F1: 0.94 MACC 0.94\n",
      "TN: 120 FP: 26 FN: 40 TP: 98\n",
      "Sensitivity: 0.71 Specificity: 0.82 Precision: 0.79 F1: 0.75 MACC 0.77\n",
      "Validation loss 0.56 Validation Acc  0.73\n",
      "EPOCH    14\n",
      "learning rate  0.005\n",
      "Training loss 0.37 Training Acc  0.94 TN: 311291 FP: 20797 FN: 18916 TP: 313172\n",
      "Sensitivity: 0.94 Specificity: 0.94 Precision: 0.94 F1: 0.94 MACC 0.94\n",
      "TN: 129 FP: 17 FN: 51 TP: 87\n",
      "Sensitivity: 0.63 Specificity: 0.88 Precision: 0.84 F1: 0.72 MACC 0.76\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    15\n",
      "learning rate  0.005\n",
      "Training loss 0.37 Training Acc  0.94 TN: 312155 FP: 19933 FN: 18499 TP: 313589\n",
      "Sensitivity: 0.94 Specificity: 0.94 Precision: 0.94 F1: 0.94 MACC 0.94\n",
      "TN: 127 FP: 19 FN: 50 TP: 88\n",
      "Sensitivity: 0.64 Specificity: 0.87 Precision: 0.82 F1: 0.72 MACC 0.75\n",
      "Validation loss 0.57 Validation Acc  0.72\n",
      "EPOCH    16\n",
      "learning rate  0.005\n",
      "Training loss 0.37 Training Acc  0.94 TN: 313038 FP: 19050 FN: 17672 TP: 314416\n",
      "Sensitivity: 0.95 Specificity: 0.94 Precision: 0.94 F1: 0.94 MACC 0.94\n",
      "TN: 130 FP: 16 FN: 47 TP: 91\n",
      "Sensitivity: 0.66 Specificity: 0.89 Precision: 0.85 F1: 0.74 MACC 0.77\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    17\n",
      "learning rate  0.005\n",
      "Training loss 0.37 Training Acc  0.95 TN: 313700 FP: 18388 FN: 17247 TP: 314841\n",
      "Sensitivity: 0.95 Specificity: 0.94 Precision: 0.94 F1: 0.95 MACC 0.95\n",
      "TN: 124 FP: 22 FN: 42 TP: 96\n",
      "Sensitivity: 0.70 Specificity: 0.85 Precision: 0.81 F1: 0.75 MACC 0.77\n",
      "Validation loss 0.56 Validation Acc  0.73\n",
      "EPOCH    18\n",
      "learning rate  0.005\n",
      "Training loss 0.36 Training Acc  0.95 TN: 314438 FP: 17650 FN: 16624 TP: 315464\n",
      "Sensitivity: 0.95 Specificity: 0.95 Precision: 0.95 F1: 0.95 MACC 0.95\n",
      "TN: 134 FP: 12 FN: 56 TP: 82\n",
      "Sensitivity: 0.59 Specificity: 0.92 Precision: 0.87 F1: 0.71 MACC 0.76\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    19\n",
      "learning rate  0.005\n",
      "Training loss 0.36 Training Acc  0.95 TN: 314907 FP: 17181 FN: 16094 TP: 315994\n",
      "Sensitivity: 0.95 Specificity: 0.95 Precision: 0.95 F1: 0.95 MACC 0.95\n",
      "TN: 128 FP: 18 FN: 47 TP: 91\n",
      "Sensitivity: 0.66 Specificity: 0.88 Precision: 0.83 F1: 0.74 MACC 0.77\n",
      "Validation loss 0.58 Validation Acc  0.72\n",
      "EPOCH    20\n",
      "learning rate  0.005\n",
      "Training loss 0.36 Training Acc  0.95 TN: 315583 FP: 16505 FN: 15562 TP: 316526\n",
      "Sensitivity: 0.95 Specificity: 0.95 Precision: 0.95 F1: 0.95 MACC 0.95\n",
      "TN: 119 FP: 27 FN: 37 TP: 101\n",
      "Sensitivity: 0.73 Specificity: 0.82 Precision: 0.79 F1: 0.76 MACC 0.77\n",
      "Validation loss 0.56 Validation Acc  0.73\n",
      "EPOCH    21\n",
      "learning rate  0.005\n",
      "Training loss 0.36 Training Acc  0.95 TN: 316296 FP: 15792 FN: 15117 TP: 316971\n",
      "Sensitivity: 0.95 Specificity: 0.95 Precision: 0.95 F1: 0.95 MACC 0.95\n",
      "TN: 123 FP: 23 FN: 38 TP: 100\n",
      "Sensitivity: 0.72 Specificity: 0.84 Precision: 0.81 F1: 0.77 MACC 0.78\n",
      "Validation loss 0.56 Validation Acc  0.73\n",
      "EPOCH    22\n",
      "learning rate  0.005\n",
      "Training loss 0.36 Training Acc  0.95 TN: 316637 FP: 15451 FN: 14951 TP: 317137\n",
      "Sensitivity: 0.95 Specificity: 0.95 Precision: 0.95 F1: 0.95 MACC 0.95\n",
      "TN: 119 FP: 27 FN: 37 TP: 101\n",
      "Sensitivity: 0.73 Specificity: 0.82 Precision: 0.79 F1: 0.76 MACC 0.77\n",
      "Validation loss 0.56 Validation Acc  0.73\n",
      "EPOCH    23\n",
      "learning rate  0.005\n",
      "Training loss 0.36 Training Acc  0.96 TN: 317110 FP: 14978 FN: 14544 TP: 317544\n",
      "Sensitivity: 0.96 Specificity: 0.95 Precision: 0.95 F1: 0.96 MACC 0.96\n",
      "TN: 126 FP: 20 FN: 46 TP: 92\n",
      "Sensitivity: 0.67 Specificity: 0.86 Precision: 0.82 F1: 0.74 MACC 0.76\n",
      "Validation loss 0.57 Validation Acc  0.72\n",
      "EPOCH    24\n",
      "learning rate  0.005\n",
      "Training loss 0.36 Training Acc  0.96 TN: 317192 FP: 14896 FN: 14114 TP: 317974\n",
      "Sensitivity: 0.96 Specificity: 0.96 Precision: 0.96 F1: 0.96 MACC 0.96\n",
      "TN: 127 FP: 19 FN: 50 TP: 88\n",
      "Sensitivity: 0.64 Specificity: 0.87 Precision: 0.82 F1: 0.72 MACC 0.75\n",
      "Validation loss 0.58 Validation Acc  0.71\n",
      "EPOCH    25\n",
      "learning rate  0.005\n",
      "Training loss 0.35 Training Acc  0.96 TN: 317919 FP: 14169 FN: 13698 TP: 318390\n",
      "Sensitivity: 0.96 Specificity: 0.96 Precision: 0.96 F1: 0.96 MACC 0.96\n",
      "TN: 127 FP: 19 FN: 42 TP: 96\n",
      "Sensitivity: 0.70 Specificity: 0.87 Precision: 0.83 F1: 0.76 MACC 0.78\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    26\n",
      "learning rate  0.005\n",
      "Training loss 0.35 Training Acc  0.96 TN: 318222 FP: 13866 FN: 13365 TP: 318723\n",
      "Sensitivity: 0.96 Specificity: 0.96 Precision: 0.96 F1: 0.96 MACC 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN: 126 FP: 20 FN: 50 TP: 88\n",
      "Sensitivity: 0.64 Specificity: 0.86 Precision: 0.81 F1: 0.72 MACC 0.75\n",
      "Validation loss 0.58 Validation Acc  0.72\n",
      "EPOCH    27\n",
      "learning rate  0.005\n",
      "Training loss 0.35 Training Acc  0.96 TN: 318366 FP: 13722 FN: 12884 TP: 319204\n",
      "Sensitivity: 0.96 Specificity: 0.96 Precision: 0.96 F1: 0.96 MACC 0.96\n",
      "TN: 124 FP: 22 FN: 40 TP: 98\n",
      "Sensitivity: 0.71 Specificity: 0.85 Precision: 0.82 F1: 0.76 MACC 0.78\n",
      "Validation loss 0.56 Validation Acc  0.73\n",
      "EPOCH    28\n",
      "learning rate  0.005\n",
      "Training loss 0.35 Training Acc  0.96 TN: 318695 FP: 13393 FN: 12906 TP: 319182\n",
      "Sensitivity: 0.96 Specificity: 0.96 Precision: 0.96 F1: 0.96 MACC 0.96\n",
      "TN: 126 FP: 20 FN: 38 TP: 100\n",
      "Sensitivity: 0.72 Specificity: 0.86 Precision: 0.83 F1: 0.78 MACC 0.79\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    29\n",
      "learning rate  0.005\n",
      "Training loss 0.35 Training Acc  0.96 TN: 318981 FP: 13107 FN: 12490 TP: 319598\n",
      "Sensitivity: 0.96 Specificity: 0.96 Precision: 0.96 F1: 0.96 MACC 0.96\n",
      "TN: 123 FP: 23 FN: 45 TP: 93\n",
      "Sensitivity: 0.67 Specificity: 0.84 Precision: 0.80 F1: 0.73 MACC 0.76\n",
      "Validation loss 0.57 Validation Acc  0.72\n",
      "EPOCH    30\n",
      "learning rate  0.005\n",
      "Training loss 0.35 Training Acc  0.96 TN: 319001 FP: 13087 FN: 12517 TP: 319571\n",
      "Sensitivity: 0.96 Specificity: 0.96 Precision: 0.96 F1: 0.96 MACC 0.96\n",
      "TN: 122 FP: 24 FN: 38 TP: 100\n",
      "Sensitivity: 0.72 Specificity: 0.84 Precision: 0.81 F1: 0.76 MACC 0.78\n",
      "Validation loss 0.56 Validation Acc  0.73\n",
      "EPOCH    31\n",
      "learning rate  0.005\n",
      "Training loss 0.35 Training Acc  0.96 TN: 319433 FP: 12655 FN: 11944 TP: 320144\n",
      "Sensitivity: 0.96 Specificity: 0.96 Precision: 0.96 F1: 0.96 MACC 0.96\n",
      "TN: 124 FP: 22 FN: 34 TP: 104\n",
      "Sensitivity: 0.75 Specificity: 0.85 Precision: 0.83 F1: 0.79 MACC 0.80\n",
      "Validation loss 0.57 Validation Acc  0.72\n",
      "EPOCH    32\n",
      "learning rate  0.005\n",
      "Training loss 0.35 Training Acc  0.96 TN: 319536 FP: 12552 FN: 11809 TP: 320279\n",
      "Sensitivity: 0.96 Specificity: 0.96 Precision: 0.96 F1: 0.96 MACC 0.96\n",
      "TN: 124 FP: 22 FN: 33 TP: 105\n",
      "Sensitivity: 0.76 Specificity: 0.85 Precision: 0.83 F1: 0.79 MACC 0.81\n",
      "Validation loss 0.57 Validation Acc  0.72\n",
      "EPOCH    33\n",
      "learning rate  0.005\n",
      "Training loss 0.35 Training Acc  0.96 TN: 319801 FP: 12287 FN: 11579 TP: 320509\n",
      "Sensitivity: 0.97 Specificity: 0.96 Precision: 0.96 F1: 0.96 MACC 0.96\n",
      "TN: 126 FP: 20 FN: 44 TP: 94\n",
      "Sensitivity: 0.68 Specificity: 0.86 Precision: 0.82 F1: 0.75 MACC 0.77\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    34\n",
      "learning rate  0.005\n",
      "Training loss 0.35 Training Acc  0.96 TN: 319993 FP: 12095 FN: 11182 TP: 320906\n",
      "Sensitivity: 0.97 Specificity: 0.96 Precision: 0.96 F1: 0.97 MACC 0.96\n",
      "TN: 126 FP: 20 FN: 36 TP: 102\n",
      "Sensitivity: 0.74 Specificity: 0.86 Precision: 0.84 F1: 0.78 MACC 0.80\n",
      "Validation loss 0.56 Validation Acc  0.74\n",
      "EPOCH    35\n",
      "learning rate  0.005\n",
      "Training loss 0.35 Training Acc  0.97 TN: 320284 FP: 11804 FN: 11013 TP: 321075\n",
      "Sensitivity: 0.97 Specificity: 0.96 Precision: 0.96 F1: 0.97 MACC 0.97\n",
      "TN: 125 FP: 21 FN: 37 TP: 101\n",
      "Sensitivity: 0.73 Specificity: 0.86 Precision: 0.83 F1: 0.78 MACC 0.79\n",
      "Validation loss 0.57 Validation Acc  0.72\n",
      "EPOCH    36\n",
      "learning rate  0.005\n",
      "Training loss 0.35 Training Acc  0.97 TN: 320666 FP: 11422 FN: 10627 TP: 321461\n",
      "Sensitivity: 0.97 Specificity: 0.97 Precision: 0.97 F1: 0.97 MACC 0.97\n",
      "TN: 127 FP: 19 FN: 40 TP: 98\n",
      "Sensitivity: 0.71 Specificity: 0.87 Precision: 0.84 F1: 0.77 MACC 0.79\n",
      "Validation loss 0.57 Validation Acc  0.72\n",
      "EPOCH    37\n",
      "learning rate  0.005\n",
      "Training loss 0.35 Training Acc  0.97 TN: 320923 FP: 11165 FN: 10628 TP: 321460\n",
      "Sensitivity: 0.97 Specificity: 0.97 Precision: 0.97 F1: 0.97 MACC 0.97\n",
      "TN: 109 FP: 37 FN: 29 TP: 109\n",
      "Sensitivity: 0.79 Specificity: 0.75 Precision: 0.75 F1: 0.77 MACC 0.77\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    38\n",
      "learning rate  0.005\n",
      "Training loss 0.35 Training Acc  0.97 TN: 320881 FP: 11207 FN: 10572 TP: 321516\n",
      "Sensitivity: 0.97 Specificity: 0.97 Precision: 0.97 F1: 0.97 MACC 0.97\n",
      "TN: 126 FP: 20 FN: 42 TP: 96\n",
      "Sensitivity: 0.70 Specificity: 0.86 Precision: 0.83 F1: 0.76 MACC 0.78\n",
      "Validation loss 0.57 Validation Acc  0.72\n",
      "EPOCH    39\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.97 TN: 321245 FP: 10843 FN: 10309 TP: 321779\n",
      "Sensitivity: 0.97 Specificity: 0.97 Precision: 0.97 F1: 0.97 MACC 0.97\n",
      "TN: 126 FP: 20 FN: 36 TP: 102\n",
      "Sensitivity: 0.74 Specificity: 0.86 Precision: 0.84 F1: 0.78 MACC 0.80\n",
      "Validation loss 0.56 Validation Acc  0.73\n",
      "EPOCH    40\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.97 TN: 321410 FP: 10678 FN: 9980 TP: 322108\n",
      "Sensitivity: 0.97 Specificity: 0.97 Precision: 0.97 F1: 0.97 MACC 0.97\n",
      "TN: 125 FP: 21 FN: 42 TP: 96\n",
      "Sensitivity: 0.70 Specificity: 0.86 Precision: 0.82 F1: 0.75 MACC 0.78\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    41\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.97 TN: 321628 FP: 10460 FN: 9823 TP: 322265\n",
      "Sensitivity: 0.97 Specificity: 0.97 Precision: 0.97 F1: 0.97 MACC 0.97\n",
      "TN: 122 FP: 24 FN: 40 TP: 98\n",
      "Sensitivity: 0.71 Specificity: 0.84 Precision: 0.80 F1: 0.75 MACC 0.77\n",
      "Validation loss 0.58 Validation Acc  0.72\n",
      "EPOCH    42\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.97 TN: 321755 FP: 10333 FN: 9681 TP: 322407\n",
      "Sensitivity: 0.97 Specificity: 0.97 Precision: 0.97 F1: 0.97 MACC 0.97\n",
      "TN: 125 FP: 21 FN: 46 TP: 92\n",
      "Sensitivity: 0.67 Specificity: 0.86 Precision: 0.81 F1: 0.73 MACC 0.76\n",
      "Validation loss 0.58 Validation Acc  0.72\n",
      "EPOCH    43\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.97 TN: 321867 FP: 10221 FN: 9400 TP: 322688\n",
      "Sensitivity: 0.97 Specificity: 0.97 Precision: 0.97 F1: 0.97 MACC 0.97\n",
      "TN: 123 FP: 23 FN: 40 TP: 98\n",
      "Sensitivity: 0.71 Specificity: 0.84 Precision: 0.81 F1: 0.76 MACC 0.78\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    44\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.97 TN: 321946 FP: 10142 FN: 9372 TP: 322716\n",
      "Sensitivity: 0.97 Specificity: 0.97 Precision: 0.97 F1: 0.97 MACC 0.97\n",
      "TN: 121 FP: 25 FN: 40 TP: 98\n",
      "Sensitivity: 0.71 Specificity: 0.83 Precision: 0.80 F1: 0.75 MACC 0.77\n",
      "Validation loss 0.56 Validation Acc  0.73\n",
      "EPOCH    45\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.97 TN: 322209 FP: 9879 FN: 9241 TP: 322847\n",
      "Sensitivity: 0.97 Specificity: 0.97 Precision: 0.97 F1: 0.97 MACC 0.97\n",
      "TN: 128 FP: 18 FN: 44 TP: 94\n",
      "Sensitivity: 0.68 Specificity: 0.88 Precision: 0.84 F1: 0.75 MACC 0.78\n",
      "Validation loss 0.57 Validation Acc  0.72\n",
      "EPOCH    46\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.97 TN: 322319 FP: 9769 FN: 9127 TP: 322961\n",
      "Sensitivity: 0.97 Specificity: 0.97 Precision: 0.97 F1: 0.97 MACC 0.97\n",
      "TN: 127 FP: 19 FN: 60 TP: 78\n",
      "Sensitivity: 0.57 Specificity: 0.87 Precision: 0.80 F1: 0.66 MACC 0.72\n",
      "Validation loss 0.59 Validation Acc  0.71\n",
      "EPOCH    47\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.97 TN: 322596 FP: 9492 FN: 8971 TP: 323117\n",
      "Sensitivity: 0.97 Specificity: 0.97 Precision: 0.97 F1: 0.97 MACC 0.97\n",
      "TN: 122 FP: 24 FN: 32 TP: 106\n",
      "Sensitivity: 0.77 Specificity: 0.84 Precision: 0.82 F1: 0.79 MACC 0.80\n",
      "Validation loss 0.56 Validation Acc  0.73\n",
      "EPOCH    48\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.97 TN: 322647 FP: 9441 FN: 8682 TP: 323406\n",
      "Sensitivity: 0.97 Specificity: 0.97 Precision: 0.97 F1: 0.97 MACC 0.97\n",
      "TN: 124 FP: 22 FN: 42 TP: 96\n",
      "Sensitivity: 0.70 Specificity: 0.85 Precision: 0.81 F1: 0.75 MACC 0.77\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    49\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.97 TN: 322659 FP: 9429 FN: 8788 TP: 323300\n",
      "Sensitivity: 0.97 Specificity: 0.97 Precision: 0.97 F1: 0.97 MACC 0.97\n",
      "TN: 125 FP: 21 FN: 42 TP: 96\n",
      "Sensitivity: 0.70 Specificity: 0.86 Precision: 0.82 F1: 0.75 MACC 0.78\n",
      "Validation loss 0.58 Validation Acc  0.72\n",
      "EPOCH    50\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.97 TN: 322896 FP: 9192 FN: 8606 TP: 323482\n",
      "Sensitivity: 0.97 Specificity: 0.97 Precision: 0.97 F1: 0.97 MACC 0.97\n",
      "TN: 122 FP: 24 FN: 41 TP: 97\n",
      "Sensitivity: 0.70 Specificity: 0.84 Precision: 0.80 F1: 0.75 MACC 0.77\n",
      "Validation loss 0.58 Validation Acc  0.72\n",
      "EPOCH    51\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.97 TN: 323196 FP: 8892 FN: 8342 TP: 323746\n",
      "Sensitivity: 0.97 Specificity: 0.97 Precision: 0.97 F1: 0.97 MACC 0.97\n",
      "TN: 124 FP: 22 FN: 46 TP: 92\n",
      "Sensitivity: 0.67 Specificity: 0.85 Precision: 0.81 F1: 0.73 MACC 0.76\n",
      "Validation loss 0.58 Validation Acc  0.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH    52\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.97 TN: 323182 FP: 8906 FN: 8420 TP: 323668\n",
      "Sensitivity: 0.97 Specificity: 0.97 Precision: 0.97 F1: 0.97 MACC 0.97\n",
      "TN: 123 FP: 23 FN: 33 TP: 105\n",
      "Sensitivity: 0.76 Specificity: 0.84 Precision: 0.82 F1: 0.79 MACC 0.80\n",
      "Validation loss 0.57 Validation Acc  0.72\n",
      "EPOCH    53\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.97 TN: 323365 FP: 8723 FN: 8145 TP: 323943\n",
      "Sensitivity: 0.98 Specificity: 0.97 Precision: 0.97 F1: 0.97 MACC 0.97\n",
      "TN: 121 FP: 25 FN: 41 TP: 97\n",
      "Sensitivity: 0.70 Specificity: 0.83 Precision: 0.80 F1: 0.75 MACC 0.77\n",
      "Validation loss 0.58 Validation Acc  0.72\n",
      "EPOCH    54\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.98 TN: 323406 FP: 8682 FN: 7874 TP: 324214\n",
      "Sensitivity: 0.98 Specificity: 0.97 Precision: 0.97 F1: 0.98 MACC 0.98\n",
      "TN: 120 FP: 26 FN: 29 TP: 109\n",
      "Sensitivity: 0.79 Specificity: 0.82 Precision: 0.81 F1: 0.80 MACC 0.81\n",
      "Validation loss 0.56 Validation Acc  0.73\n",
      "EPOCH    55\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.98 TN: 323662 FP: 8426 FN: 7983 TP: 324105\n",
      "Sensitivity: 0.98 Specificity: 0.97 Precision: 0.97 F1: 0.98 MACC 0.98\n",
      "TN: 118 FP: 28 FN: 34 TP: 104\n",
      "Sensitivity: 0.75 Specificity: 0.81 Precision: 0.79 F1: 0.77 MACC 0.78\n",
      "Validation loss 0.57 Validation Acc  0.72\n",
      "EPOCH    56\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.98 TN: 323552 FP: 8536 FN: 7939 TP: 324149\n",
      "Sensitivity: 0.98 Specificity: 0.97 Precision: 0.97 F1: 0.98 MACC 0.98\n",
      "TN: 123 FP: 23 FN: 42 TP: 96\n",
      "Sensitivity: 0.70 Specificity: 0.84 Precision: 0.81 F1: 0.75 MACC 0.77\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    57\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.98 TN: 323699 FP: 8389 FN: 7961 TP: 324127\n",
      "Sensitivity: 0.98 Specificity: 0.97 Precision: 0.97 F1: 0.98 MACC 0.98\n",
      "TN: 125 FP: 21 FN: 39 TP: 99\n",
      "Sensitivity: 0.72 Specificity: 0.86 Precision: 0.82 F1: 0.77 MACC 0.79\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    58\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.98 TN: 323854 FP: 8234 FN: 7756 TP: 324332\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 125 FP: 21 FN: 45 TP: 93\n",
      "Sensitivity: 0.67 Specificity: 0.86 Precision: 0.82 F1: 0.74 MACC 0.77\n",
      "Validation loss 0.58 Validation Acc  0.72\n",
      "EPOCH    59\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.98 TN: 323750 FP: 8338 FN: 7571 TP: 324517\n",
      "Sensitivity: 0.98 Specificity: 0.97 Precision: 0.97 F1: 0.98 MACC 0.98\n",
      "TN: 123 FP: 23 FN: 40 TP: 98\n",
      "Sensitivity: 0.71 Specificity: 0.84 Precision: 0.81 F1: 0.76 MACC 0.78\n",
      "Validation loss 0.57 Validation Acc  0.72\n",
      "EPOCH    60\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.98 TN: 324140 FP: 7948 FN: 7607 TP: 324481\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 124 FP: 22 FN: 43 TP: 95\n",
      "Sensitivity: 0.69 Specificity: 0.85 Precision: 0.81 F1: 0.75 MACC 0.77\n",
      "Validation loss 0.57 Validation Acc  0.72\n",
      "EPOCH    61\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.98 TN: 324180 FP: 7908 FN: 7333 TP: 324755\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 125 FP: 21 FN: 38 TP: 100\n",
      "Sensitivity: 0.72 Specificity: 0.86 Precision: 0.83 F1: 0.77 MACC 0.79\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    62\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.98 TN: 324153 FP: 7935 FN: 7312 TP: 324776\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 125 FP: 21 FN: 42 TP: 96\n",
      "Sensitivity: 0.70 Specificity: 0.86 Precision: 0.82 F1: 0.75 MACC 0.78\n",
      "Validation loss 0.58 Validation Acc  0.72\n",
      "EPOCH    63\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.98 TN: 324421 FP: 7667 FN: 7114 TP: 324974\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 125 FP: 21 FN: 46 TP: 92\n",
      "Sensitivity: 0.67 Specificity: 0.86 Precision: 0.81 F1: 0.73 MACC 0.76\n",
      "Validation loss 0.58 Validation Acc  0.71\n",
      "EPOCH    64\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.98 TN: 324277 FP: 7811 FN: 7114 TP: 324974\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 121 FP: 25 FN: 35 TP: 103\n",
      "Sensitivity: 0.75 Specificity: 0.83 Precision: 0.80 F1: 0.77 MACC 0.79\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    65\n",
      "learning rate  0.005\n",
      "Training loss 0.34 Training Acc  0.98 TN: 324561 FP: 7527 FN: 7057 TP: 325031\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 121 FP: 25 FN: 35 TP: 103\n",
      "Sensitivity: 0.75 Specificity: 0.83 Precision: 0.80 F1: 0.77 MACC 0.79\n",
      "Validation loss 0.56 Validation Acc  0.73\n",
      "EPOCH    66\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 324683 FP: 7405 FN: 6922 TP: 325166\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 122 FP: 24 FN: 34 TP: 104\n",
      "Sensitivity: 0.75 Specificity: 0.84 Precision: 0.81 F1: 0.78 MACC 0.79\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    67\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 324796 FP: 7292 FN: 6945 TP: 325143\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 117 FP: 29 FN: 35 TP: 103\n",
      "Sensitivity: 0.75 Specificity: 0.80 Precision: 0.78 F1: 0.76 MACC 0.77\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    68\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 324695 FP: 7393 FN: 6866 TP: 325222\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 119 FP: 27 FN: 36 TP: 102\n",
      "Sensitivity: 0.74 Specificity: 0.82 Precision: 0.79 F1: 0.76 MACC 0.78\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    69\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 324631 FP: 7457 FN: 6784 TP: 325304\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 124 FP: 22 FN: 37 TP: 101\n",
      "Sensitivity: 0.73 Specificity: 0.85 Precision: 0.82 F1: 0.77 MACC 0.79\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    70\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 324807 FP: 7281 FN: 6659 TP: 325429\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 121 FP: 25 FN: 39 TP: 99\n",
      "Sensitivity: 0.72 Specificity: 0.83 Precision: 0.80 F1: 0.76 MACC 0.77\n",
      "Validation loss 0.58 Validation Acc  0.73\n",
      "EPOCH    71\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 324947 FP: 7141 FN: 6481 TP: 325607\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 122 FP: 24 FN: 38 TP: 100\n",
      "Sensitivity: 0.72 Specificity: 0.84 Precision: 0.81 F1: 0.76 MACC 0.78\n",
      "Validation loss 0.58 Validation Acc  0.72\n",
      "EPOCH    72\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 325093 FP: 6995 FN: 6511 TP: 325577\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 121 FP: 25 FN: 35 TP: 103\n",
      "Sensitivity: 0.75 Specificity: 0.83 Precision: 0.80 F1: 0.77 MACC 0.79\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    73\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 325029 FP: 7059 FN: 6348 TP: 325740\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 114 FP: 32 FN: 30 TP: 108\n",
      "Sensitivity: 0.78 Specificity: 0.78 Precision: 0.77 F1: 0.78 MACC 0.78\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    74\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 325138 FP: 6950 FN: 6524 TP: 325564\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 125 FP: 21 FN: 42 TP: 96\n",
      "Sensitivity: 0.70 Specificity: 0.86 Precision: 0.82 F1: 0.75 MACC 0.78\n",
      "Validation loss 0.58 Validation Acc  0.72\n",
      "EPOCH    75\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 325241 FP: 6847 FN: 6323 TP: 325765\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 116 FP: 30 FN: 36 TP: 102\n",
      "Sensitivity: 0.74 Specificity: 0.79 Precision: 0.77 F1: 0.76 MACC 0.77\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    76\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 325172 FP: 6916 FN: 6173 TP: 325915\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 121 FP: 25 FN: 37 TP: 101\n",
      "Sensitivity: 0.73 Specificity: 0.83 Precision: 0.80 F1: 0.77 MACC 0.78\n",
      "Validation loss 0.57 Validation Acc  0.72\n",
      "EPOCH    77\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 325369 FP: 6719 FN: 6164 TP: 325924\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN: 120 FP: 26 FN: 38 TP: 100\n",
      "Sensitivity: 0.72 Specificity: 0.82 Precision: 0.79 F1: 0.76 MACC 0.77\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    78\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 325189 FP: 6899 FN: 6041 TP: 326047\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 113 FP: 33 FN: 31 TP: 107\n",
      "Sensitivity: 0.78 Specificity: 0.77 Precision: 0.76 F1: 0.77 MACC 0.77\n",
      "Validation loss 0.58 Validation Acc  0.72\n",
      "EPOCH    79\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 325436 FP: 6652 FN: 6014 TP: 326074\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 121 FP: 25 FN: 42 TP: 96\n",
      "Sensitivity: 0.70 Specificity: 0.83 Precision: 0.79 F1: 0.74 MACC 0.76\n",
      "Validation loss 0.58 Validation Acc  0.73\n",
      "EPOCH    80\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 325436 FP: 6652 FN: 6140 TP: 325948\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 128 FP: 18 FN: 44 TP: 94\n",
      "Sensitivity: 0.68 Specificity: 0.88 Precision: 0.84 F1: 0.75 MACC 0.78\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    81\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 325521 FP: 6567 FN: 5897 TP: 326191\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 122 FP: 24 FN: 37 TP: 101\n",
      "Sensitivity: 0.73 Specificity: 0.84 Precision: 0.81 F1: 0.77 MACC 0.78\n",
      "Validation loss 0.58 Validation Acc  0.72\n",
      "EPOCH    82\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 325632 FP: 6456 FN: 5918 TP: 326170\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 115 FP: 31 FN: 28 TP: 110\n",
      "Sensitivity: 0.80 Specificity: 0.79 Precision: 0.78 F1: 0.79 MACC 0.79\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    83\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 325591 FP: 6497 FN: 5807 TP: 326281\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 117 FP: 29 FN: 36 TP: 102\n",
      "Sensitivity: 0.74 Specificity: 0.80 Precision: 0.78 F1: 0.76 MACC 0.77\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    84\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 325604 FP: 6484 FN: 5897 TP: 326191\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 121 FP: 25 FN: 35 TP: 103\n",
      "Sensitivity: 0.75 Specificity: 0.83 Precision: 0.80 F1: 0.77 MACC 0.79\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    85\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 325588 FP: 6500 FN: 5733 TP: 326355\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 124 FP: 22 FN: 41 TP: 97\n",
      "Sensitivity: 0.70 Specificity: 0.85 Precision: 0.82 F1: 0.75 MACC 0.78\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    86\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 325773 FP: 6315 FN: 5713 TP: 326375\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 116 FP: 30 FN: 31 TP: 107\n",
      "Sensitivity: 0.78 Specificity: 0.79 Precision: 0.78 F1: 0.78 MACC 0.78\n",
      "Validation loss 0.56 Validation Acc  0.74\n",
      "EPOCH    87\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 325887 FP: 6201 FN: 5511 TP: 326577\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 124 FP: 22 FN: 35 TP: 103\n",
      "Sensitivity: 0.75 Specificity: 0.85 Precision: 0.82 F1: 0.78 MACC 0.80\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    88\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 325809 FP: 6279 FN: 5655 TP: 326433\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 115 FP: 31 FN: 31 TP: 107\n",
      "Sensitivity: 0.78 Specificity: 0.79 Precision: 0.78 F1: 0.78 MACC 0.78\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    89\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 325850 FP: 6238 FN: 5572 TP: 326516\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 111 FP: 35 FN: 27 TP: 111\n",
      "Sensitivity: 0.80 Specificity: 0.76 Precision: 0.76 F1: 0.78 MACC 0.78\n",
      "Validation loss 0.57 Validation Acc  0.72\n",
      "EPOCH    90\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 325938 FP: 6150 FN: 5476 TP: 326612\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 123 FP: 23 FN: 41 TP: 97\n",
      "Sensitivity: 0.70 Specificity: 0.84 Precision: 0.81 F1: 0.75 MACC 0.77\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    91\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 326050 FP: 6038 FN: 5504 TP: 326584\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 119 FP: 27 FN: 31 TP: 107\n",
      "Sensitivity: 0.78 Specificity: 0.82 Precision: 0.80 F1: 0.79 MACC 0.80\n",
      "Validation loss 0.56 Validation Acc  0.74\n",
      "EPOCH    92\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 325919 FP: 6169 FN: 5365 TP: 326723\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 123 FP: 23 FN: 38 TP: 100\n",
      "Sensitivity: 0.72 Specificity: 0.84 Precision: 0.81 F1: 0.77 MACC 0.78\n",
      "Validation loss 0.58 Validation Acc  0.72\n",
      "EPOCH    93\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 326110 FP: 5978 FN: 5390 TP: 326698\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 123 FP: 23 FN: 33 TP: 105\n",
      "Sensitivity: 0.76 Specificity: 0.84 Precision: 0.82 F1: 0.79 MACC 0.80\n",
      "Validation loss 0.57 Validation Acc  0.74\n",
      "EPOCH    94\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 326134 FP: 5954 FN: 5264 TP: 326824\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 119 FP: 27 FN: 38 TP: 100\n",
      "Sensitivity: 0.72 Specificity: 0.82 Precision: 0.79 F1: 0.75 MACC 0.77\n",
      "Validation loss 0.57 Validation Acc  0.72\n",
      "EPOCH    95\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 326326 FP: 5762 FN: 5223 TP: 326865\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 119 FP: 27 FN: 40 TP: 98\n",
      "Sensitivity: 0.71 Specificity: 0.82 Precision: 0.78 F1: 0.75 MACC 0.76\n",
      "Validation loss 0.57 Validation Acc  0.72\n",
      "EPOCH    96\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 326414 FP: 5674 FN: 5329 TP: 326759\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 117 FP: 29 FN: 31 TP: 107\n",
      "Sensitivity: 0.78 Specificity: 0.80 Precision: 0.79 F1: 0.78 MACC 0.79\n",
      "Validation loss 0.56 Validation Acc  0.73\n",
      "EPOCH    97\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 326270 FP: 5818 FN: 5133 TP: 326955\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 122 FP: 24 FN: 37 TP: 101\n",
      "Sensitivity: 0.73 Specificity: 0.84 Precision: 0.81 F1: 0.77 MACC 0.78\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    98\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 326281 FP: 5807 FN: 5240 TP: 326848\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 123 FP: 23 FN: 34 TP: 104\n",
      "Sensitivity: 0.75 Specificity: 0.84 Precision: 0.82 F1: 0.78 MACC 0.80\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    99\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 326272 FP: 5816 FN: 5136 TP: 326952\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 123 FP: 23 FN: 48 TP: 90\n",
      "Sensitivity: 0.65 Specificity: 0.84 Precision: 0.80 F1: 0.72 MACC 0.75\n",
      "Validation loss 0.58 Validation Acc  0.73\n",
      "EPOCH    100\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 326375 FP: 5713 FN: 5120 TP: 326968\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 125 FP: 21 FN: 39 TP: 99\n",
      "Sensitivity: 0.72 Specificity: 0.86 Precision: 0.82 F1: 0.77 MACC 0.79\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    101\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 326580 FP: 5508 FN: 5095 TP: 326993\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 115 FP: 31 FN: 34 TP: 104\n",
      "Sensitivity: 0.75 Specificity: 0.79 Precision: 0.77 F1: 0.76 MACC 0.77\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    102\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 326539 FP: 5549 FN: 4915 TP: 327173\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 113 FP: 33 FN: 32 TP: 106\n",
      "Sensitivity: 0.77 Specificity: 0.77 Precision: 0.76 F1: 0.77 MACC 0.77\n",
      "Validation loss 0.56 Validation Acc  0.74\n",
      "EPOCH    103\n",
      "learning rate  0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.33 Training Acc  0.98 TN: 326524 FP: 5564 FN: 5046 TP: 327042\n",
      "Sensitivity: 0.98 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 117 FP: 29 FN: 35 TP: 103\n",
      "Sensitivity: 0.75 Specificity: 0.80 Precision: 0.78 F1: 0.76 MACC 0.77\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    104\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 326571 FP: 5517 FN: 4959 TP: 327129\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 117 FP: 29 FN: 40 TP: 98\n",
      "Sensitivity: 0.71 Specificity: 0.80 Precision: 0.77 F1: 0.74 MACC 0.76\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    105\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 326606 FP: 5482 FN: 4912 TP: 327176\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 120 FP: 26 FN: 39 TP: 99\n",
      "Sensitivity: 0.72 Specificity: 0.82 Precision: 0.79 F1: 0.75 MACC 0.77\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    106\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 326647 FP: 5441 FN: 4886 TP: 327202\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 120 FP: 26 FN: 31 TP: 107\n",
      "Sensitivity: 0.78 Specificity: 0.82 Precision: 0.80 F1: 0.79 MACC 0.80\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    107\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 326719 FP: 5369 FN: 4914 TP: 327174\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 116 FP: 30 FN: 28 TP: 110\n",
      "Sensitivity: 0.80 Specificity: 0.79 Precision: 0.79 F1: 0.79 MACC 0.80\n",
      "Validation loss 0.56 Validation Acc  0.74\n",
      "EPOCH    108\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 326667 FP: 5421 FN: 4696 TP: 327392\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 114 FP: 32 FN: 30 TP: 108\n",
      "Sensitivity: 0.78 Specificity: 0.78 Precision: 0.77 F1: 0.78 MACC 0.78\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    109\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 326732 FP: 5356 FN: 4652 TP: 327436\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 121 FP: 25 FN: 35 TP: 103\n",
      "Sensitivity: 0.75 Specificity: 0.83 Precision: 0.80 F1: 0.77 MACC 0.79\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    110\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 326624 FP: 5464 FN: 4754 TP: 327334\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 126 FP: 20 FN: 49 TP: 89\n",
      "Sensitivity: 0.64 Specificity: 0.86 Precision: 0.82 F1: 0.72 MACC 0.75\n",
      "Validation loss 0.58 Validation Acc  0.72\n",
      "EPOCH    111\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 326742 FP: 5346 FN: 4671 TP: 327417\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 122 FP: 24 FN: 44 TP: 94\n",
      "Sensitivity: 0.68 Specificity: 0.84 Precision: 0.80 F1: 0.73 MACC 0.76\n",
      "Validation loss 0.58 Validation Acc  0.72\n",
      "EPOCH    112\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 326669 FP: 5419 FN: 4640 TP: 327448\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 118 FP: 28 FN: 31 TP: 107\n",
      "Sensitivity: 0.78 Specificity: 0.81 Precision: 0.79 F1: 0.78 MACC 0.79\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    113\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.98 TN: 326723 FP: 5365 FN: 4641 TP: 327447\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.98 F1: 0.98 MACC 0.98\n",
      "TN: 116 FP: 30 FN: 29 TP: 109\n",
      "Sensitivity: 0.79 Specificity: 0.79 Precision: 0.78 F1: 0.79 MACC 0.79\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    114\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 326911 FP: 5177 FN: 4652 TP: 327436\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.98 F1: 0.99 MACC 0.99\n",
      "TN: 116 FP: 30 FN: 37 TP: 101\n",
      "Sensitivity: 0.73 Specificity: 0.79 Precision: 0.77 F1: 0.75 MACC 0.76\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    115\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 326824 FP: 5264 FN: 4662 TP: 327426\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.98 F1: 0.99 MACC 0.99\n",
      "TN: 121 FP: 25 FN: 34 TP: 104\n",
      "Sensitivity: 0.75 Specificity: 0.83 Precision: 0.81 F1: 0.78 MACC 0.79\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    116\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 326881 FP: 5207 FN: 4592 TP: 327496\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.98 F1: 0.99 MACC 0.99\n",
      "TN: 117 FP: 29 FN: 28 TP: 110\n",
      "Sensitivity: 0.80 Specificity: 0.80 Precision: 0.79 F1: 0.79 MACC 0.80\n",
      "Validation loss 0.56 Validation Acc  0.74\n",
      "EPOCH    117\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 326899 FP: 5189 FN: 4478 TP: 327610\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.98 F1: 0.99 MACC 0.99\n",
      "TN: 116 FP: 30 FN: 28 TP: 110\n",
      "Sensitivity: 0.80 Specificity: 0.79 Precision: 0.79 F1: 0.79 MACC 0.80\n",
      "Validation loss 0.56 Validation Acc  0.74\n",
      "EPOCH    118\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 326792 FP: 5296 FN: 4531 TP: 327557\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.98 F1: 0.99 MACC 0.99\n",
      "TN: 122 FP: 24 FN: 41 TP: 97\n",
      "Sensitivity: 0.70 Specificity: 0.84 Precision: 0.80 F1: 0.75 MACC 0.77\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    119\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 326948 FP: 5140 FN: 4471 TP: 327617\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.98 F1: 0.99 MACC 0.99\n",
      "TN: 114 FP: 32 FN: 28 TP: 110\n",
      "Sensitivity: 0.80 Specificity: 0.78 Precision: 0.77 F1: 0.79 MACC 0.79\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    120\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 326987 FP: 5101 FN: 4490 TP: 327598\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.98 F1: 0.99 MACC 0.99\n",
      "TN: 114 FP: 32 FN: 31 TP: 107\n",
      "Sensitivity: 0.78 Specificity: 0.78 Precision: 0.77 F1: 0.77 MACC 0.78\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    121\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 326948 FP: 5140 FN: 4344 TP: 327744\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.98 F1: 0.99 MACC 0.99\n",
      "TN: 120 FP: 26 FN: 37 TP: 101\n",
      "Sensitivity: 0.73 Specificity: 0.82 Precision: 0.80 F1: 0.76 MACC 0.78\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    122\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 327075 FP: 5013 FN: 4393 TP: 327695\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.98 F1: 0.99 MACC 0.99\n",
      "TN: 125 FP: 21 FN: 40 TP: 98\n",
      "Sensitivity: 0.71 Specificity: 0.86 Precision: 0.82 F1: 0.76 MACC 0.78\n",
      "Validation loss 0.58 Validation Acc  0.73\n",
      "EPOCH    123\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 327106 FP: 4982 FN: 4442 TP: 327646\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.99 F1: 0.99 MACC 0.99\n",
      "TN: 117 FP: 29 FN: 38 TP: 100\n",
      "Sensitivity: 0.72 Specificity: 0.80 Precision: 0.78 F1: 0.75 MACC 0.76\n",
      "Validation loss 0.58 Validation Acc  0.72\n",
      "EPOCH    124\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 327043 FP: 5045 FN: 4419 TP: 327669\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.98 F1: 0.99 MACC 0.99\n",
      "TN: 124 FP: 22 FN: 48 TP: 90\n",
      "Sensitivity: 0.65 Specificity: 0.85 Precision: 0.80 F1: 0.72 MACC 0.75\n",
      "Validation loss 0.58 Validation Acc  0.73\n",
      "EPOCH    125\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 327086 FP: 5002 FN: 4308 TP: 327780\n",
      "Sensitivity: 0.99 Specificity: 0.98 Precision: 0.98 F1: 0.99 MACC 0.99\n",
      "TN: 129 FP: 17 FN: 42 TP: 96\n",
      "Sensitivity: 0.70 Specificity: 0.88 Precision: 0.85 F1: 0.76 MACC 0.79\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    126\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 327187 FP: 4901 FN: 4311 TP: 327777\n",
      "Sensitivity: 0.99 Specificity: 0.99 Precision: 0.99 F1: 0.99 MACC 0.99\n",
      "TN: 123 FP: 23 FN: 42 TP: 96\n",
      "Sensitivity: 0.70 Specificity: 0.84 Precision: 0.81 F1: 0.75 MACC 0.77\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    127\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 327181 FP: 4907 FN: 4247 TP: 327841\n",
      "Sensitivity: 0.99 Specificity: 0.99 Precision: 0.99 F1: 0.99 MACC 0.99\n",
      "TN: 120 FP: 26 FN: 42 TP: 96\n",
      "Sensitivity: 0.70 Specificity: 0.82 Precision: 0.79 F1: 0.74 MACC 0.76\n",
      "Validation loss 0.58 Validation Acc  0.72\n",
      "EPOCH    128\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 327223 FP: 4865 FN: 4224 TP: 327864\n",
      "Sensitivity: 0.99 Specificity: 0.99 Precision: 0.99 F1: 0.99 MACC 0.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN: 120 FP: 26 FN: 45 TP: 93\n",
      "Sensitivity: 0.67 Specificity: 0.82 Precision: 0.78 F1: 0.72 MACC 0.75\n",
      "Validation loss 0.58 Validation Acc  0.72\n",
      "EPOCH    129\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 327247 FP: 4841 FN: 4141 TP: 327947\n",
      "Sensitivity: 0.99 Specificity: 0.99 Precision: 0.99 F1: 0.99 MACC 0.99\n",
      "TN: 116 FP: 30 FN: 28 TP: 110\n",
      "Sensitivity: 0.80 Specificity: 0.79 Precision: 0.79 F1: 0.79 MACC 0.80\n",
      "Validation loss 0.56 Validation Acc  0.74\n",
      "EPOCH    130\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 327131 FP: 4957 FN: 4210 TP: 327878\n",
      "Sensitivity: 0.99 Specificity: 0.99 Precision: 0.99 F1: 0.99 MACC 0.99\n",
      "TN: 118 FP: 28 FN: 34 TP: 104\n",
      "Sensitivity: 0.75 Specificity: 0.81 Precision: 0.79 F1: 0.77 MACC 0.78\n",
      "Validation loss 0.56 Validation Acc  0.73\n",
      "EPOCH    131\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 327219 FP: 4869 FN: 4208 TP: 327880\n",
      "Sensitivity: 0.99 Specificity: 0.99 Precision: 0.99 F1: 0.99 MACC 0.99\n",
      "TN: 113 FP: 33 FN: 25 TP: 113\n",
      "Sensitivity: 0.82 Specificity: 0.77 Precision: 0.77 F1: 0.80 MACC 0.80\n",
      "Validation loss 0.56 Validation Acc  0.73\n",
      "EPOCH    132\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 327214 FP: 4874 FN: 4217 TP: 327871\n",
      "Sensitivity: 0.99 Specificity: 0.99 Precision: 0.99 F1: 0.99 MACC 0.99\n",
      "TN: 119 FP: 27 FN: 43 TP: 95\n",
      "Sensitivity: 0.69 Specificity: 0.82 Precision: 0.78 F1: 0.73 MACC 0.75\n",
      "Validation loss 0.58 Validation Acc  0.72\n",
      "EPOCH    133\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 327399 FP: 4689 FN: 4164 TP: 327924\n",
      "Sensitivity: 0.99 Specificity: 0.99 Precision: 0.99 F1: 0.99 MACC 0.99\n",
      "TN: 111 FP: 35 FN: 27 TP: 111\n",
      "Sensitivity: 0.80 Specificity: 0.76 Precision: 0.76 F1: 0.78 MACC 0.78\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    134\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 327356 FP: 4732 FN: 4071 TP: 328017\n",
      "Sensitivity: 0.99 Specificity: 0.99 Precision: 0.99 F1: 0.99 MACC 0.99\n",
      "TN: 113 FP: 33 FN: 34 TP: 104\n",
      "Sensitivity: 0.75 Specificity: 0.77 Precision: 0.76 F1: 0.76 MACC 0.76\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    135\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 327313 FP: 4775 FN: 4025 TP: 328063\n",
      "Sensitivity: 0.99 Specificity: 0.99 Precision: 0.99 F1: 0.99 MACC 0.99\n",
      "TN: 113 FP: 33 FN: 26 TP: 112\n",
      "Sensitivity: 0.81 Specificity: 0.77 Precision: 0.77 F1: 0.79 MACC 0.79\n",
      "Validation loss 0.56 Validation Acc  0.74\n",
      "EPOCH    136\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 327268 FP: 4820 FN: 3952 TP: 328136\n",
      "Sensitivity: 0.99 Specificity: 0.99 Precision: 0.99 F1: 0.99 MACC 0.99\n",
      "TN: 115 FP: 31 FN: 32 TP: 106\n",
      "Sensitivity: 0.77 Specificity: 0.79 Precision: 0.77 F1: 0.77 MACC 0.78\n",
      "Validation loss 0.56 Validation Acc  0.74\n",
      "EPOCH    137\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 327273 FP: 4815 FN: 4027 TP: 328061\n",
      "Sensitivity: 0.99 Specificity: 0.99 Precision: 0.99 F1: 0.99 MACC 0.99\n",
      "TN: 119 FP: 27 FN: 42 TP: 96\n",
      "Sensitivity: 0.70 Specificity: 0.82 Precision: 0.78 F1: 0.74 MACC 0.76\n",
      "Validation loss 0.57 Validation Acc  0.73\n",
      "EPOCH    138\n",
      "learning rate  0.005\n",
      "Training loss 0.33 Training Acc  0.99 TN: 327459 FP: 4629 FN: 3885 TP: 328203\n",
      "Sensitivity: 0.99 Specificity: 0.99 Precision: 0.99 F1: 0.99 MACC 0.99\n",
      "TN: 112 FP: 34 FN: 34 TP: 104\n",
      "Sensitivity: 0.75 Specificity: 0.77 Precision: 0.75 F1: 0.75 MACC 0.76\n",
      "Validation loss 0.58 Validation Acc  0.72\n",
      "EPOCH    139\n",
      "learning rate  0.005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-ad613efec18f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# class_loss = class_criterion(cls,torch.argmax(y,axis=1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mflow_source\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### HEartnet\n",
    "\n",
    "epochs = 1500\n",
    "# torch.save(model,os.path.join(path,'model.pt'))\n",
    "logger.on_train_begin()\n",
    "print(\"steps \", flow_source.steps_per_epoch)\n",
    "\n",
    "# with open(path+fold+'/model1.json', 'w') as outfile:\n",
    "#     outfile.write(str(mod))\n",
    "for e in range(epochs):\n",
    "    print(\"EPOCH   \",e+1)\n",
    "    print(\"learning rate \",optimizer.param_groups[0]['lr'])\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    acc = 0\n",
    "    y_pred = None\n",
    "    y_true = None\n",
    "    N = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    for i in range(flow_source.steps_per_epoch+1):\n",
    "        \n",
    "        \n",
    "        x,y = flow_source.next()\n",
    "        if(str(class_criterion) in ['MSELoss()','BCELoss()']):\n",
    "            y = to_categorical(y,2)\n",
    "            y = y.astype(np.float32)\n",
    "        x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
    "        x = x.type(torch.FloatTensor).cuda()\n",
    "        # print(x.shape)\n",
    "        x,y = Variable(x),Variable(y)\n",
    "        \n",
    "#         y = y.cuda()\n",
    "        y = y.long().cuda()        \n",
    "        cls = model(x)\n",
    "        # class_loss = class_criterion(cls,torch.argmax(y,axis=1))        \n",
    "        loss = class_criterion(cls,y)\n",
    "        loss.backward()\n",
    "        epoch_loss = epoch_loss + loss\n",
    "        if(i%50==0 or i==flow_source.steps_per_epoch):            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "#         if(str(class_criterion)in ['MSELoss()','BCELoss()']):\n",
    "#             y = torch.argmax(y,axis=1)\n",
    "        if(y_pred is None):\n",
    "            y_pred = torch.argmax(cls,axis=1)\n",
    "            y_true = y\n",
    "        else:\n",
    "            y_pred = torch.cat((y_pred,torch.argmax(cls,axis=1)))\n",
    "            y_true = torch.cat((y_true,y))\n",
    "    \n",
    "        acc = acc + torch.sum(y==torch.argmax(cls,axis=1))\n",
    "        N = N+len(y)\n",
    "    epoch_loss_print = (epoch_loss.item()) if (type(epoch_loss)==torch.Tensor) else epoch_loss\n",
    "    acc_print = (acc.item()) if (type(acc)==torch.Tensor) else acc\n",
    "    print(\"Training loss\", \"%.2f\"%(epoch_loss_print/flow_source.steps_per_epoch),end=' ')\n",
    "    print(\"Training Acc \", \"%.2f\"%(acc_print/N),end=' ')\n",
    "    trainLog(y_true,y_pred)\n",
    "    logger.logs['train_loss'] = (epoch_loss_print/flow_source.steps_per_epoch)\n",
    "    logger.logs['train_acc'] = (acc_print/N)\n",
    "    # Validate \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    acc = 0\n",
    "    N = 0\n",
    "    y_pred = None\n",
    "    y_true = None\n",
    "    with torch.no_grad():\n",
    "        start_idx = 0\n",
    "        for i,s in enumerate(val_parts):\n",
    "            if(s==0):\n",
    "                continue\n",
    "            x,y = x_val[start_idx:start_idx+s],y_val[start_idx:start_idx+s]\n",
    "            start_idx = start_idx+s\n",
    "            \n",
    "            if(str(class_criterion) in ['MSELoss()','BCELoss()']):\n",
    "                y = to_categorical(y,2)\n",
    "                y = y.astype(np.float32)\n",
    "            \n",
    "            x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
    "            x = x.type(torch.FloatTensor).cuda()\n",
    "            \n",
    "            x,y = Variable(x),Variable(y)\n",
    "            #x = x.reshape(x.shape[0],1,x.shape[1],x.shape[2])\n",
    "            \n",
    "#             y = y.cuda()\n",
    "            y = y.long().cuda()\n",
    "            cls= model(x)\n",
    "            # val_class_loss = class_criterion(cls,torch.argmax(y,axis=1))\n",
    "            val_class_loss = class_criterion(cls,y)\n",
    "            \n",
    "            if(str(class_criterion) in ['MSELoss()','BCELoss()']):\n",
    "                y = torch.argmax(y,axis=1)\n",
    "            \n",
    "            acc = acc + torch.sum(y==torch.argmax(cls,axis=1))\n",
    "            N = N+len(y)\n",
    "            epoch_loss = epoch_loss + val_class_loss\n",
    "            if(y_pred is None):\n",
    "                y_pred = torch.argmax(cls,axis=1)\n",
    "                y_true = y\n",
    "            else:\n",
    "                y_pred = torch.cat((y_pred,torch.argmax(cls,axis=1)))\n",
    "                y_true = torch.cat((y_true,y))\n",
    "        Macc,sensitivity,specificity,precision,F1 = log_macc(y_pred,y_true,val_parts)\n",
    "        \n",
    "        print(\"Validation loss\", \"%.2f\"%(epoch_loss.item()/len(val_parts)),end=' ')\n",
    "        print(\"Validation Acc \", \"%.2f\"%(acc.item()/N))\n",
    "        logger.logs['val_loss'] = (epoch_loss.item()/len(val_parts))\n",
    "        logger.logs['val_acc'] = (acc.item()/N)\n",
    "        acc = (acc.item()/N)\n",
    "        logger.logs['val_macc'] = Macc\n",
    "        logger.logs['precision'] = precision\n",
    "        logger.logs['sensitivity'] = sensitivity\n",
    "        logger.logs['specificity'] = specificity\n",
    "        logger.logs['F1'] = F1\n",
    "    lr_schedule.step()\n",
    "    torch.save(model.state_dict(),checkpoint_name.format(epoch=e,val_acc=acc,macc=Macc))\n",
    "    logger.on_epoch_end(e)\n",
    "    flow_source.reset()\n",
    "logger.on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bank = {i:to_numpy(model.filterbank[0].weight) for i in range(64)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"filterbank.pickle\",\"wb\") as f:\n",
    "    pickle.dump(bank,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps  32415\n",
      "EPOCH    1\n",
      "learning rate  0.005\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.91 GiB total capacity; 10.49 GiB already allocated; 17.56 MiB free; 11.02 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-5ed17556f2dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# class_loss = class_criterion(cls,torch.argmax(y,axis=1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mclass_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-2a029779a3d2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;31m#,mfcc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mhealthra2/Data/heart_sound/Adversarial-Heart-Sound-Classification/codes/HeartCepTorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hp_lambda)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0mclss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mhealthra2/Data/heart_sound/Adversarial-Heart-Sound-Classification/codes/HeartCepTorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;31m#Res block 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.91 GiB total capacity; 10.49 GiB already allocated; 17.56 MiB free; 11.02 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "epochs = 400\n",
    "\n",
    "logger\n",
    "logger.on_train_begin()\n",
    "optimizer.zero_grad()\n",
    "print(\"steps \", flow_source.steps_per_epoch)\n",
    "for e in range(start_epoch,epochs):\n",
    "    print(\"EPOCH   \",e+1)\n",
    "    print(\"learning rate \",optimizer.param_groups[0]['lr'])\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_loss_list = []\n",
    "    acc = 0\n",
    "    N = 0\n",
    "    for i in range(flow_source.steps_per_epoch+1):\n",
    "        \n",
    "        \n",
    "        \n",
    "        x,y = flow_source.next()\n",
    "        x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
    "        x = x.type(torch.FloatTensor).cuda()\n",
    "        x,y = Variable(x),Variable(y)\n",
    "        y = y.long().cuda()        \n",
    "        cls = model(x)\n",
    "        # class_loss = class_criterion(cls,torch.argmax(y,axis=1))        \n",
    "        class_loss = class_criterion(cls,y)\n",
    "        loss = class_loss\n",
    "        epoch_loss = epoch_loss + loss\n",
    "        epoch_loss_list.append(loss)\n",
    "        acc = acc + torch.sum(y==torch.argmax(cls,axis=1))\n",
    "        N = N+len(y)\n",
    "        \n",
    "        if(i%50==0 or i==flow_source.steps_per_epoch):\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "    print(\"Training loss\", \"%.2f\"%(epoch_loss.item()/flow_source.steps_per_epoch),end=' ')\n",
    "    print(\"Training loss list\", \"%.2f\"%(torch.mean(torch.tensor(epoch_loss_list)).item()),end=' ')\n",
    "    print(\"Training Acc \", \"%.2f\"%(acc.item()/N))\n",
    "    logger.logs['train_loss'] = (epoch_loss.item()/flow_source.steps_per_epoch)\n",
    "    logger.logs['train_acc'] = (acc.item()/N)\n",
    "    # Validate \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    acc = 0\n",
    "    N = 0\n",
    "    y_pred = None\n",
    "    y_true = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        start_idx = 0\n",
    "        for i,s in enumerate(val_parts):\n",
    "            x,y = x_val[start_idx:start_idx+s],y_val[start_idx:start_idx+s]\n",
    "            start_idx = start_idx+s\n",
    "            x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
    "            x = x.type(torch.FloatTensor).cuda()\n",
    "            x,y = Variable(x),Variable(y)\n",
    "            y = y.long().cuda()\n",
    "            cls= model(x)\n",
    "            val_class_loss = class_criterion(cls,y)\n",
    "            acc = acc + torch.sum(y==torch.argmax(cls,axis=1))\n",
    "            N = N+len(y)\n",
    "            epoch_loss = epoch_loss + val_class_loss\n",
    "            if(y_pred is None):\n",
    "                y_pred = torch.argmax(cls,axis=1)\n",
    "                y_true = y\n",
    "            else:\n",
    "                y_pred = torch.cat((y_pred,torch.argmax(cls,axis=1)))\n",
    "                y_true = torch.cat((y_true,y))\n",
    "        Macc,sensitivity,specificity,precision,F1 = log_macc(y_pred,y_true,val_parts)\n",
    "        \n",
    "        print(\"Validation loss\", \"%.2f\"%(epoch_loss.item()/len(val_parts)),end=' ')\n",
    "        print(\"Validation Acc \", \"%.2f\"%(acc.item()/N))\n",
    "        logger.logs['val_loss'] = (epoch_loss.item()/len(val_parts))\n",
    "        logger.logs['val_acc'] = (acc.item()/N)\n",
    "        acc = (acc.item()/N)\n",
    "        logger.logs['val_macc'] = Macc\n",
    "        logger.logs['precision'] = precision\n",
    "        logger.logs['sensitivity'] = sensitivity\n",
    "        logger.logs['specificity'] = specificity\n",
    "        logger.logs['F1'] = F1\n",
    "#     lr_schedule.step()\n",
    "    torch.save(model.state_dict(),checkpoint_name.format(epoch=e,val_acc=acc,macc=Macc))\n",
    "    logger.on_epoch_end(e) \n",
    "    flow_source.reset()\n",
    "logger.on_train_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFCC gen(not learning) -> Classfier(learning) two modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../../Heartnet_Results/logs/gammatone_torch_layer/fold0_noFIR batch1500 continued .005_2020-04-14 16.10.52.323265/weights/weights.0154-acc_0.7942-macc_0.8296.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2]), torch.Size([12]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps  6483\n",
      "EPOCH    1\n",
      "learning rate  0.001\n",
      "Training loss 0.61 Training Acc  0.68 TN: 44755 FP: 20085 FN: 21183 TP: 43657\n",
      "Sensitivity: 0.67 Specificity: 0.69 Precision: 0.68 F1: 0.68 MACC 0.68\n",
      "TN: 119 FP: 27 FN: 78 TP: 60\n",
      "Sensitivity: 0.43 Specificity: 0.82 Precision: 0.69 F1: 0.53 MACC 0.62\n",
      "Validation loss 0.62 Validation Acc  0.67\n",
      "EPOCH    2\n",
      "learning rate  0.00095\n",
      "Training loss 0.54 Training Acc  0.77 TN: 48762 FP: 16078 FN: 14198 TP: 50642\n",
      "Sensitivity: 0.78 Specificity: 0.75 Precision: 0.76 F1: 0.77 MACC 0.77\n",
      "TN: 72 FP: 74 FN: 30 TP: 108\n",
      "Sensitivity: 0.78 Specificity: 0.49 Precision: 0.59 F1: 0.67 MACC 0.64\n",
      "Validation loss 0.60 Validation Acc  0.68\n",
      "EPOCH    3\n",
      "learning rate  0.0009025\n",
      "Training loss 0.50 Training Acc  0.81 TN: 50005 FP: 14835 FN: 10196 TP: 54644\n",
      "Sensitivity: 0.84 Specificity: 0.77 Precision: 0.79 F1: 0.81 MACC 0.81\n",
      "TN: 66 FP: 80 FN: 20 TP: 118\n",
      "Sensitivity: 0.86 Specificity: 0.45 Precision: 0.60 F1: 0.70 MACC 0.65\n",
      "Validation loss 0.61 Validation Acc  0.68\n",
      "EPOCH    4\n",
      "learning rate  0.000857375\n",
      "Training loss 0.48 Training Acc  0.83 TN: 50769 FP: 14071 FN: 8435 TP: 56405\n",
      "Sensitivity: 0.87 Specificity: 0.78 Precision: 0.80 F1: 0.83 MACC 0.83\n",
      "TN: 72 FP: 74 FN: 22 TP: 116\n",
      "Sensitivity: 0.84 Specificity: 0.49 Precision: 0.61 F1: 0.71 MACC 0.67\n",
      "Validation loss 0.61 Validation Acc  0.69\n",
      "EPOCH    5\n",
      "learning rate  0.0008145062499999999\n",
      "Training loss 0.47 Training Acc  0.84 TN: 51194 FP: 13646 FN: 7244 TP: 57596\n",
      "Sensitivity: 0.89 Specificity: 0.79 Precision: 0.81 F1: 0.85 MACC 0.84\n",
      "TN: 66 FP: 80 FN: 13 TP: 125\n",
      "Sensitivity: 0.91 Specificity: 0.45 Precision: 0.61 F1: 0.73 MACC 0.68\n",
      "Validation loss 0.60 Validation Acc  0.70\n",
      "EPOCH    6\n",
      "learning rate  0.0007737809374999998\n",
      "Training loss 0.46 Training Acc  0.85 TN: 51611 FP: 13229 FN: 6703 TP: 58137\n",
      "Sensitivity: 0.90 Specificity: 0.80 Precision: 0.81 F1: 0.85 MACC 0.85\n",
      "TN: 59 FP: 87 FN: 6 TP: 132\n",
      "Sensitivity: 0.96 Specificity: 0.40 Precision: 0.60 F1: 0.74 MACC 0.68\n",
      "Validation loss 0.60 Validation Acc  0.70\n",
      "EPOCH    7\n",
      "learning rate  0.0007350918906249997\n",
      "Training loss 0.46 Training Acc  0.85 TN: 51768 FP: 13072 FN: 6083 TP: 58757\n",
      "Sensitivity: 0.91 Specificity: 0.80 Precision: 0.82 F1: 0.86 MACC 0.85\n",
      "TN: 66 FP: 80 FN: 8 TP: 130\n",
      "Sensitivity: 0.94 Specificity: 0.45 Precision: 0.62 F1: 0.75 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    8\n",
      "learning rate  0.0006983372960937497\n",
      "Training loss 0.45 Training Acc  0.86 TN: 52117 FP: 12723 FN: 5735 TP: 59105\n",
      "Sensitivity: 0.91 Specificity: 0.80 Precision: 0.82 F1: 0.86 MACC 0.86\n",
      "TN: 63 FP: 83 FN: 6 TP: 132\n",
      "Sensitivity: 0.96 Specificity: 0.43 Precision: 0.61 F1: 0.75 MACC 0.69\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    9\n",
      "learning rate  0.0006634204312890621\n",
      "Training loss 0.45 Training Acc  0.86 TN: 52148 FP: 12692 FN: 5504 TP: 59336\n",
      "Sensitivity: 0.92 Specificity: 0.80 Precision: 0.82 F1: 0.87 MACC 0.86\n",
      "TN: 61 FP: 85 FN: 6 TP: 132\n",
      "Sensitivity: 0.96 Specificity: 0.42 Precision: 0.61 F1: 0.74 MACC 0.69\n",
      "Validation loss 0.60 Validation Acc  0.71\n",
      "EPOCH    10\n",
      "learning rate  0.000630249409724609\n",
      "Training loss 0.45 Training Acc  0.86 TN: 52345 FP: 12495 FN: 5386 TP: 59454\n",
      "Sensitivity: 0.92 Specificity: 0.81 Precision: 0.83 F1: 0.87 MACC 0.86\n",
      "TN: 63 FP: 83 FN: 5 TP: 133\n",
      "Sensitivity: 0.96 Specificity: 0.43 Precision: 0.62 F1: 0.75 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    11\n",
      "learning rate  0.0005987369392383785\n",
      "Training loss 0.44 Training Acc  0.87 TN: 52440 FP: 12400 FN: 5080 TP: 59760\n",
      "Sensitivity: 0.92 Specificity: 0.81 Precision: 0.83 F1: 0.87 MACC 0.87\n",
      "TN: 69 FP: 77 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.47 Precision: 0.64 F1: 0.77 MACC 0.73\n",
      "Validation loss 0.58 Validation Acc  0.73\n",
      "EPOCH    12\n",
      "learning rate  0.0005688000922764595\n",
      "Training loss 0.44 Training Acc  0.87 TN: 52656 FP: 12184 FN: 4977 TP: 59863\n",
      "Sensitivity: 0.92 Specificity: 0.81 Precision: 0.83 F1: 0.87 MACC 0.87\n",
      "TN: 66 FP: 80 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.45 Precision: 0.63 F1: 0.77 MACC 0.72\n",
      "Validation loss 0.58 Validation Acc  0.73\n",
      "EPOCH    13\n",
      "learning rate  0.0005403600876626365\n",
      "Training loss 0.44 Training Acc  0.87 TN: 52651 FP: 12189 FN: 4744 TP: 60096\n",
      "Sensitivity: 0.93 Specificity: 0.81 Precision: 0.83 F1: 0.88 MACC 0.87\n",
      "TN: 63 FP: 83 FN: 4 TP: 134\n",
      "Sensitivity: 0.97 Specificity: 0.43 Precision: 0.62 F1: 0.75 MACC 0.70\n",
      "Validation loss 0.58 Validation Acc  0.73\n",
      "EPOCH    14\n",
      "learning rate  0.0005133420832795047\n",
      "Training loss 0.44 Training Acc  0.87 TN: 52706 FP: 12134 FN: 4767 TP: 60073\n",
      "Sensitivity: 0.93 Specificity: 0.81 Precision: 0.83 F1: 0.88 MACC 0.87\n",
      "TN: 64 FP: 82 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    15\n",
      "learning rate  0.00048767497911552944\n",
      "Training loss 0.44 Training Acc  0.87 TN: 52893 FP: 11947 FN: 4690 TP: 60150\n",
      "Sensitivity: 0.93 Specificity: 0.82 Precision: 0.83 F1: 0.88 MACC 0.87\n",
      "TN: 64 FP: 82 FN: 4 TP: 134\n",
      "Sensitivity: 0.97 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.58 Validation Acc  0.73\n",
      "EPOCH    16\n",
      "learning rate  0.00046329123015975297\n",
      "Training loss 0.44 Training Acc  0.87 TN: 52908 FP: 11932 FN: 4427 TP: 60413\n",
      "Sensitivity: 0.93 Specificity: 0.82 Precision: 0.84 F1: 0.88 MACC 0.87\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    17\n",
      "learning rate  0.0004401266686517653\n",
      "Training loss 0.44 Training Acc  0.87 TN: 52974 FP: 11866 FN: 4454 TP: 60386\n",
      "Sensitivity: 0.93 Specificity: 0.82 Precision: 0.84 F1: 0.88 MACC 0.87\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    18\n",
      "learning rate  0.00041812033521917703\n",
      "Training loss 0.44 Training Acc  0.87 TN: 53053 FP: 11787 FN: 4589 TP: 60251\n",
      "Sensitivity: 0.93 Specificity: 0.82 Precision: 0.84 F1: 0.88 MACC 0.87\n",
      "TN: 63 FP: 83 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    19\n",
      "learning rate  0.00039721431845821814\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53100 FP: 11740 FN: 4339 TP: 60501\n",
      "Sensitivity: 0.93 Specificity: 0.82 Precision: 0.84 F1: 0.88 MACC 0.88\n",
      "TN: 65 FP: 81 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.45 Precision: 0.63 F1: 0.77 MACC 0.72\n",
      "Validation loss 0.58 Validation Acc  0.73\n",
      "EPOCH    20\n",
      "learning rate  0.0003773536025353072\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53141 FP: 11699 FN: 4326 TP: 60514\n",
      "Sensitivity: 0.93 Specificity: 0.82 Precision: 0.84 F1: 0.88 MACC 0.88\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    21\n",
      "learning rate  0.0003584859224085418\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53196 FP: 11644 FN: 4287 TP: 60553\n",
      "Sensitivity: 0.93 Specificity: 0.82 Precision: 0.84 F1: 0.88 MACC 0.88\n",
      "TN: 65 FP: 81 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.45 Precision: 0.63 F1: 0.77 MACC 0.72\n",
      "Validation loss 0.58 Validation Acc  0.73\n",
      "EPOCH    22\n",
      "learning rate  0.0003405616262881147\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53296 FP: 11544 FN: 4240 TP: 60600\n",
      "Sensitivity: 0.93 Specificity: 0.82 Precision: 0.84 F1: 0.88 MACC 0.88\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    23\n",
      "learning rate  0.00032353354497370894\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53294 FP: 11546 FN: 4123 TP: 60717\n",
      "Sensitivity: 0.94 Specificity: 0.82 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 64 FP: 82 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    24\n",
      "learning rate  0.00030735686772502346\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53263 FP: 11577 FN: 4157 TP: 60683\n",
      "Sensitivity: 0.94 Specificity: 0.82 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    25\n",
      "learning rate  0.00029198902433877225\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53354 FP: 11486 FN: 4065 TP: 60775\n",
      "Sensitivity: 0.94 Specificity: 0.82 Precision: 0.84 F1: 0.89 MACC 0.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN: 66 FP: 80 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.45 Precision: 0.63 F1: 0.76 MACC 0.72\n",
      "Validation loss 0.58 Validation Acc  0.73\n",
      "EPOCH    26\n",
      "learning rate  0.00027738957312183364\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53303 FP: 11537 FN: 4156 TP: 60684\n",
      "Sensitivity: 0.94 Specificity: 0.82 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 65 FP: 81 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.45 Precision: 0.63 F1: 0.77 MACC 0.72\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    27\n",
      "learning rate  0.0002635200944657419\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53423 FP: 11417 FN: 4070 TP: 60770\n",
      "Sensitivity: 0.94 Specificity: 0.82 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 65 FP: 81 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.45 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    28\n",
      "learning rate  0.0002503440897424548\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53291 FP: 11549 FN: 3898 TP: 60942\n",
      "Sensitivity: 0.94 Specificity: 0.82 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    29\n",
      "learning rate  0.00023782688525533205\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53538 FP: 11302 FN: 3967 TP: 60873\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    30\n",
      "learning rate  0.00022593554099256544\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53404 FP: 11436 FN: 3924 TP: 60916\n",
      "Sensitivity: 0.94 Specificity: 0.82 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 59 FP: 87 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.40 Precision: 0.61 F1: 0.75 MACC 0.69\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    31\n",
      "learning rate  0.00021463876394293716\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53500 FP: 11340 FN: 3906 TP: 60934\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 63 FP: 83 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    32\n",
      "learning rate  0.0002039068257457903\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53447 FP: 11393 FN: 3962 TP: 60878\n",
      "Sensitivity: 0.94 Specificity: 0.82 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    33\n",
      "learning rate  0.00019371148445850077\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53567 FP: 11273 FN: 3907 TP: 60933\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    34\n",
      "learning rate  0.00018402591023557573\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53446 FP: 11394 FN: 3883 TP: 60957\n",
      "Sensitivity: 0.94 Specificity: 0.82 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    35\n",
      "learning rate  0.00017482461472379692\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53544 FP: 11296 FN: 3759 TP: 61081\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    36\n",
      "learning rate  0.00016608338398760707\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53534 FP: 11306 FN: 3741 TP: 61099\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    37\n",
      "learning rate  0.0001577792147882267\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53593 FP: 11247 FN: 3713 TP: 61127\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    38\n",
      "learning rate  0.00014989025404881537\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53599 FP: 11241 FN: 3704 TP: 61136\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 59 FP: 87 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.40 Precision: 0.61 F1: 0.75 MACC 0.69\n",
      "Validation loss 0.60 Validation Acc  0.72\n",
      "EPOCH    39\n",
      "learning rate  0.00014239574134637458\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53582 FP: 11258 FN: 3739 TP: 61101\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    40\n",
      "learning rate  0.00013527595427905584\n",
      "Training loss 0.43 Training Acc  0.89 TN: 53651 FP: 11189 FN: 3695 TP: 61145\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    41\n",
      "learning rate  0.00012851215656510304\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53646 FP: 11194 FN: 3758 TP: 61082\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.88\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    42\n",
      "learning rate  0.00012208654873684788\n",
      "Training loss 0.43 Training Acc  0.89 TN: 53615 FP: 11225 FN: 3670 TP: 61170\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.89\n",
      "TN: 65 FP: 81 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.45 Precision: 0.63 F1: 0.77 MACC 0.72\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    43\n",
      "learning rate  0.00011598222130000548\n",
      "Training loss 0.43 Training Acc  0.89 TN: 53552 FP: 11288 FN: 3607 TP: 61233\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    44\n",
      "learning rate  0.00011018311023500519\n",
      "Training loss 0.43 Training Acc  0.89 TN: 53618 FP: 11222 FN: 3685 TP: 61155\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    45\n",
      "learning rate  0.00010467395472325493\n",
      "Training loss 0.43 Training Acc  0.89 TN: 53687 FP: 11153 FN: 3740 TP: 61100\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    46\n",
      "learning rate  9.944025698709218e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53744 FP: 11096 FN: 3706 TP: 61134\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    47\n",
      "learning rate  9.446824413773756e-05\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53616 FP: 11224 FN: 3698 TP: 61142\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    48\n",
      "learning rate  8.974483193085068e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53713 FP: 11127 FN: 3643 TP: 61197\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    49\n",
      "learning rate  8.525759033430814e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53662 FP: 11178 FN: 3629 TP: 61211\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    50\n",
      "learning rate  8.099471081759274e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.42 Training Acc  0.89 TN: 53720 FP: 11120 FN: 3623 TP: 61217\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    51\n",
      "learning rate  7.69449752767131e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53722 FP: 11118 FN: 3638 TP: 61202\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    52\n",
      "learning rate  7.309772651287744e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53651 FP: 11189 FN: 3622 TP: 61218\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    53\n",
      "learning rate  6.944284018723356e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53724 FP: 11116 FN: 3585 TP: 61255\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    54\n",
      "learning rate  6.597069817787189e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53741 FP: 11099 FN: 3563 TP: 61277\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    55\n",
      "learning rate  6.267216326897829e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53629 FP: 11211 FN: 3612 TP: 61228\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    56\n",
      "learning rate  5.953855510552937e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53769 FP: 11071 FN: 3623 TP: 61217\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    57\n",
      "learning rate  5.65616273502529e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53661 FP: 11179 FN: 3555 TP: 61285\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    58\n",
      "learning rate  5.373354598274025e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53716 FP: 11124 FN: 3553 TP: 61287\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    59\n",
      "learning rate  5.104686868360323e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53706 FP: 11134 FN: 3507 TP: 61333\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    60\n",
      "learning rate  4.849452524942307e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53670 FP: 11170 FN: 3602 TP: 61238\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    61\n",
      "learning rate  4.606979898695191e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53747 FP: 11093 FN: 3587 TP: 61253\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    62\n",
      "learning rate  4.376630903760431e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53688 FP: 11152 FN: 3547 TP: 61293\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    63\n",
      "learning rate  4.157799358572409e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53779 FP: 11061 FN: 3521 TP: 61319\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    64\n",
      "learning rate  3.9499093906437885e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53846 FP: 10994 FN: 3583 TP: 61257\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    65\n",
      "learning rate  3.752413921111599e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53674 FP: 11166 FN: 3472 TP: 61368\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    66\n",
      "learning rate  3.564793225056019e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53722 FP: 11118 FN: 3546 TP: 61294\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    67\n",
      "learning rate  3.3865535638032174e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53726 FP: 11114 FN: 3506 TP: 61334\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    68\n",
      "learning rate  3.2172258856130564e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53727 FP: 11113 FN: 3566 TP: 61274\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    69\n",
      "learning rate  3.056364591332403e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53723 FP: 11117 FN: 3500 TP: 61340\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    70\n",
      "learning rate  2.903546361765783e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53778 FP: 11062 FN: 3486 TP: 61354\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    71\n",
      "learning rate  2.758369043677494e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53824 FP: 11016 FN: 3481 TP: 61359\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    72\n",
      "learning rate  2.620450591493619e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53759 FP: 11081 FN: 3600 TP: 61240\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    73\n",
      "learning rate  2.489428061918938e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53713 FP: 11127 FN: 3512 TP: 61328\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    74\n",
      "learning rate  2.364956658822991e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53776 FP: 11064 FN: 3502 TP: 61338\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    75\n",
      "learning rate  2.2467088258818413e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53791 FP: 11049 FN: 3555 TP: 61285\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    76\n",
      "learning rate  2.134373384587749e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53766 FP: 11074 FN: 3498 TP: 61342\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    77\n",
      "learning rate  2.0276547153583614e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53762 FP: 11078 FN: 3450 TP: 61390\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    78\n",
      "learning rate  1.9262719795904432e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53838 FP: 11002 FN: 3642 TP: 61198\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    79\n",
      "learning rate  1.829958380610921e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53771 FP: 11069 FN: 3468 TP: 61372\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    80\n",
      "learning rate  1.738460461580375e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53803 FP: 11037 FN: 3493 TP: 61347\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    81\n",
      "learning rate  1.6515374385013564e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53821 FP: 11019 FN: 3542 TP: 61298\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    82\n",
      "learning rate  1.5689605665762886e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53823 FP: 11017 FN: 3516 TP: 61324\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    83\n",
      "learning rate  1.490512538247474e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53814 FP: 11026 FN: 3508 TP: 61332\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    84\n",
      "learning rate  1.4159869113351003e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53779 FP: 11061 FN: 3489 TP: 61351\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    85\n",
      "learning rate  1.3451875657683452e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53772 FP: 11068 FN: 3489 TP: 61351\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    86\n",
      "learning rate  1.277928187479928e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53820 FP: 11020 FN: 3493 TP: 61347\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    87\n",
      "learning rate  1.2140317781059316e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53727 FP: 11113 FN: 3606 TP: 61234\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    88\n",
      "learning rate  1.153330189200635e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53759 FP: 11081 FN: 3456 TP: 61384\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    89\n",
      "learning rate  1.0956636797406032e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53810 FP: 11030 FN: 3529 TP: 61311\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    90\n",
      "learning rate  1.0408804957535729e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53776 FP: 11064 FN: 3462 TP: 61378\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    91\n",
      "learning rate  9.888364709658941e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53830 FP: 11010 FN: 3521 TP: 61319\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    92\n",
      "learning rate  9.393946474175994e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53839 FP: 11001 FN: 3422 TP: 61418\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    93\n",
      "learning rate  8.924249150467194e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53798 FP: 11042 FN: 3464 TP: 61376\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    94\n",
      "learning rate  8.478036692943835e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53778 FP: 11062 FN: 3473 TP: 61367\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    95\n",
      "learning rate  8.054134858296643e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53803 FP: 11037 FN: 3419 TP: 61421\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    96\n",
      "learning rate  7.65142811538181e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53787 FP: 11053 FN: 3476 TP: 61364\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    97\n",
      "learning rate  7.26885670961272e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53803 FP: 11037 FN: 3530 TP: 61310\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    98\n",
      "learning rate  6.905413874132084e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53797 FP: 11043 FN: 3444 TP: 61396\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    99\n",
      "learning rate  6.5601431804254795e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.42 Training Acc  0.89 TN: 53733 FP: 11107 FN: 3456 TP: 61384\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    100\n",
      "learning rate  6.232136021404205e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53842 FP: 10998 FN: 3462 TP: 61378\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    101\n",
      "learning rate  5.920529220333994e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53823 FP: 11017 FN: 3531 TP: 61309\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    102\n",
      "learning rate  5.624502759317295e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53778 FP: 11062 FN: 3428 TP: 61412\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    103\n",
      "learning rate  5.34327762135143e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53799 FP: 11041 FN: 3479 TP: 61361\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    104\n",
      "learning rate  5.076113740283858e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53856 FP: 10984 FN: 3557 TP: 61283\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    105\n",
      "learning rate  4.8223080532696655e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53740 FP: 11100 FN: 3381 TP: 61459\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    106\n",
      "learning rate  4.581192650606182e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53855 FP: 10985 FN: 3402 TP: 61438\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    107\n",
      "learning rate  4.3521330180758725e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53835 FP: 11005 FN: 3414 TP: 61426\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    108\n",
      "learning rate  4.1345263671720786e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53714 FP: 11126 FN: 3457 TP: 61383\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    109\n",
      "learning rate  3.927800048813474e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53807 FP: 11033 FN: 3453 TP: 61387\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    110\n",
      "learning rate  3.7314100463728006e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53725 FP: 11115 FN: 3411 TP: 61429\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    111\n",
      "learning rate  3.5448395440541604e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53838 FP: 11002 FN: 3529 TP: 61311\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    112\n",
      "learning rate  3.3675975668514524e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53795 FP: 11045 FN: 3424 TP: 61416\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    113\n",
      "learning rate  3.1992176885088796e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53707 FP: 11133 FN: 3445 TP: 61395\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    114\n",
      "learning rate  3.0392568040834356e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53756 FP: 11084 FN: 3583 TP: 61257\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    115\n",
      "learning rate  2.8872939638792635e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53785 FP: 11055 FN: 3441 TP: 61399\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    116\n",
      "learning rate  2.7429292656853003e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53753 FP: 11087 FN: 3495 TP: 61345\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    117\n",
      "learning rate  2.605782802401035e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53713 FP: 11127 FN: 3393 TP: 61447\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 59 FP: 87 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.40 Precision: 0.61 F1: 0.75 MACC 0.69\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    118\n",
      "learning rate  2.475493662280983e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53830 FP: 11010 FN: 3515 TP: 61325\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    119\n",
      "learning rate  2.351718979166934e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53894 FP: 10946 FN: 3488 TP: 61352\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    120\n",
      "learning rate  2.234133030208587e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53897 FP: 10943 FN: 3451 TP: 61389\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    121\n",
      "learning rate  2.1224263786981576e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53799 FP: 11041 FN: 3495 TP: 61345\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    122\n",
      "learning rate  2.0163050597632494e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53792 FP: 11048 FN: 3526 TP: 61314\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    123\n",
      "learning rate  1.915489806775087e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53903 FP: 10937 FN: 3424 TP: 61416\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    124\n",
      "learning rate  1.8197153164363325e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53682 FP: 11158 FN: 3488 TP: 61352\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    125\n",
      "learning rate  1.7287295506145157e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53835 FP: 11005 FN: 3497 TP: 61343\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    126\n",
      "learning rate  1.6422930730837899e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53814 FP: 11026 FN: 3417 TP: 61423\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    127\n",
      "learning rate  1.5601784194296004e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53839 FP: 11001 FN: 3462 TP: 61378\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    128\n",
      "learning rate  1.4821694984581202e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53868 FP: 10972 FN: 3476 TP: 61364\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    129\n",
      "learning rate  1.4080610235352142e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53869 FP: 10971 FN: 3499 TP: 61341\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    130\n",
      "learning rate  1.3376579723584535e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53813 FP: 11027 FN: 3519 TP: 61321\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 65 FP: 81 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.45 Precision: 0.63 F1: 0.77 MACC 0.72\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    131\n",
      "learning rate  1.2707750737405307e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53824 FP: 11016 FN: 3516 TP: 61324\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    132\n",
      "learning rate  1.2072363200535042e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53816 FP: 11024 FN: 3493 TP: 61347\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    133\n",
      "learning rate  1.146874504050829e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53871 FP: 10969 FN: 3406 TP: 61434\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    134\n",
      "learning rate  1.0895307788482876e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53781 FP: 11059 FN: 3526 TP: 61314\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    135\n",
      "learning rate  1.0350542399058731e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53803 FP: 11037 FN: 3498 TP: 61342\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    136\n",
      "learning rate  9.833015279105794e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53829 FP: 11011 FN: 3502 TP: 61338\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    137\n",
      "learning rate  9.341364515150503e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53751 FP: 11089 FN: 3467 TP: 61373\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    138\n",
      "learning rate  8.874296289392978e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53869 FP: 10971 FN: 3469 TP: 61371\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    139\n",
      "learning rate  8.430581474923329e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53857 FP: 10983 FN: 3465 TP: 61375\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    140\n",
      "learning rate  8.009052401177162e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53772 FP: 11068 FN: 3497 TP: 61343\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    141\n",
      "learning rate  7.608599781118303e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53902 FP: 10938 FN: 3422 TP: 61418\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    142\n",
      "learning rate  7.228169792062388e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53828 FP: 11012 FN: 3489 TP: 61351\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    143\n",
      "learning rate  6.866761302459269e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53799 FP: 11041 FN: 3489 TP: 61351\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 65 FP: 81 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.45 Precision: 0.63 F1: 0.77 MACC 0.72\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    144\n",
      "learning rate  6.523423237336305e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53795 FP: 11045 FN: 3447 TP: 61393\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 65 FP: 81 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.45 Precision: 0.63 F1: 0.77 MACC 0.72\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    145\n",
      "learning rate  6.197252075469489e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53808 FP: 11032 FN: 3457 TP: 61383\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    146\n",
      "learning rate  5.887389471696014e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53804 FP: 11036 FN: 3526 TP: 61314\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    147\n",
      "learning rate  5.593019998111213e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53786 FP: 11054 FN: 3559 TP: 61281\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    148\n",
      "learning rate  5.313368998205652e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.42 Training Acc  0.89 TN: 53814 FP: 11026 FN: 3444 TP: 61396\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    149\n",
      "learning rate  5.047700548295369e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53834 FP: 11006 FN: 3470 TP: 61370\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    150\n",
      "learning rate  4.7953155208806e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53770 FP: 11070 FN: 3414 TP: 61426\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    151\n",
      "learning rate  4.55554974483657e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53850 FP: 10990 FN: 3449 TP: 61391\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    152\n",
      "learning rate  4.327772257594741e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53864 FP: 10976 FN: 3528 TP: 61312\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    153\n",
      "learning rate  4.111383644715004e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53805 FP: 11035 FN: 3506 TP: 61334\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    154\n",
      "learning rate  3.905814462479254e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53854 FP: 10986 FN: 3503 TP: 61337\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    155\n",
      "learning rate  3.710523739355291e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53889 FP: 10951 FN: 3433 TP: 61407\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    156\n",
      "learning rate  3.524997552387526e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53806 FP: 11034 FN: 3522 TP: 61318\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    157\n",
      "learning rate  3.34874767476815e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53799 FP: 11041 FN: 3495 TP: 61345\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    158\n",
      "learning rate  3.181310291029742e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53775 FP: 11065 FN: 3449 TP: 61391\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    159\n",
      "learning rate  3.0222447764782547e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53876 FP: 10964 FN: 3547 TP: 61293\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    160\n",
      "learning rate  2.8711325376543416e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53774 FP: 11066 FN: 3497 TP: 61343\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    161\n",
      "learning rate  2.7275759107716244e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53791 FP: 11049 FN: 3488 TP: 61352\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    162\n",
      "learning rate  2.591197115233043e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53827 FP: 11013 FN: 3504 TP: 61336\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    163\n",
      "learning rate  2.4616372594713905e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53843 FP: 10997 FN: 3492 TP: 61348\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    164\n",
      "learning rate  2.3385553964978208e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53873 FP: 10967 FN: 3533 TP: 61307\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    165\n",
      "learning rate  2.2216276266729297e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53883 FP: 10957 FN: 3505 TP: 61335\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    166\n",
      "learning rate  2.110546245339283e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53878 FP: 10962 FN: 3467 TP: 61373\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    167\n",
      "learning rate  2.0050189330723186e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53875 FP: 10965 FN: 3453 TP: 61387\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    168\n",
      "learning rate  1.9047679864187027e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53879 FP: 10961 FN: 3400 TP: 61440\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    169\n",
      "learning rate  1.8095295870977674e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53778 FP: 11062 FN: 3433 TP: 61407\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    170\n",
      "learning rate  1.719053107742879e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53813 FP: 11027 FN: 3442 TP: 61398\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    171\n",
      "learning rate  1.633100452355735e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53850 FP: 10990 FN: 3481 TP: 61359\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    172\n",
      "learning rate  1.5514454297379483e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53778 FP: 11062 FN: 3452 TP: 61388\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    173\n",
      "learning rate  1.473873158251051e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53792 FP: 11048 FN: 3420 TP: 61420\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    174\n",
      "learning rate  1.4001795003384983e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53878 FP: 10962 FN: 3486 TP: 61354\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    175\n",
      "learning rate  1.3301705253215733e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53833 FP: 11007 FN: 3487 TP: 61353\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    176\n",
      "learning rate  1.2636619990554947e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53834 FP: 11006 FN: 3514 TP: 61326\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    177\n",
      "learning rate  1.20047889910272e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53806 FP: 11034 FN: 3468 TP: 61372\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    178\n",
      "learning rate  1.1404549541475839e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53721 FP: 11119 FN: 3499 TP: 61341\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    179\n",
      "learning rate  1.0834322064402047e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53753 FP: 11087 FN: 3489 TP: 61351\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    180\n",
      "learning rate  1.0292605961181944e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53792 FP: 11048 FN: 3489 TP: 61351\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    181\n",
      "learning rate  9.777975663122845e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53789 FP: 11051 FN: 3498 TP: 61342\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    182\n",
      "learning rate  9.289076879966702e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53759 FP: 11081 FN: 3426 TP: 61414\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    183\n",
      "learning rate  8.824623035968367e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53811 FP: 11029 FN: 3414 TP: 61426\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    184\n",
      "learning rate  8.383391884169949e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53818 FP: 11022 FN: 3411 TP: 61429\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    185\n",
      "learning rate  7.964222289961451e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53781 FP: 11059 FN: 3524 TP: 61316\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    186\n",
      "learning rate  7.566011175463377e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53776 FP: 11064 FN: 3447 TP: 61393\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    187\n",
      "learning rate  7.187710616690207e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53787 FP: 11053 FN: 3489 TP: 61351\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    188\n",
      "learning rate  6.828325085855697e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53840 FP: 11000 FN: 3479 TP: 61361\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    189\n",
      "learning rate  6.486908831562912e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53716 FP: 11124 FN: 3460 TP: 61380\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    190\n",
      "learning rate  6.162563389984765e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53839 FP: 11001 FN: 3539 TP: 61301\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.42 Precision: 0.61 F1: 0.75 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    191\n",
      "learning rate  5.854435220485527e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53824 FP: 11016 FN: 3500 TP: 61340\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    192\n",
      "learning rate  5.56171345946125e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53807 FP: 11033 FN: 3447 TP: 61393\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    193\n",
      "learning rate  5.2836277864881873e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53759 FP: 11081 FN: 3359 TP: 61481\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    194\n",
      "learning rate  5.019446397163778e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53814 FP: 11026 FN: 3460 TP: 61380\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    195\n",
      "learning rate  4.7684740773055885e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53832 FP: 11008 FN: 3493 TP: 61347\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    196\n",
      "learning rate  4.530050373440309e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53796 FP: 11044 FN: 3465 TP: 61375\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    197\n",
      "learning rate  4.3035478547682935e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.42 Training Acc  0.89 TN: 53791 FP: 11049 FN: 3456 TP: 61384\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    198\n",
      "learning rate  4.0883704620298786e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53871 FP: 10969 FN: 3475 TP: 61365\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    199\n",
      "learning rate  3.883951938928385e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53816 FP: 11024 FN: 3506 TP: 61334\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    200\n",
      "learning rate  3.689754341981965e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53828 FP: 11012 FN: 3447 TP: 61393\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    201\n",
      "learning rate  3.505266624882867e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53804 FP: 11036 FN: 3452 TP: 61388\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    202\n",
      "learning rate  3.330003293638723e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53867 FP: 10973 FN: 3461 TP: 61379\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    203\n",
      "learning rate  3.1635031289567866e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53809 FP: 11031 FN: 3484 TP: 61356\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    204\n",
      "learning rate  3.005327972508947e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53788 FP: 11052 FN: 3468 TP: 61372\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    205\n",
      "learning rate  2.8550615738834995e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53818 FP: 11022 FN: 3536 TP: 61304\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    206\n",
      "learning rate  2.7123084951893245e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53740 FP: 11100 FN: 3444 TP: 61396\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    207\n",
      "learning rate  2.576693070429858e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53823 FP: 11017 FN: 3454 TP: 61386\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    208\n",
      "learning rate  2.447858416908365e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53791 FP: 11049 FN: 3481 TP: 61359\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    209\n",
      "learning rate  2.3254654960629467e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53831 FP: 11009 FN: 3482 TP: 61358\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    210\n",
      "learning rate  2.2091922212597992e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53711 FP: 11129 FN: 3431 TP: 61409\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    211\n",
      "learning rate  2.0987326101968092e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53797 FP: 11043 FN: 3486 TP: 61354\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    212\n",
      "learning rate  1.9937959796869687e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53803 FP: 11037 FN: 3507 TP: 61333\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    213\n",
      "learning rate  1.8941061807026202e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53794 FP: 11046 FN: 3414 TP: 61426\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    214\n",
      "learning rate  1.799400871667489e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53778 FP: 11062 FN: 3495 TP: 61345\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    215\n",
      "learning rate  1.7094308280841145e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53812 FP: 11028 FN: 3431 TP: 61409\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    216\n",
      "learning rate  1.6239592866799087e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53835 FP: 11005 FN: 3433 TP: 61407\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    217\n",
      "learning rate  1.5427613223459133e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53782 FP: 11058 FN: 3509 TP: 61331\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    218\n",
      "learning rate  1.4656232562286176e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53846 FP: 10994 FN: 3492 TP: 61348\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    219\n",
      "learning rate  1.3923420934171867e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53763 FP: 11077 FN: 3474 TP: 61366\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    220\n",
      "learning rate  1.3227249887463272e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53849 FP: 10991 FN: 3499 TP: 61341\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    221\n",
      "learning rate  1.2565887393090107e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53786 FP: 11054 FN: 3494 TP: 61346\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    222\n",
      "learning rate  1.1937593023435602e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53782 FP: 11058 FN: 3439 TP: 61401\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    223\n",
      "learning rate  1.1340713372263822e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53801 FP: 11039 FN: 3487 TP: 61353\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    224\n",
      "learning rate  1.077367770365063e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53813 FP: 11027 FN: 3458 TP: 61382\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    225\n",
      "learning rate  1.0234993818468098e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53796 FP: 11044 FN: 3464 TP: 61376\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    226\n",
      "learning rate  9.723244127544693e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53785 FP: 11055 FN: 3452 TP: 61388\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    227\n",
      "learning rate  9.237081921167457e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53789 FP: 11051 FN: 3477 TP: 61363\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    228\n",
      "learning rate  8.775227825109085e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53884 FP: 10956 FN: 3453 TP: 61387\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    229\n",
      "learning rate  8.33646643385363e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53803 FP: 11037 FN: 3429 TP: 61411\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    230\n",
      "learning rate  7.919643112160947e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53809 FP: 11031 FN: 3458 TP: 61382\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    231\n",
      "learning rate  7.5236609565529e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53809 FP: 11031 FN: 3498 TP: 61342\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    232\n",
      "learning rate  7.147477908725255e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53783 FP: 11057 FN: 3458 TP: 61382\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    233\n",
      "learning rate  6.790104013288992e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53804 FP: 11036 FN: 3495 TP: 61345\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    234\n",
      "learning rate  6.450598812624543e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53805 FP: 11035 FN: 3554 TP: 61286\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    235\n",
      "learning rate  6.1280688719933155e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53746 FP: 11094 FN: 3506 TP: 61334\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    236\n",
      "learning rate  5.8216654283936495e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53800 FP: 11040 FN: 3504 TP: 61336\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    237\n",
      "learning rate  5.530582156973966e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53835 FP: 11005 FN: 3423 TP: 61417\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    238\n",
      "learning rate  5.254053049125268e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53824 FP: 11016 FN: 3479 TP: 61361\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    239\n",
      "learning rate  4.991350396669004e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53798 FP: 11042 FN: 3465 TP: 61375\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    240\n",
      "learning rate  4.741782876835554e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53803 FP: 11037 FN: 3460 TP: 61380\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    241\n",
      "learning rate  4.504693732993776e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53820 FP: 11020 FN: 3499 TP: 61341\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    242\n",
      "learning rate  4.279459046344087e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53810 FP: 11030 FN: 3495 TP: 61345\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    243\n",
      "learning rate  4.065486094026883e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53846 FP: 10994 FN: 3476 TP: 61364\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    244\n",
      "learning rate  3.862211789325538e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53848 FP: 10992 FN: 3510 TP: 61330\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    245\n",
      "learning rate  3.6691011998592615e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53795 FP: 11045 FN: 3454 TP: 61386\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    246\n",
      "learning rate  3.4856461398662983e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.42 Training Acc  0.89 TN: 53857 FP: 10983 FN: 3450 TP: 61390\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    247\n",
      "learning rate  3.311363832872983e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53793 FP: 11047 FN: 3472 TP: 61368\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    248\n",
      "learning rate  3.145795641229334e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53846 FP: 10994 FN: 3527 TP: 61313\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    249\n",
      "learning rate  2.9885058591678673e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53846 FP: 10994 FN: 3498 TP: 61342\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    250\n",
      "learning rate  2.8390805662094737e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53833 FP: 11007 FN: 3506 TP: 61334\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    251\n",
      "learning rate  2.6971265378989998e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53883 FP: 10957 FN: 3502 TP: 61338\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    252\n",
      "learning rate  2.5622702110040496e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53829 FP: 11011 FN: 3543 TP: 61297\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    253\n",
      "learning rate  2.434156700453847e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53853 FP: 10987 FN: 3515 TP: 61325\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    254\n",
      "learning rate  2.3124488654311547e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53769 FP: 11071 FN: 3502 TP: 61338\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    255\n",
      "learning rate  2.196826422159597e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53773 FP: 11067 FN: 3429 TP: 61411\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    256\n",
      "learning rate  2.0869851010516168e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53793 FP: 11047 FN: 3372 TP: 61468\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    257\n",
      "learning rate  1.9826358459990357e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53855 FP: 10985 FN: 3491 TP: 61349\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    258\n",
      "learning rate  1.883504053699084e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53799 FP: 11041 FN: 3455 TP: 61385\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    259\n",
      "learning rate  1.7893288510141295e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53761 FP: 11079 FN: 3449 TP: 61391\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    260\n",
      "learning rate  1.699862408463423e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53784 FP: 11056 FN: 3540 TP: 61300\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    261\n",
      "learning rate  1.6148692880402517e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53878 FP: 10962 FN: 3449 TP: 61391\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    262\n",
      "learning rate  1.534125823638239e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53795 FP: 11045 FN: 3502 TP: 61338\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    263\n",
      "learning rate  1.457419532456327e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53835 FP: 11005 FN: 3535 TP: 61305\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    264\n",
      "learning rate  1.3845485558335106e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53809 FP: 11031 FN: 3469 TP: 61371\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    265\n",
      "learning rate  1.315321128041835e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53793 FP: 11047 FN: 3474 TP: 61366\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    266\n",
      "learning rate  1.2495550716397432e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53841 FP: 10999 FN: 3516 TP: 61324\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    267\n",
      "learning rate  1.187077318057756e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53869 FP: 10971 FN: 3426 TP: 61414\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    268\n",
      "learning rate  1.1277234521548681e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53820 FP: 11020 FN: 3454 TP: 61386\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    269\n",
      "learning rate  1.0713372795471246e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53876 FP: 10964 FN: 3467 TP: 61373\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    270\n",
      "learning rate  1.0177704155697684e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53891 FP: 10949 FN: 3475 TP: 61365\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    271\n",
      "learning rate  9.6688189479128e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53855 FP: 10985 FN: 3508 TP: 61332\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    272\n",
      "learning rate  9.185378000517159e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53839 FP: 11001 FN: 3448 TP: 61392\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    273\n",
      "learning rate  8.7261091004913e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53839 FP: 11001 FN: 3530 TP: 61310\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    274\n",
      "learning rate  8.289803645466735e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53812 FP: 11028 FN: 3497 TP: 61343\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    275\n",
      "learning rate  7.875313463193398e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53796 FP: 11044 FN: 3559 TP: 61281\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 65 FP: 81 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.45 Precision: 0.63 F1: 0.77 MACC 0.72\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    276\n",
      "learning rate  7.481547790033728e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53855 FP: 10985 FN: 3520 TP: 61320\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    277\n",
      "learning rate  7.107470400532042e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53768 FP: 11072 FN: 3467 TP: 61373\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    278\n",
      "learning rate  6.752096880505439e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53763 FP: 11077 FN: 3545 TP: 61295\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    279\n",
      "learning rate  6.414492036480167e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53815 FP: 11025 FN: 3539 TP: 61301\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 65 FP: 81 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.45 Precision: 0.63 F1: 0.77 MACC 0.72\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    280\n",
      "learning rate  6.093767434656158e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53778 FP: 11062 FN: 3520 TP: 61320\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    281\n",
      "learning rate  5.78907906292335e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53873 FP: 10967 FN: 3418 TP: 61422\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    282\n",
      "learning rate  5.499625109777182e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53797 FP: 11043 FN: 3527 TP: 61313\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    283\n",
      "learning rate  5.224643854288323e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53794 FP: 11046 FN: 3499 TP: 61341\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    284\n",
      "learning rate  4.963411661573907e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53846 FP: 10994 FN: 3438 TP: 61402\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    285\n",
      "learning rate  4.715241078495211e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53771 FP: 11069 FN: 3430 TP: 61410\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    286\n",
      "learning rate  4.4794790245704503e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53798 FP: 11042 FN: 3452 TP: 61388\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    287\n",
      "learning rate  4.2555050733419276e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53807 FP: 11033 FN: 3404 TP: 61436\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    288\n",
      "learning rate  4.042729819674831e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53765 FP: 11075 FN: 3397 TP: 61443\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    289\n",
      "learning rate  3.840593328691089e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53809 FP: 11031 FN: 3418 TP: 61422\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    290\n",
      "learning rate  3.648563662256535e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53773 FP: 11067 FN: 3509 TP: 61331\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    291\n",
      "learning rate  3.466135479143708e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53829 FP: 11011 FN: 3507 TP: 61333\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    292\n",
      "learning rate  3.2928287051865226e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53778 FP: 11062 FN: 3492 TP: 61348\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    293\n",
      "learning rate  3.1281872699271963e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53853 FP: 10987 FN: 3507 TP: 61333\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    294\n",
      "learning rate  2.9717779064308364e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53774 FP: 11066 FN: 3399 TP: 61441\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    295\n",
      "learning rate  2.8231890111092944e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.42 Training Acc  0.89 TN: 53773 FP: 11067 FN: 3524 TP: 61316\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    296\n",
      "learning rate  2.6820295605538294e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53799 FP: 11041 FN: 3482 TP: 61358\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    297\n",
      "learning rate  2.547928082526138e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53802 FP: 11038 FN: 3477 TP: 61363\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    298\n",
      "learning rate  2.4205316783998307e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53817 FP: 11023 FN: 3468 TP: 61372\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    299\n",
      "learning rate  2.299505094479839e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53708 FP: 11132 FN: 3472 TP: 61368\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    300\n",
      "learning rate  2.184529839755847e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53796 FP: 11044 FN: 3469 TP: 61371\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    301\n",
      "learning rate  2.0753033477680545e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53803 FP: 11037 FN: 3532 TP: 61308\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    302\n",
      "learning rate  1.9715381803796517e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53779 FP: 11061 FN: 3477 TP: 61363\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    303\n",
      "learning rate  1.872961271360669e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53838 FP: 11002 FN: 3551 TP: 61289\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    304\n",
      "learning rate  1.7793132077926355e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53812 FP: 11028 FN: 3516 TP: 61324\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    305\n",
      "learning rate  1.6903475474030036e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53778 FP: 11062 FN: 3532 TP: 61308\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    306\n",
      "learning rate  1.6058301700328533e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53828 FP: 11012 FN: 3467 TP: 61373\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    307\n",
      "learning rate  1.5255386615312106e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53767 FP: 11073 FN: 3499 TP: 61341\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    308\n",
      "learning rate  1.44926172845465e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53799 FP: 11041 FN: 3527 TP: 61313\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    309\n",
      "learning rate  1.3767986420319173e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53776 FP: 11064 FN: 3454 TP: 61386\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    310\n",
      "learning rate  1.3079587099303215e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53829 FP: 11011 FN: 3482 TP: 61358\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    311\n",
      "learning rate  1.2425607744338054e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53854 FP: 10986 FN: 3527 TP: 61313\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    312\n",
      "learning rate  1.1804327357121152e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53856 FP: 10984 FN: 3482 TP: 61358\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    313\n",
      "learning rate  1.1214110989265094e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53797 FP: 11043 FN: 3480 TP: 61360\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    314\n",
      "learning rate  1.0653405439801838e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53766 FP: 11074 FN: 3448 TP: 61392\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    315\n",
      "learning rate  1.0120735167811745e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53802 FP: 11038 FN: 3514 TP: 61326\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    316\n",
      "learning rate  9.614698409421157e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53880 FP: 10960 FN: 3461 TP: 61379\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    317\n",
      "learning rate  9.133963488950098e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53838 FP: 11002 FN: 3536 TP: 61304\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    318\n",
      "learning rate  8.677265314502593e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53794 FP: 11046 FN: 3452 TP: 61388\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    319\n",
      "learning rate  8.243402048777463e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53818 FP: 11022 FN: 3534 TP: 61306\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    320\n",
      "learning rate  7.83123194633859e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53819 FP: 11021 FN: 3497 TP: 61343\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    321\n",
      "learning rate  7.43967034902166e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53804 FP: 11036 FN: 3457 TP: 61383\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    322\n",
      "learning rate  7.067686831570576e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53838 FP: 11002 FN: 3401 TP: 61439\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    323\n",
      "learning rate  6.714302489992046e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53776 FP: 11064 FN: 3451 TP: 61389\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    324\n",
      "learning rate  6.378587365492443e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53826 FP: 11014 FN: 3444 TP: 61396\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    325\n",
      "learning rate  6.059657997217821e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53823 FP: 11017 FN: 3360 TP: 61480\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    326\n",
      "learning rate  5.75667509735693e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53861 FP: 10979 FN: 3495 TP: 61345\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    327\n",
      "learning rate  5.468841342489083e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53913 FP: 10927 FN: 3507 TP: 61333\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    328\n",
      "learning rate  5.195399275364629e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53867 FP: 10973 FN: 3423 TP: 61417\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    329\n",
      "learning rate  4.935629311596397e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53771 FP: 11069 FN: 3504 TP: 61336\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    330\n",
      "learning rate  4.688847846016577e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53803 FP: 11037 FN: 3411 TP: 61429\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    331\n",
      "learning rate  4.4544054537157477e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53766 FP: 11074 FN: 3531 TP: 61309\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    332\n",
      "learning rate  4.23168518102996e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53867 FP: 10973 FN: 3492 TP: 61348\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    333\n",
      "learning rate  4.020100921978462e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53825 FP: 11015 FN: 3460 TP: 61380\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    334\n",
      "learning rate  3.8190958758795384e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53750 FP: 11090 FN: 3546 TP: 61294\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    335\n",
      "learning rate  3.628141082085561e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53794 FP: 11046 FN: 3529 TP: 61311\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    336\n",
      "learning rate  3.446734027981283e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53849 FP: 10991 FN: 3511 TP: 61329\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    337\n",
      "learning rate  3.274397326582219e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53830 FP: 11010 FN: 3442 TP: 61398\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    338\n",
      "learning rate  3.110677460253108e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53797 FP: 11043 FN: 3478 TP: 61362\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    339\n",
      "learning rate  2.955143587240452e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53942 FP: 10898 FN: 3463 TP: 61377\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    340\n",
      "learning rate  2.807386407878429e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53802 FP: 11038 FN: 3519 TP: 61321\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    341\n",
      "learning rate  2.6670170874845074e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53833 FP: 11007 FN: 3510 TP: 61330\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    342\n",
      "learning rate  2.533666233110282e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53803 FP: 11037 FN: 3509 TP: 61331\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    343\n",
      "learning rate  2.406982921454768e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53812 FP: 11028 FN: 3486 TP: 61354\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 59 FP: 87 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.40 Precision: 0.61 F1: 0.75 MACC 0.69\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    344\n",
      "learning rate  2.2866337753820293e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.42 Training Acc  0.89 TN: 53784 FP: 11056 FN: 3447 TP: 61393\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    345\n",
      "learning rate  2.1723020866129277e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53813 FP: 11027 FN: 3428 TP: 61412\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    346\n",
      "learning rate  2.0636869822822813e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53849 FP: 10991 FN: 3334 TP: 61506\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    347\n",
      "learning rate  1.9605026331681672e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53887 FP: 10953 FN: 3505 TP: 61335\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    348\n",
      "learning rate  1.8624775015097587e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53795 FP: 11045 FN: 3471 TP: 61369\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    349\n",
      "learning rate  1.7693536264342708e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53820 FP: 11020 FN: 3474 TP: 61366\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    350\n",
      "learning rate  1.680885945112557e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53783 FP: 11057 FN: 3489 TP: 61351\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    351\n",
      "learning rate  1.596841647856929e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53800 FP: 11040 FN: 3481 TP: 61359\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    352\n",
      "learning rate  1.5169995654640826e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53854 FP: 10986 FN: 3434 TP: 61406\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    353\n",
      "learning rate  1.4411495871908784e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53796 FP: 11044 FN: 3538 TP: 61302\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    354\n",
      "learning rate  1.3690921078313344e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53782 FP: 11058 FN: 3513 TP: 61327\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    355\n",
      "learning rate  1.3006375024397676e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53777 FP: 11063 FN: 3511 TP: 61329\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    356\n",
      "learning rate  1.2356056273177791e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53841 FP: 10999 FN: 3478 TP: 61362\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    357\n",
      "learning rate  1.1738253459518902e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53822 FP: 11018 FN: 3522 TP: 61318\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    358\n",
      "learning rate  1.1151340786542956e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53844 FP: 10996 FN: 3480 TP: 61360\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    359\n",
      "learning rate  1.0593773747215807e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53793 FP: 11047 FN: 3485 TP: 61355\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    360\n",
      "learning rate  1.0064085059855017e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53847 FP: 10993 FN: 3484 TP: 61356\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    361\n",
      "learning rate  9.560880806862266e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53817 FP: 11023 FN: 3451 TP: 61389\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    362\n",
      "learning rate  9.082836766519153e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53868 FP: 10972 FN: 3521 TP: 61319\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    363\n",
      "learning rate  8.628694928193194e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53871 FP: 10969 FN: 3505 TP: 61335\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    364\n",
      "learning rate  8.197260181783534e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53823 FP: 11017 FN: 3466 TP: 61374\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    365\n",
      "learning rate  7.787397172694357e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53750 FP: 11090 FN: 3427 TP: 61413\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    366\n",
      "learning rate  7.398027314059639e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53790 FP: 11050 FN: 3468 TP: 61372\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    367\n",
      "learning rate  7.028125948356657e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53846 FP: 10994 FN: 3524 TP: 61316\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    368\n",
      "learning rate  6.676719650938824e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53843 FP: 10997 FN: 3434 TP: 61406\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    369\n",
      "learning rate  6.3428836683918825e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53840 FP: 11000 FN: 3474 TP: 61366\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    370\n",
      "learning rate  6.025739484972288e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53810 FP: 11030 FN: 3471 TP: 61369\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    371\n",
      "learning rate  5.724452510723673e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53784 FP: 11056 FN: 3442 TP: 61398\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    372\n",
      "learning rate  5.43822988518749e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53807 FP: 11033 FN: 3494 TP: 61346\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    373\n",
      "learning rate  5.166318390928115e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53802 FP: 11038 FN: 3489 TP: 61351\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    374\n",
      "learning rate  4.908002471381709e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53826 FP: 11014 FN: 3483 TP: 61357\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    375\n",
      "learning rate  4.662602347812623e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53753 FP: 11087 FN: 3494 TP: 61346\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    376\n",
      "learning rate  4.429472230421991e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53817 FP: 11023 FN: 3503 TP: 61337\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    377\n",
      "learning rate  4.207998618900892e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53777 FP: 11063 FN: 3387 TP: 61453\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    378\n",
      "learning rate  3.997598687955847e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53798 FP: 11042 FN: 3495 TP: 61345\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    379\n",
      "learning rate  3.797718753558054e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53785 FP: 11055 FN: 3476 TP: 61364\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    380\n",
      "learning rate  3.607832815880151e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53816 FP: 11024 FN: 3475 TP: 61365\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    381\n",
      "learning rate  3.4274411750861436e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53807 FP: 11033 FN: 3497 TP: 61343\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    382\n",
      "learning rate  3.2560691163318364e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53862 FP: 10978 FN: 3477 TP: 61363\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    383\n",
      "learning rate  3.0932656605152445e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53803 FP: 11037 FN: 3500 TP: 61340\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    384\n",
      "learning rate  2.9386023774894822e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53795 FP: 11045 FN: 3449 TP: 61391\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    385\n",
      "learning rate  2.791672258615008e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53753 FP: 11087 FN: 3578 TP: 61262\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    386\n",
      "learning rate  2.6520886456842577e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53841 FP: 10999 FN: 3462 TP: 61378\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    387\n",
      "learning rate  2.519484213400045e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53823 FP: 11017 FN: 3476 TP: 61364\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    388\n",
      "learning rate  2.3935100027300425e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53822 FP: 11018 FN: 3482 TP: 61358\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    389\n",
      "learning rate  2.2738345025935404e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53754 FP: 11086 FN: 3464 TP: 61376\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    390\n",
      "learning rate  2.1601427774638634e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53807 FP: 11033 FN: 3439 TP: 61401\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    391\n",
      "learning rate  2.05213563859067e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53767 FP: 11073 FN: 3539 TP: 61301\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    392\n",
      "learning rate  1.9495288566611362e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53784 FP: 11056 FN: 3461 TP: 61379\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    393\n",
      "learning rate  1.8520524138280794e-12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.42 Training Acc  0.89 TN: 53835 FP: 11005 FN: 3428 TP: 61412\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    394\n",
      "learning rate  1.7594497931366753e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53860 FP: 10980 FN: 3483 TP: 61357\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    395\n",
      "learning rate  1.6714773034798415e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53745 FP: 11095 FN: 3442 TP: 61398\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    396\n",
      "learning rate  1.5879034383058493e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53891 FP: 10949 FN: 3549 TP: 61291\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    397\n",
      "learning rate  1.5085082663905568e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53832 FP: 11008 FN: 3460 TP: 61380\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    398\n",
      "learning rate  1.4330828530710288e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53741 FP: 11099 FN: 3511 TP: 61329\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    399\n",
      "learning rate  1.3614287104174773e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53792 FP: 11048 FN: 3394 TP: 61446\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    400\n",
      "learning rate  1.2933572748966033e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53836 FP: 11004 FN: 3488 TP: 61352\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "mfcc_gen.cuda()\n",
    "mfcc_gen.eval()\n",
    "epochs = 400\n",
    "# torch.save(model,os.path.join(path,'model.pt'))\n",
    "logger.on_train_begin()\n",
    "print(\"steps \", flow_source.steps_per_epoch)\n",
    "\n",
    "# with open(path+fold+'/model1.json', 'w') as outfile:\n",
    "#     outfile.write(str(mod))\n",
    "for e in range(epochs):\n",
    "    print(\"EPOCH   \",e+1)\n",
    "    print(\"learning rate \",optimizer.param_groups[0]['lr'])\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    acc = 0\n",
    "    y_pred = None\n",
    "    y_true = None\n",
    "    N = 0\n",
    "    for i in range(flow_source.steps_per_epoch+1):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x,y = flow_source.next()\n",
    "        if(str(class_criterion) in ['MSELoss()','BCELoss()']):\n",
    "            y = to_categorical(y,2)\n",
    "            y = y.astype(np.float32)\n",
    "        x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
    "        x = x.type(torch.FloatTensor).cuda()\n",
    "        holdx = x\n",
    "        x = mfcc_gen(x)\n",
    "        if(len(x)>1):\n",
    "            x = x[0]\n",
    "        hold = x\n",
    "        x = x.transpose(2,1)\n",
    "        x = x.unsqueeze(1)\n",
    "        # print(x.shape)\n",
    "        x,y = Variable(x),Variable(y)\n",
    "        \n",
    "#         y = y.cuda()\n",
    "        y = y.long().cuda()        \n",
    "        cls = model(x)\n",
    "        # class_loss = class_criterion(cls,torch.argmax(y,axis=1))        \n",
    "        loss = class_criterion(cls,y)\n",
    "        epoch_loss = epoch_loss + loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         if(str(class_criterion)in ['MSELoss()','BCELoss()']):\n",
    "#             y = torch.argmax(y,axis=1)\n",
    "        if(y_pred is None):\n",
    "            y_pred = torch.argmax(cls,axis=1)\n",
    "            y_true = y\n",
    "        else:\n",
    "            y_pred = torch.cat((y_pred,torch.argmax(cls,axis=1)))\n",
    "            y_true = torch.cat((y_true,y))\n",
    "    \n",
    "        acc = acc + torch.sum(y==torch.argmax(cls,axis=1))\n",
    "        N = N+len(y)\n",
    "    print(\"Training loss\", \"%.2f\"%(epoch_loss.item()/flow_source.steps_per_epoch),end=' ')\n",
    "    print(\"Training Acc \", \"%.2f\"%(acc.item()/N),end=' ')\n",
    "    trainLog(y_true,y_pred)\n",
    "    logger.logs['train_loss'] = (epoch_loss.item()/flow_source.steps_per_epoch)\n",
    "    logger.logs['train_acc'] = (acc.item()/N)\n",
    "    # Validate \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    acc = 0\n",
    "    N = 0\n",
    "    y_pred = None\n",
    "    y_true = None\n",
    "    with torch.no_grad():\n",
    "        start_idx = 0\n",
    "        for i,s in enumerate(val_parts):\n",
    "            if(s==0):\n",
    "                continue\n",
    "            x,y = x_val[start_idx:start_idx+s],y_val[start_idx:start_idx+s]\n",
    "            start_idx = start_idx+s\n",
    "            \n",
    "            if(str(class_criterion) in ['MSELoss()','BCELoss()']):\n",
    "                y = to_categorical(y,2)\n",
    "                y = y.astype(np.float32)\n",
    "            \n",
    "            x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
    "            x = x.type(torch.FloatTensor).cuda()\n",
    "            holdvalx = x\n",
    "            x = mfcc_gen(x)\n",
    "            if(len(x)>1):\n",
    "                x = x[0]\n",
    "            holdval = x\n",
    "            x = x.transpose(2,1)\n",
    "            x = x.unsqueeze(1)\n",
    "            x,y = Variable(x),Variable(y)\n",
    "            #x = x.reshape(x.shape[0],1,x.shape[1],x.shape[2])\n",
    "            \n",
    "#             y = y.cuda()\n",
    "            y = y.long().cuda()\n",
    "            cls= model(x)\n",
    "            # val_class_loss = class_criterion(cls,torch.argmax(y,axis=1))\n",
    "            val_class_loss = class_criterion(cls,y)\n",
    "            \n",
    "            if(str(class_criterion) in ['MSELoss()','BCELoss()']):\n",
    "                y = torch.argmax(y,axis=1)\n",
    "            \n",
    "            acc = acc + torch.sum(y==torch.argmax(cls,axis=1))\n",
    "            N = N+len(y)\n",
    "            epoch_loss = epoch_loss + val_class_loss\n",
    "            if(y_pred is None):\n",
    "                y_pred = torch.argmax(cls,axis=1)\n",
    "                y_true = y\n",
    "            else:\n",
    "                y_pred = torch.cat((y_pred,torch.argmax(cls,axis=1)))\n",
    "                y_true = torch.cat((y_true,y))\n",
    "        Macc,sensitivity,specificity,precision,F1 = log_macc(y_pred,y_true,val_parts)\n",
    "        \n",
    "        print(\"Validation loss\", \"%.2f\"%(epoch_loss.item()/len(val_parts)),end=' ')\n",
    "        print(\"Validation Acc \", \"%.2f\"%(acc.item()/N))\n",
    "        logger.logs['val_loss'] = (epoch_loss.item()/len(val_parts))\n",
    "        logger.logs['val_acc'] = (acc.item()/N)\n",
    "        acc = (acc.item()/N)\n",
    "        logger.logs['val_macc'] = Macc\n",
    "        logger.logs['precision'] = precision\n",
    "        logger.logs['sensitivity'] = sensitivity\n",
    "        logger.logs['specificity'] = specificity\n",
    "        logger.logs['F1'] = F1\n",
    "    lr_schedule.step()\n",
    "    torch.save(model.state_dict(),checkpoint_name.format(epoch=e,val_acc=acc,macc=Macc))\n",
    "    logger.on_epoch_end(e)\n",
    "    flow_source.reset()\n",
    "logger.on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss_print = (epoch_loss.item()) if (type(epoch_loss)==torch.Tensor) else epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(int, int)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(epoch_loss_print),type(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "torch.argmax(model(torch.ones(x.shape).cuda()),1),torch.argmax(model(torch.zeros(x.shape).cuda()),1),y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(model.parameters())[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc,gm,gmnorm = mfcc_gen(holdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(to_numpy(holdx[idx]).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(to_numpy(holdval[0]),origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(model.parameters())[0]\n",
    "a1 = list(model.parameters())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape,a1,b.shape, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = list(model.parameters())[0]\n",
    "b1 = list(model.parameters())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(model.parameters())[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model(hold.transpose(2,1).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(to_numpy(holdval[0]),origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(to_numpy(hold[0]),origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.equal(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc(x):\n",
    "    x = x.transpose(2,1).unsqueeze(1)\n",
    "    \n",
    "#     x = model.extractor.conv0(x)\n",
    "#     x = model.extractor.bn0(x)\n",
    "#     x = model.extractor.conv1(x)\n",
    "    \n",
    "    x = F.relu(model.extractor.bn0(model.extractor.conv0(x)))\n",
    "    #Res block 1\n",
    "    x1 = model.extractor.drop(F.relu(model.extractor.bn1(model.extractor.conv1(x))))\n",
    "    x1 = F.relu(F.max_pool2d(model.extractor.drop(model.extractor.bn11(model.extractor.conv11(x1))), 2))\n",
    "    x = torch.cat((x,torch.zeros_like(x)), axis=1)\n",
    "    x = F.max_pool2d(x,2)\n",
    "    x = x+x1\n",
    "\n",
    "\n",
    "    #Res block 2\n",
    "    x1 = model.extractor.drop(F.relu(model.extractor.bn2(model.extractor.conv2(x))))\n",
    "    x1 = F.relu(model.extractor.drop(model.extractor.bn21(model.extractor.conv21(x1))))\n",
    "    x = torch.cat((x,torch.zeros_like(x)), axis=1)\n",
    "    x = F.max_pool2d(x,2)\n",
    "    x = x+x1\n",
    "    #Res block 3\n",
    "    x1 = model.extractor.drop(F.relu(model.extractor.bn3(model.extractor.conv3(x))))\n",
    "    x1 = F.relu(model.extractor.drop(model.extractor.bn31(model.extractor.conv31(x1))))\n",
    "    x = torch.cat((x,torch.zeros_like(x)), axis=1)\n",
    "    x = F.max_pool2d(x,2)\n",
    "    x = x+x1\n",
    "    #Res block 4\n",
    "    x1 = model.extractor.drop(F.relu(model.extractor.bn4(model.extractor.conv4(x))))\n",
    "    x1 = F.relu(model.extractor.drop(model.extractor.bn41(model.extractor.conv41(x1))))\n",
    "    x = torch.cat((x,torch.zeros_like(x)), axis=1)\n",
    "    x = F.max_pool2d(x,2)\n",
    "    x = x+x1\n",
    "\n",
    "    #last conv\n",
    "    # x = model.extractor.drop(F.relu(model.extractor.bn5(model.extractor.conv5(x))))\n",
    "    x = F.max_pool2d(x,((2,1) if(model.extractor.form) else (1,2)))  ### change withinput\n",
    "    x = x.view(x.size(0),-1)\n",
    "    \n",
    "    x = model.classifier.relu(model.classifier.fc1(x))\n",
    "    x = model.classifier.fc2(F.dropout(x))\n",
    "    x = model.classifier.soft(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdval1 = holdval[:1]\n",
    "holdval1[:,:,150:] = 0\n",
    "torch.sum(holdval1[:,:,150:])\n",
    "hold1 = hold[:1]\n",
    "hold1[:,:,100:] = 0\n",
    "torch.sum(hold1[:,:,100:])\n",
    "holdval1.shape, hold1.shape\n",
    "wow = torch.rand((1,64,240)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = calc(hold[:1])\n",
    "b = calc(holdval[:1])\n",
    "print(a.shape, b.shape)\n",
    "torch.equal(a,b),a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotf(a.transpose(1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotf(b.transpose(1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotf(holdx[:1].reshape(2500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotf(holdvalx[:1].reshape(2500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOad model \n",
    "path = \"../../Heartnet_Results/logs/gammatone_torch_layer/\"\n",
    "fold = 'a_bcdEf learnable_2020-03-04 17.19.23.260152'\n",
    "weight = sorted(os.listdir(os.path.join(path,fold,'weights')))[-1]\n",
    "model.load_state_dict(torch.load(os.path.join(path,fold,'weights',weight)))\n",
    "start_epoch = int(weight.split('-',maxsplit=1)[0].split('.')[-1])+1\n",
    "logger = CSVLogger(os.path.join(path,fold)+'/'+'training.csv')\n",
    "checkpoint_name = os.path.join(path,fold,'weights') + \"/\" + 'weights.{epoch:04d}-acc_{val_acc:.4f}-macc_{macc:.4f}.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+fold+'/model1.json', 'w') as outfile:\n",
    "    outfile.write(str(mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.on_off(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhealthra2/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type WHOLE_MODEL. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model,os.path.join(path,'model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([0, 1, 2500]), torch.Size([0]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "epochs = 400\n",
    "print(\"steps \", flow_source.steps_per_epoch)\n",
    "for e in range(epochs):\n",
    "    print(\"EPOCH   \",e+1)\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i in range(flow_source.steps_per_epoch+1):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x,[y,yd] = flow_source.next()\n",
    "        x,y,yd = torch.from_numpy(x),torch.from_numpy(y),torch.from_numpy(yd)\n",
    "        x,y,yd = Variable(x),Variable(y),Variable(yd)\n",
    "        x = x.type(torch.FloatTensor).cuda()\n",
    "        #x = x.reshape(x.shape[0],1,x.shape[1],x.shape[2])\n",
    "        y = y.long().cuda()\n",
    "        yd = yd.long().cuda()\n",
    "        cls, dom = model(x)\n",
    "        class_loss = class_criterion(cls,torch.argmax(y,axis=1))\n",
    "        domain_loss_source = domain_criterion(dom,torch.argmax(yd,axis=1))\n",
    "        \n",
    "        if(args.dann):\n",
    "            x,[y,yd] = flow_target.next()\n",
    "            x,y,yd = torch.from_numpy(x),torch.from_numpy(y),torch.from_numpy(yd)\n",
    "            x,y,yd = Variable(x),Variable(y),Variable(yd)\n",
    "            x = x.type(torch.FloatTensor).cuda()\n",
    "            x = x.reshape(x.shape[0],1,x.shape[1],x.shape[2])\n",
    "            y = y.long().cuda()\n",
    "            yd = yd.long().cuda()\n",
    "            cls, dom = model(x)\n",
    "            domain_loss_target = domain_criterion(dom,torch.argmax(yd,axis=1))\n",
    "            loss = class_loss + domain_loss_source+domain_loss_target\n",
    "        else:\n",
    "            loss = class_loss\n",
    "        epoch_loss = epoch_loss + loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Training loss fo\", \"%.2f\"%(epoch_loss.item()/flow_source.steps_per_epoch),end=' ')\n",
    "    # Validate \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x,y,yd = torch.from_numpy(x_val),torch.from_numpy(y_val),torch.from_numpy(val_domain)\n",
    "        x,y,yd = Variable(x),Variable(y),Variable(yd)\n",
    "        x = x.type(torch.FloatTensor).cuda()\n",
    "        #x = x.reshape(x.shape[0],1,x.shape[1],x.shape[2])\n",
    "        y = y.long().cuda()\n",
    "        yd = yd.long().cuda()\n",
    "        cls, dom = model(x)\n",
    "        val_class_loss = class_criterion(cls,torch.argmax(y,axis=1))\n",
    "        val_domain_loss = domain_criterion(dom,torch.argmax(yd,axis=1))\n",
    "        print(\"val_Class_loss  \",\"%.2f\"%val_class_loss.item())\n",
    "        print(\"val_dom_loss    \", \"%.2f\"%val_domain_loss.item())\n",
    "        log_macc(cls,dom,y,val_parts)\n",
    "    flow_source.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_gen = MFCC_Gen(fs=1000,filters=64,momentum=)\n",
    "mfcc_gen.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfcc_gen.eval()\n",
    "out = mfcc_gen(torch.from_numpy(x_train[0]).unsqueeze(0).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "outnp1 = (out.squeeze(0).cpu().detach().numpy())\n",
    "plt.imshow(outnp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(outnp1),np.max(outnp1),np.std(outnp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_gen2 = MFCC_Gen2(fs=1000,filters=64)\n",
    "mfcc_gen2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_gen2.train()\n",
    "mfcc_gen2.eval()\n",
    "out = mfcc_gen2(torch.from_numpy(x_train[0]).unsqueeze(0).cuda())\n",
    "plt.figure(figsize=(20,20))\n",
    "outnp = (out.squeeze(0).squeeze(0).cpu().detach().numpy())\n",
    "plt.imshow(outnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(outnp),np.max(outnp),np.std(outnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HeartCepTorch import Conv_Gammatone,MFCC_Gen\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.utils import _single\n",
    "class MFCC_Gen2(nn.Module):\n",
    "    def __init__(self,kernel_size = 81,filters = 26,fs=1000,winlen=0.025,winstep=0.01,dimension=1,momentum=0.001):\n",
    "        super(MFCC_Gen2,self).__init__()\n",
    "        self.gamma = Conv_Gammatone(in_channels=1,out_channels=filters ,kernel_size=kernel_size,fsHz=fs)\n",
    "#         self.gamma = nn.Conv1d(in_channels=1,out_channels=filters ,kernel_size=81,stride=1)\n",
    "        self.gammanorm = nn.BatchNorm1d(filters,momentum=momentum)\n",
    "        self.mfcc = nn.Conv1d(filters,filters,int(winlen*fs),stride=int(winstep*fs),padding=0,bias=False)\n",
    "        self.normmfcc = nn.BatchNorm1d(filters,momentum=momentum)\n",
    "        self.normmfcc2D = nn.BatchNorm2d(1,momentum=momentum)\n",
    "        with torch.no_grad():\n",
    "            self.mfcc.weight = Parameter(torch.stack([torch.eye(filters) for i in range(int(winlen*fs))],dim=2))\n",
    "        for x in self.mfcc.named_parameters():\n",
    "            x[1].requires_grad = False\n",
    "        for x in self.gamma.named_parameters():\n",
    "            x[1].requires_grad = False\n",
    "    def forward(self,x):\n",
    "        x = self.gamma(x)\n",
    "#         x = self.gammanorm(x)\n",
    "        x = torch.pow(torch.abs(x),2)\n",
    "        x = self.mfcc(x)\n",
    "        x = torch.log(x+0.0000000000000001)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.normmfcc2D(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mfcc_models import Smallnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Smallnet(2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary(net.cuda(),(1,2500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filt = (8, 4)\n",
    "num_dense = 20\n",
    "lr = 0.01\n",
    "bn_momentum = 0.99\n",
    "eps = 1.1e-5\n",
    "bias = False\n",
    "l2_reg = 0.04864911065093751\n",
    "l2_reg_dense = 0.\n",
    "kernel_size = 5\n",
    "maxnorm = 10000.\n",
    "dropout_rate = 0.5\n",
    "dropout_rate_dense = 0.\n",
    "padding = 'valid'\n",
    "activation_function = 'relu'\n",
    "subsam = 2\n",
    "FIR_train= True\n",
    "trainable = True\n",
    "hp_lambda = np.float32(0)\n",
    "lr_decay =0.0001132885\n",
    "random_seed = 1\n",
    "num_class =2\n",
    "num_class_domain = num_class_domain\n",
    "tipe= 1\n",
    "decision = 'majority' \n",
    "channels = '0101'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadpath='../../Adversarial Heart Sound Results/models/bcdefghi_a 2019-10-16 12:45:03.236037/weights.0036-0.5964.hdf5'\n",
    "loadatttrain = '../../Adversarial Heart Sound Results/models/attention/bcdefghi_a 2019-10-20 12:37:52.424551/weights.0010-0.5444.hdf5'\n",
    "loadpath = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = heartnet(loadpath,activation_function, bn_momentum, bias, dropout_rate, dropout_rate_dense,\n",
    "                             eps, kernel_size, l2_reg, l2_reg_dense, lr, lr_decay, maxnorm,\n",
    "                             padding, random_seed, subsam, num_filt, num_dense, FIR_train, trainable, tipe,\n",
    "                             num_class=num_class,num_class_domain=9,hp_lambda=hp_lambda,segments=channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"conv1d_linearphase_type_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(flow,#steps_per_epoch=len(x_train) // batch_size,\n",
    "                    steps_per_epoch=flow.steps_per_epoch,\n",
    "                    # max_queue_size=20,\n",
    "                    use_multiprocessing=False,\n",
    "                    epochs=200,\n",
    "                    verbose=1,\n",
    "                    shuffle=True,\n",
    "                    callbacks=[log_macc(val_parts, decision=decision,verbose=1,val_files=val_files,wav_files=val_wav_files,checkpoint_name = 'ansari')],\n",
    "                    validation_data=(x_val, [y_val,val_domain]),\n",
    "                    initial_epoch=0,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = hmmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = (hm.replace('</pre></div></div><div class=\"output_area\"><div class=\"run_this_cell\"></div><div class=\"prompt\"></div><div class=\"output_subarea output_text output_stream output_stdout\"><pre>',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = {'Training loss':'train_loss','Training Acc':'train_acc','Sensitivity:':'sensitivity',\n",
    "        'Specificity:':'specificity','Precision:':'precision','F1:':'val_F1','MACC':'val_macc',\n",
    "        'Validation loss':'val_loss','Validation Acc':'val_acc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm.split(\"EPOCH\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in keys:\n",
    "    print(k,hm.split('EPOCH')[3].split(k)[-1].split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,x in enumerate(hm.split(\"EPOCH\")):\n",
    "#     print(i,x)\n",
    "    if(len(x)<10):\n",
    "        continue\n",
    "    for k in keys:\n",
    "        print(keys[k],x.split(k)[1].split()[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = logger.filename[:-4]+'2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file, 'w') as f:\n",
    "#     print('epoch',end=',')\n",
    "    f.write('epoch,')\n",
    "    for k in keys:\n",
    "#         print(keys[k],end=',')\n",
    "        f.write(keys[k]+',')\n",
    "    f.write('\\n')\n",
    "#     print()\n",
    "    for i,x in enumerate(hm.split(\"EPOCH\")):\n",
    "        print(i,x)\n",
    "        if(len(x)<10):\n",
    "            continue\n",
    "        f.write(x.split('\\n')[0].split()[0]+',')\n",
    "#         print(x.split('\\n')[0].split()[0],end=',')\n",
    "        for k in keys:\n",
    "#             print(x.split(k)[1].split()[0],end=',')\n",
    "            \n",
    "            if(k[:3] in ['Tra','Val']):\n",
    "                f.write(x.split(k)[1].split()[0]+',')\n",
    "            else:\n",
    "                print(k,x.split(k)[-1].split()[0],end=',')\n",
    "                f.write(x.split(k)[-1].split()[0]+',')\n",
    "#             f.write(keys[k]+' '+x.split(k)[1].split()[0])\n",
    "        f.write('\\n')\n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domainClass = [(cls,dfc) for cls in range(2) for dfc in train_domain]\n",
    "meta_label = [hey.index((cl,df)) for (cl,df) in zip(y,yd)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set('sdff'+'cdc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[e%50>39 for e in range(60)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.xticks([10*x for x in range(21)])\n",
    "plt.plot([e%50>39 for e in range(200)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np , math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_lambda = 0.01\n",
    "epochs = 400\n",
    "def sig_moid(epoch):\n",
    "    minEpoch = 150\n",
    "    if(False):#args.fixed):\n",
    "        return hp_lambda\n",
    "    if hp_lambda == 0:\n",
    "        return hp_lambda\n",
    "    #     if epoch<minEpoch:\n",
    "    #         return np.float32(0.0)\n",
    "    maxx =  8\n",
    "    p = 3*(epoch-(epochs/2)) / (epochs/2)\n",
    "    lam = 8/(1+math.e**(-p))\n",
    "    lam = lam#*(epoch%50<10)\n",
    "    # hp_lambda = hp_lambda * (params['hp_decay_const'] ** global_epoch_counter)\n",
    "    return lam\n",
    "def f_hp_anneal(epoch):\n",
    "    minEpoch = 150\n",
    "    if(False):#args.fixed):\n",
    "        return hp_lambda\n",
    "    if hp_lambda == 0:\n",
    "        return hp_lambda\n",
    "    #     if epoch<minEpoch:\n",
    "    #         return np.float32(0.0)\n",
    "    gamma =  4\n",
    "    p = (epoch) / (epochs)\n",
    "    lam =  (8 / (2 + 3*(math.e ** (- gamma * p)))) - 1+hp_lambda  # 3 porjonto jaabe\n",
    "    lam = lam#*(epoch%50<10)\n",
    "    # hp_lambda = hp_lambda * (params['hp_decay_const'] ** global_epoch_counter)\n",
    "    return lam\n",
    "def sin_up(epoch):\n",
    "    minEpoch = 150\n",
    "    if(False):#args.fixed):\n",
    "        return hp_lambda\n",
    "    if hp_lambda == 0:\n",
    "        return hp_lambda\n",
    "    #     if epoch<minEpoch:\n",
    "    #         return np.float32(0.0)\n",
    "    maxx =  4\n",
    "    p = (epoch) / (epochs)\n",
    "    lam = (1+math.sin(p*(math.pi)+1.5*math.pi))*maxx+hp_lambda\n",
    "    lam = lam#*(epoch%50<10)\n",
    "    # hp_lambda = hp_lambda * (params['hp_decay_const'] ** global_epoch_counter)\n",
    "    return lam\n",
    "def f_hp_decay(epoch):\n",
    "    minEpoch = 150\n",
    "    if hp_lambda == 0:\n",
    "        return hp_lambda\n",
    "    if epoch<minEpoch:\n",
    "        return np.float32(0.0)\n",
    "    gamma =  4\n",
    "\n",
    "    p = (epoch-minEpoch) / (epochs)\n",
    "    lam =  (10 /2* (1 + 1*(math.e ** (- gamma * p)))) - 1+hp_lambda  # 3 porjonto jaabe\n",
    "    # hp_lambda = hp_lambda * (params['hp_decay_const'] ** global_epoch_counter)\n",
    "    return np.float32(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([sig_moid(e) for e in range(epochs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([sin_up(e) for e in range(epochs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f_hp_anneal(e) for e in range(epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):         \n",
    "    lr0 = .00128437\n",
    "    #print(\"learning rate , lr 0 \", lr, lr0)\n",
    "    a = 1\n",
    "    b = 1\n",
    "    p = epoch/epochs\n",
    "    lrate = lr0/math.pow((1+a*p),b)\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([step_decay(e) for e in range(epochs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import, division\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.engine.topology import InputSpec\n",
    "import tensorflow as tf\n",
    "from keras.utils import conv_utils\n",
    "from keras.layers import activations, initializers, regularizers, constraints\n",
    "import numpy as np\n",
    "from scipy.fftpack import dct\n",
    "from keras.backend.common import normalize_data_format\n",
    "from keras.layers.merge import Multiply\n",
    "class Attt(Layer):\n",
    "    '''Custom Layer for ResNet used for BatchNormalization.\n",
    "\n",
    "    Linear learnable weight vector , does dot multiplication on a vector\n",
    "    # Arguments\n",
    "        axis: integer, axis along which to normalize in mode 0. For instance,\n",
    "            if your input tensor has shape (samples, channels, rows, cols),\n",
    "            set axis to 1 to normalize per feature map (channels axis).'''\n",
    "\n",
    "    def __init__(self, weights=None, axis=-1,init='he_normal',**kwargs):\n",
    "        self.axis = axis\n",
    "        self.init = initializers.get(init)\n",
    "        self.kernel = weights\n",
    "        super(Attt, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        if len(input_shape)>2:\n",
    "            raise ValueError(\"Input to attention layer hasn't been flattened\")\n",
    "        self.input_dim = input_shape[-1]            \n",
    "        self.kernel = self.add_weight(shape=(self.input_dim,),\n",
    "                                      initializer=initializers.Ones(),\n",
    "                                      name='kernel',\n",
    "                                      constraint=constraints.NonNeg()\n",
    "                                      #constraint=constraints.min_max_norm(min_value=0.0, max_value=1.0)\n",
    "                                      #constraint=constraints.UnitNorm(axis=self.axis)\n",
    "                                     )\n",
    "        self.input_spec = InputSpec(min_ndim=2, axes={-1: self.input_dim})            \n",
    "        self.built = True\n",
    "    def call(self, inputs):\n",
    "        output = tf.multiply(inputs,self.kernel)\n",
    "        return output\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'axis': self.axis,\n",
    "            'kernel_initializer': initializers.serialize(self.init)\n",
    "        }\n",
    "        base_config = super(Attt, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv1D, MaxPooling1D, Dense, Dropout, Flatten, Activation, AveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print('outer',i)\n",
    "    for j in range(3):\n",
    "        print('        innter',j)\n",
    "        if(j==1):\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
