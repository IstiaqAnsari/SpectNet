{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/mhealthra2/anaconda3/envs/torch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mhealthra2/anaconda3/envs/torch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mhealthra2/anaconda3/envs/torch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mhealthra2/anaconda3/envs/torch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mhealthra2/anaconda3/envs/torch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mhealthra2/anaconda3/envs/torch/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "# import tensorflow as tf\n",
    "# from keras.backend.tensorflow_backend import set_session\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "# set_session(tf.Session(config=config))\n",
    "# from clr_callback import CyclicLR\n",
    "# import dill\n",
    "from BalancedDannAudioDataGenerator import BalancedAudioDataGenerator, AudioDataGenerator\n",
    "import os,time\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "import math\n",
    "import pandas as pd\n",
    "import tables,h5py\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "from keras.utils import plot_model\n",
    "# from Heartnet import heartnet,getAttentionModel\n",
    "from collections import Counter\n",
    "# from torchviz import make_dot\n",
    "def to_numpy(x):\n",
    "    return x.cpu().detach().numpy()\n",
    "def plotf(x):\n",
    "    plt.plot(to_numpy(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import log_macc, results_log\n",
    "from dataLoader import reshape_folds\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import Evaluator\n",
    "import dataLoader\n",
    "# from custom_layers import Attention\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wow():\n",
    "    def __init__(self):\n",
    "        self.dann = False\n",
    "        self.self = False\n",
    "        self.reduce = None\n",
    "        self.shuffle = 1\n",
    "        self.mfcc = False\n",
    "args = wow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-43c314985a8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_domain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_parts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_domain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_parts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_wav_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataLoader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_domains\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_domains\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mhealthra2/Data/heart_sound/Adversarial-Heart-Sound-Classification/codes/dataLoader.py\u001b[0m in \u001b[0;36mgetData\u001b[0;34m(fold_dir, train_folds, test_folds, split, shuffle)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mallData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfoldname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msevere\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_folds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mallData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfoldname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mallData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowDistribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mallData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_domain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_parts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_valdom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_parts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_wav_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mhealthra2/Data/heart_sound/Adversarial-Heart-Sound-Classification/codes/dataholder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, f, n, severe, split, normalize, shuffle)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#self.wav_q = self.data['sig_quality'][:][0].astype('int32')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_parts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_parts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwav_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wav_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mhealthra2/Data/heart_sound/Adversarial-Heart-Sound-Classification/codes/dataholder.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#self.wav_q = self.data['sig_quality'][:][0].astype('int32')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_parts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_parts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwav_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wav_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mhealthra2/Data/heart_sound/Adversarial-Heart-Sound-Classification/codes/dataholder.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#self.wav_q = self.data['sig_quality'][:][0].astype('int32')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_parts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_parts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwav_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wav_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_domains = 'a'\n",
    "train_domains = 'cdebf'\n",
    "source_domain = train_domains\n",
    "target_domain = test_domains\n",
    "\n",
    "test_split = 0\n",
    "fold_dir = '../data/all_folds_wav_name/'\n",
    "\n",
    "if(args.self):\n",
    "    print(\"Self training activated\")\n",
    "    x_train, y_train, y_domain, train_parts, x_val, y_val, val_domain, val_parts,val_wav_files = dataLoader.getData(fold_dir,'',test_domains,0.9,shuffle=args.shuffle)\n",
    "    print(x_train.shape, x_val.shape)\n",
    "else:\n",
    "    x_train, y_train, y_domain, train_parts,x_val, y_val, val_domain, val_parts, val_wav_files = dataLoader.getData(fold_dir,train_domains,test_domains,test_split,shuffle = args.shuffle)\n",
    "\n",
    "if(args.reduce):\n",
    "    print(\"Reduction \", args.reduce)\n",
    "    x_train,_,y_train,_,y_domain,_ = train_test_split(x_train.transpose(),y_train,y_domain,stratify=y_train,test_size = args.reduce)\n",
    "    x_train = x_train.transpose()\n",
    "\n",
    "    #x_val,_,y_val,_,val_domain,_ = train_test_split(x_val.transpose(),y_val,val_domain,stratify=y_val,test_size = args.reduce)\n",
    "    #x_val = x_val.transpose()\n",
    "\n",
    "val_files = val_domain\n",
    "#Create meta labels and domain labels\n",
    "\n",
    "if(test_split>0):\n",
    "    source_domain = \"\".join(set(source_domain).union(set(target_domain)))\n",
    "    #domains = domains + test_domains\n",
    "\n",
    "if(args.self):\n",
    "    print(\"self training\")\n",
    "    source_domain = test_domains\n",
    "\n",
    "domains = set(source_domain + target_domain)\n",
    "#num_class_domain = len(set(train_domains + test_domains))\n",
    "num_class_domain = len(domains)\n",
    "num_class = 2\n",
    "\n",
    "domainClass_source = [(cls,dfc) for cls in range(2) for dfc in source_domain]\n",
    "domainClass_target = [(cls,dfc) for cls in range(2) for dfc in target_domain]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load old fold0_noFIR.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trainX', 'trainY', 'train_files', 'train_parts', 'valX', 'valY', 'val_files', 'val_parts']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "path = '../data/fold0_noFIR.mat'\n",
    "data = h5py.File(path, 'r')\n",
    "print(list(data.keys()))\n",
    "x_train = data['trainX'][:].astype('float32')\n",
    "x_train = np.expand_dims(x_train.transpose(),1)\n",
    "x_val = data['valX'][:].astype('float32')\n",
    "x_val = np.expand_dims(x_val.transpose(),1)\n",
    "y_train = data['trainY'][:].astype('int32')\n",
    "y_train = y_train.transpose()\n",
    "y_train = y_train[:,0]\n",
    "y_train[y_train<0] = 0\n",
    "y_val = data['valY'][:].astype('int32')\n",
    "y_val = y_val.transpose()\n",
    "y_val = y_val[:,0]\n",
    "y_val[y_val<0] = 0\n",
    "val_parts = data['val_parts'][:].astype('int32').squeeze(0)\n",
    "val_files = data['val_files'][:].astype('int32').squeeze(0)\n",
    "train_files = data['train_files'][:].astype('int32').squeeze(0)\n",
    "domains = list(Counter(train_files).keys())\n",
    "domainClass = [(cls,dfc) for cls in range(2) for dfc in domains]\n",
    "meta_labels = [domainClass.index((cl,df)) for (cl,df) in zip((y_train),train_files)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mfcc extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Convert to MFCC\n",
    "import python_speech_features as psf\n",
    "from matplotlib import cm\n",
    "if(args.mfcc):\n",
    "    train_mfcc = np.array([(psf.base.mfcc(x,samplerate=1000,winlen=0.05,winstep=0.01)) for x in x_train.transpose()])\n",
    "    val_mfcc = np.array([(psf.base.mfcc(x,samplerate=1000,winlen=0.05,winstep=0.01)) for x in x_val.transpose()])\n",
    "    \n",
    "    train_mfcc = (train_mfcc-np.mean(train_mfcc))/np.std(train_mfcc)\n",
    "    val_mfcc = (val_mfcc-np.mean(val_mfcc))/np.std(val_mfcc)\n",
    "    #train_mfcc = train_mfcc/np.max(np.abs(train_mfcc))\n",
    "    #val_mfcc = val_mfcc/np.max(np.abs(val_mfcc))\n",
    "    \n",
    "    del x_train, x_val\n",
    "    x_train = train_mfcc.copy()\n",
    "    x_val = val_mfcc.copy()\n",
    "    print(x_train.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaped x  (67328, 2500, 1)\n",
      "reshaped x  (14167, 2500, 1)\n",
      "reshaped Y  (67328, 1)\n",
      "reshaped Y  (67328, 1)\n",
      "reshaped Y  (14167, 1)\n",
      "Y domain  Counter({3: 57642, 2: 4119, 1: 2985, 5: 1781, 0: 801})\n",
      "Val domain  Counter({'a': 14167})\n",
      "Source Meta labels  Counter({2: 54790, 4: 3012, 7: 2852, 3: 2396, 5: 1425, 9: 1107, 8: 589, 6: 493, 0: 356, 1: 308})\n",
      "Target Meta labels  Counter()\n",
      "Train files  (67328, 2)   Domain  (67328, 1)\n",
      "Test files  (14167, 2)   Domain  (14167, 6)\n"
     ]
    }
   ],
   "source": [
    "meta_labels_source = [domainClass_source.index((cl,df)) for (cl,df) in zip(y_train,y_domain)]\n",
    "meta_labels_target = None\n",
    "if(args.dann):\n",
    "    meta_labels_target = [domainClass_target.index((cl,df)) for (cl,df) in zip((y_val),(val_domain))]\n",
    "    \n",
    "\n",
    "domains = \"\".join(set(source_domain).union(set(target_domain)))\n",
    "\n",
    "y_domain_source = np.array([list(domains).index(lab) for lab in y_domain])\n",
    "\n",
    "y_domain_target = np.array([list(domains).index(lab) for lab in val_domain])\n",
    "\n",
    "################### Reshaping ############\n",
    "\n",
    "if(args.mfcc):\n",
    "    [], [y_train,y_domain,y_val] = reshape_folds([],[y_train,y_domain_source,y_val])\n",
    "else:\n",
    "    [x_train,x_val], [y_train,y_domain,y_val] = reshape_folds([x_train,x_val],[y_train,y_domain_source,y_val])\n",
    "y_train = to_categorical(y_train, num_classes=num_class)\n",
    "\n",
    "print(\"Y domain \", Counter([x[0] for x in y_domain]))\n",
    "print(\"Val domain \", Counter(val_domain))\n",
    "print(\"Source Meta labels \", Counter(meta_labels_source))\n",
    "print(\"Target Meta labels \", Counter(meta_labels_target))\n",
    "y_domain_source = to_categorical(y_domain_source,num_classes=num_class_domain)\n",
    "\n",
    "y_val = to_categorical(y_val, num_classes=num_class)\n",
    "y_domain_target = to_categorical(y_domain_target,num_classes=num_class_domain)\n",
    "\n",
    "\n",
    "val_domain = y_domain_target\n",
    "print(\"Train files \", y_train.shape, \"  Domain \", y_domain.shape)\n",
    "print(\"Test files \", y_val.shape, \"  Domain \", val_domain.shape)\n",
    "\n",
    "### Batch Size limmiter \n",
    "batch_size = 1000\n",
    "if(batch_size > max(y_train.shape)):\n",
    "    print(\"Batch size if given greater than train files size. limiting batch size\")\n",
    "    batch_size = max(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((67328, 2500, 1),\n",
       " (14167, 2500, 1),\n",
       " (67328, 2),\n",
       " (14167, 2),\n",
       " (67328, 1),\n",
       " (14167, 6),\n",
       " Counter({5: 1425,\n",
       "          0: 356,\n",
       "          1: 308,\n",
       "          6: 493,\n",
       "          2: 54790,\n",
       "          7: 2852,\n",
       "          3: 2396,\n",
       "          8: 589,\n",
       "          4: 3012,\n",
       "          9: 1107}),\n",
       " Counter())"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,x_val.shape, y_train.shape, y_val.shape, y_domain.shape,val_domain.shape,Counter(meta_labels_source),Counter(meta_labels_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.argmax(y_train,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = np.argmax(y_val,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79810, 1, 2500), (79810,), (6710, 1, 2500), (6710,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,y_train.shape,x_val.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## change 2500 axis for pytorch \n",
    "x_train = x_train.transpose((0,2,1))\n",
    "x_val = x_val.transpose((0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79810, 1, 2500), (79810,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:2100,:,:]\n",
    "y_train = y_train[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2883  2790   223   294 55347  3294  9075   446  1080   497  2470  1411]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "Chunk size selected as 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mhealthra2/Data/heart_sound/Adversarial-Heart-Sound-Classification/codes/BalancedDannAudioDataGenerator.py:841: UserWarning: `meta_labels` specified, will use meta_labels instead of target_label\n",
      "  warnings.warn('`meta_labels` specified, will use meta_labels instead of target_label')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 12\n",
    "datagen_source = BalancedAudioDataGenerator(shift=.1,data_format = 'channels_first')\n",
    "flow_source = datagen_source.flow(x_train, y_train,\n",
    "                meta_label=meta_labels,\n",
    "                batch_size=batch_size, shuffle=True,\n",
    "                seed=1)\n",
    "# datagen_source_balanced = AudioDataGenerator(shift=.1,data_format = 'channels_first')\n",
    "# flow_source = datagen_source_balanced.flow(x_train, y_train,\n",
    "#                 batch_size=batch_size, shuffle=True,\n",
    "#                 seed=1)\n",
    "# datagen_val = BalancedAudioDataGenerator(shift=.1,data_format = 'channels_first')\n",
    "# flow_val = datagen_val.flow(x_val, y_val,\n",
    "#                 meta_label=y_val,\n",
    "#                 batch_size=batch_size, shuffle=True,\n",
    "#                 seed=1)\n",
    "try:\n",
    "    flow_source.steps_per_epoch = len(flow_source)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 1, 2500])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1, 2500])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = flow_source.next()\n",
    "x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
    "x = x.type(torch.FloatTensor).cuda()\n",
    "holdx = x\n",
    "print(x.shape)\n",
    "# x = mfcc_gen(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1, 64, 1, 240])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hold = x\n",
    "x = x.transpose(2,1)\n",
    "x = x.unsqueeze(1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from torchvision import datasets, transforms\n",
    "from torchsummary import summary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import HeartCepTorch,importlib\n",
    "HeartCepTorch = importlib.reload(HeartCepTorch)\n",
    "from HeartCepTorch import MFCC_Gen,Network,MFCC_Gen_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "from HeartCepTorch import Conv_Gammatone_coeff\n",
    "class Branch(nn.Module):\n",
    "    def __init__(self,c_in, c_out, kernel_size=5,stride=1,dropout = 0.5):\n",
    "        super(Branch, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(c_in, c_out*2, kernel_size=kernel_size,stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(c_out*2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.conv2 = nn.Conv1d(c_out*2, c_out, kernel_size=kernel_size,stride=stride)\n",
    "        self.bn2 = nn.BatchNorm1d(c_out)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "    def forward(self, x):\n",
    "        x = self.drop(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(x)\n",
    "        x = self.drop(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,filters=64,kernel_size=81,fs=1000):\n",
    "        super(Net,self).__init__()\n",
    "        wow = Conv_Gammatone_coeff(1,filters,kernel_size,fs)\n",
    "        self.filterbank = nn.ModuleList()\n",
    "        for i in range(filters):\n",
    "            conv = nn.Conv1d(1,1,kernel_size=kernel_size)\n",
    "            conv.weight = Parameter(wow.weight[i:i+1])\n",
    "            self.filterbank.append(conv)\n",
    "        self.branches = nn.ModuleList()\n",
    "        for i in range(filters):\n",
    "            branch = Branch(1,4)\n",
    "            self.branches.append(branch)\n",
    "        self.dense = nn.Linear(4*602*filters,20)\n",
    "        self.cls = nn.Linear(20,2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.soft = nn.Softmax()\n",
    "    def forward(self,x):\n",
    "        x = [c(x) for c in self.filterbank]\n",
    "        x = [c(xx) for (xx,c) in zip(x,self.branches)]\n",
    "        x = torch.cat(x,dim=1)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.relu(self.dense(x))\n",
    "        x = self.soft(self.cls(x))\n",
    "        return x\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network(2,0)\n",
    "mfcc_gen = MFCC_Gen_coeff(fs=1000,filters=64,momentum=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_gen.gamma.weight = Parameter(torch.from_numpy(bank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/media/mhealthra2/Data/heart_sound/Heartnet_Results/logs/gammatone_torch_layer/fold0_noFIR dbt 64 HNET_2020-05-17 17.56.06.569274/weights/'\n",
    "weights = os.listdir(path)\n",
    "wow = dict(torch.load(path+weights[53]))\n",
    "gamma_weight = torch.cat([wow['filterbank.{}.weight'.format(i)] for i in range(64)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "mfcc_gen.gamma.weight = Parameter(gamma_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7eff518c85d0>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(to_numpy(mfcc_gen.gamma.weight[35]).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 16, 244, 65]             112\n",
      "       BatchNorm2d-2          [-1, 16, 244, 65]              32\n",
      "            Conv2d-3          [-1, 32, 242, 63]           4,640\n",
      "       BatchNorm2d-4          [-1, 32, 242, 63]              64\n",
      "         Dropout2d-5          [-1, 32, 242, 63]               0\n",
      "            Conv2d-6          [-1, 32, 244, 65]           9,248\n",
      "       BatchNorm2d-7          [-1, 32, 244, 65]              64\n",
      "         Dropout2d-8          [-1, 32, 244, 65]               0\n",
      "            Conv2d-9          [-1, 64, 122, 32]          18,496\n",
      "      BatchNorm2d-10          [-1, 64, 122, 32]             128\n",
      "        Dropout2d-11          [-1, 64, 122, 32]               0\n",
      "           Conv2d-12           [-1, 64, 61, 16]          36,928\n",
      "      BatchNorm2d-13           [-1, 64, 61, 16]             128\n",
      "        Dropout2d-14           [-1, 64, 61, 16]               0\n",
      "           Conv2d-15          [-1, 128, 61, 16]          73,856\n",
      "      BatchNorm2d-16          [-1, 128, 61, 16]             256\n",
      "        Dropout2d-17          [-1, 128, 61, 16]               0\n",
      "           Conv2d-18           [-1, 128, 30, 8]         147,584\n",
      "      BatchNorm2d-19           [-1, 128, 30, 8]             256\n",
      "        Dropout2d-20           [-1, 128, 30, 8]               0\n",
      "           Conv2d-21           [-1, 256, 30, 8]         295,168\n",
      "      BatchNorm2d-22           [-1, 256, 30, 8]             512\n",
      "        Dropout2d-23           [-1, 256, 30, 8]               0\n",
      "           Conv2d-24           [-1, 256, 15, 4]         590,080\n",
      "      BatchNorm2d-25           [-1, 256, 15, 4]             512\n",
      "        Dropout2d-26           [-1, 256, 15, 4]               0\n",
      "        Extractor-27                 [-1, 7168]               0\n",
      "           Linear-28                  [-1, 100]         716,900\n",
      "             ReLU-29                  [-1, 100]               0\n",
      "           Linear-30                    [-1, 2]             202\n",
      "          Softmax-31                    [-1, 2]               0\n",
      " Class_classifier-32                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 1,895,166\n",
      "Trainable params: 1,895,166\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 39.18\n",
      "Params size (MB): 7.23\n",
      "Estimated Total Size (MB): 46.47\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model.cuda(),(1,240,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "    Conv_Gammatone-1             [-1, 64, 2420]             192\n",
      "       BatchNorm1d-2             [-1, 64, 2420]             192\n",
      "            Conv1d-3              [-1, 64, 240]         102,400\n",
      "       BatchNorm2d-4           [-1, 1, 64, 240]               3\n",
      "================================================================\n",
      "Total params: 102,787\n",
      "Trainable params: 195\n",
      "Non-trainable params: 102,592\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 2.60\n",
      "Params size (MB): 0.39\n",
      "Estimated Total Size (MB): 3.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(mfcc_gen.cuda(),(1,2500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WHOLE_MODEL(nn.Module):\n",
    "    def __init__(self,kernel_size = 81,filters = 26,fs=1000,winlen=0.025,winstep=0.01,dimension=1):\n",
    "        super(WHOLE_MODEL,self).__init__()\n",
    "        self.mfcc = MFCC_Gen_coeff(fs=1000,filters=64,momentum=0.99)\n",
    "        self.classifier = Network(2,0)\n",
    "#         for x in self.mfcc.gamma.named_parameters():\n",
    "#             x[1].requires_grad = False\n",
    "    def forward(self,x):\n",
    "        x = self.mfcc(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.transpose(3,2)\n",
    "        x = self.classifier(x)\n",
    "        return x#,mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WHOLE_MODEL()\n",
    "model.mfcc.mfcc.weight.requires_grad = False\n",
    "model.cuda()\n",
    "model.mfcc.mfcc.weight.requires_grad = False\n",
    "model.mfcc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1             [-1, 64, 2420]           5,312\n",
      "       BatchNorm1d-2             [-1, 64, 2420]             192\n",
      "            Conv1d-3              [-1, 64, 240]         102,400\n",
      "       BatchNorm1d-4              [-1, 64, 240]             192\n",
      "    MFCC_Gen_coeff-5              [-1, 64, 240]         107,906\n",
      "            Conv2d-6          [-1, 16, 244, 65]             128\n",
      "       BatchNorm2d-7          [-1, 16, 244, 65]              48\n",
      "            Conv2d-8          [-1, 32, 242, 63]           4,672\n",
      "       BatchNorm2d-9          [-1, 32, 242, 63]              96\n",
      "        Dropout2d-10          [-1, 32, 242, 63]               0\n",
      "           Conv2d-11          [-1, 32, 244, 65]           9,280\n",
      "      BatchNorm2d-12          [-1, 32, 244, 65]              96\n",
      "        Dropout2d-13          [-1, 32, 244, 65]               0\n",
      "           Conv2d-14          [-1, 64, 122, 32]          18,560\n",
      "      BatchNorm2d-15          [-1, 64, 122, 32]             192\n",
      "        Dropout2d-16          [-1, 64, 122, 32]               0\n",
      "           Conv2d-17           [-1, 64, 61, 16]          36,992\n",
      "      BatchNorm2d-18           [-1, 64, 61, 16]             192\n",
      "        Dropout2d-19           [-1, 64, 61, 16]               0\n",
      "           Conv2d-20          [-1, 128, 61, 16]          73,984\n",
      "      BatchNorm2d-21          [-1, 128, 61, 16]             384\n",
      "        Dropout2d-22          [-1, 128, 61, 16]               0\n",
      "           Conv2d-23           [-1, 128, 30, 8]         147,712\n",
      "      BatchNorm2d-24           [-1, 128, 30, 8]             384\n",
      "        Dropout2d-25           [-1, 128, 30, 8]               0\n",
      "           Conv2d-26           [-1, 256, 30, 8]         295,424\n",
      "      BatchNorm2d-27           [-1, 256, 30, 8]             768\n",
      "        Dropout2d-28           [-1, 256, 30, 8]               0\n",
      "           Conv2d-29           [-1, 256, 15, 4]         590,336\n",
      "      BatchNorm2d-30           [-1, 256, 15, 4]             768\n",
      "        Dropout2d-31           [-1, 256, 15, 4]               0\n",
      "        Extractor-32                 [-1, 7168]       1,178,064\n",
      "           Linear-33                  [-1, 100]         717,000\n",
      "             ReLU-34                  [-1, 100]               0\n",
      "           Linear-35                    [-1, 2]             204\n",
      "          Softmax-36                    [-1, 2]               0\n",
      " Class_classifier-37                    [-1, 2]         717,102\n",
      "          Network-38                    [-1, 2]       1,895,166\n",
      "================================================================\n",
      "Total params: 5,903,554\n",
      "Trainable params: 5,795,842\n",
      "Non-trainable params: 107,712\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 41.89\n",
      "Params size (MB): 22.52\n",
      "Estimated Total Size (MB): 64.42\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model.cuda(),(1,2500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr= .001)\n",
    "class_criterion = nn.CrossEntropyLoss()\n",
    "domain_criterion = nn.CrossEntropyLoss()\n",
    "# class_criterion = nn.BCELoss()\n",
    "start_epoch = 0\n",
    "lr_schedule = optim.lr_scheduler.ExponentialLR(optimizer,1)\n",
    "# wow= []\n",
    "# for i in range(400):\n",
    "#     optimizer.step()\n",
    "#     wow.append(optimizer.param_groups[0]['lr'])\n",
    "#     sh.step()\n",
    "# plt.plot(wow)\n",
    "# print(wow[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "eps = 0.0000001\n",
    "def log_macc(y_pred, y_val,val_parts):\n",
    "    y_pred = y_pred.cpu().detach().numpy()\n",
    "#     y_pred_domain = y_pred_domain.cpu().detach().numpy()\n",
    "    y_val = y_val.cpu().detach().numpy()\n",
    "    true = []\n",
    "    pred = []\n",
    "    files = []\n",
    "    start_idx = 0\n",
    "\n",
    "#     y_pred = np.argmax(y_pred, axis=-1)\n",
    "#     y_val = np.transpose(np.argmax(y_val, axis=-1))\n",
    "\n",
    "    for j,s in enumerate(val_parts):\n",
    "\n",
    "        if not s:  ## for e00032 in validation0 there was no cardiac cycle\n",
    "            continue\n",
    "        # ~ print \"part {} start {} stop {}\".format(s,start_idx,start_idx+int(s)-1)\n",
    "        \n",
    "        temp_ = y_val[start_idx:start_idx + int(s)]\n",
    "        temp = y_pred[start_idx:start_idx + int(s)]\n",
    "\n",
    "        if (sum(temp == 0) > sum(temp == 1)):\n",
    "            pred.append(0)\n",
    "        else:\n",
    "            pred.append(1)\n",
    "\n",
    "        if (sum(temp_ == 0) > sum(temp_ == 1)):\n",
    "            true.append(0)\n",
    "        else:\n",
    "            true.append(1)\n",
    "\n",
    "#         if val_files is not None:\n",
    "#             files.append(val_files[start_idx])\n",
    "\n",
    "        start_idx = start_idx + int(s)\n",
    "    TN, FP, FN, TP = confusion_matrix(true, pred, labels=[0,1]).ravel()\n",
    "    # TN = float(TN)\n",
    "    # TP = float(TP)\n",
    "    # FP = float(FP)\n",
    "    # FN = float(FN)\n",
    "    sensitivity = TP / (TP + FN + eps)\n",
    "    specificity = TN / (TN + FP + eps)\n",
    "    precision = TP / (TP + FP + eps)\n",
    "    F1 = 2 * (precision * sensitivity) / (precision + sensitivity + eps)\n",
    "    Macc = (sensitivity + specificity) / 2\n",
    "    \n",
    "    print(\"TN:\",TN,\"FP:\",FP,\"FN:\",FN,\"TP:\",TP)\n",
    "    print(\"Sensitivity:\",\"%.2f\"%sensitivity,\"Specificity:\",\"%.2f\"%specificity,\"Precision:\",\"%.2f\"%precision,end=' ')\n",
    "    print(\"F1:\", \"%.2f\"%F1,\"MACC\", \"%.2f\"%Macc)\n",
    "    return Macc,sensitivity,specificity,precision,F1\n",
    "def trainLog(y_true,y_pred):\n",
    "    eps = 0.0000001\n",
    "    y_pred = y_pred.cpu().detach().numpy()\n",
    "    y_true = y_true.cpu().detach().numpy()\n",
    "    TN, FP, FN, TP = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    sensitivity = TP / (TP + FN + eps)\n",
    "    specificity = TN / (TN + FP + eps)\n",
    "    precision = TP / (TP + FP + eps)\n",
    "    F1 = 2 * (precision * sensitivity) / (precision + sensitivity + eps)\n",
    "    Macc = (sensitivity + specificity) / 2\n",
    "    print(\"TN:\",TN,\"FP:\",FP,\"FN:\",FN,\"TP:\",TP)\n",
    "    print(\"Sensitivity:\",\"%.2f\"%sensitivity,\"Specificity:\",\"%.2f\"%specificity,\"Precision:\",\"%.2f\"%precision,end=' ')\n",
    "    print(\"F1:\", \"%.2f\"%F1,\"MACC\", \"%.2f\"%Macc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from datetime import datetime\n",
    "import CSVLogger\n",
    "CSVLogger = importlib.reload(CSVLogger)\n",
    "from CSVLogger import CSVLogger\n",
    "fold = \"fold0_noFIR requires grad on batch 12x50\"\n",
    "path = \"../../Heartnet_Results/logs/gammatone_torch_layer/\"\n",
    "fold = fold+'_'+str(datetime.now()).replace(':','.')\n",
    "path = path + fold\n",
    "if not os.path.isdir(path):\n",
    "    os.mkdir(path)\n",
    "    os.mkdir(os.path.join(path,'weights'))\n",
    "logger = CSVLogger(path+'/'+'training.csv')\n",
    "checkpoint_name = os.path.join(path,'weights') + \"/\" + 'weights.{epoch:04d}-acc_{val_acc:.4f}-macc_{macc:.4f}.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps  55347\n",
      "EPOCH    1\n",
      "learning rate  0.001\n",
      "Training loss 0.63 Training loss list 0.63 Training Acc  0.64\n",
      "TN: 0 FP: 146 FN: 0 TP: 138\n",
      "Sensitivity: 1.00 Specificity: 0.00 Precision: 0.49 F1: 0.65 MACC 0.50\n",
      "Validation loss 0.78 Validation Acc  0.45\n",
      "EPOCH    2\n",
      "learning rate  0.001\n",
      "Training loss 0.60 Training loss list 0.60 Training Acc  0.69\n",
      "TN: 120 FP: 26 FN: 98 TP: 40\n",
      "Sensitivity: 0.29 Specificity: 0.82 Precision: 0.61 F1: 0.39 MACC 0.56\n",
      "Validation loss 0.74 Validation Acc  0.53\n",
      "EPOCH    3\n",
      "learning rate  0.001\n",
      "Training loss 0.58 Training loss list 0.58 Training Acc  0.71\n",
      "TN: 146 FP: 0 FN: 138 TP: 0\n",
      "Sensitivity: 0.00 Specificity: 1.00 Precision: 0.00 F1: 0.00 MACC 0.50\n",
      "Validation loss 0.76 Validation Acc  0.55\n",
      "EPOCH    4\n",
      "learning rate  0.001\n",
      "Training loss 0.56 Training loss list 0.56 Training Acc  0.74\n",
      "TN: 146 FP: 0 FN: 136 TP: 2\n",
      "Sensitivity: 0.01 Specificity: 1.00 Precision: 1.00 F1: 0.03 MACC 0.51\n",
      "Validation loss 0.74 Validation Acc  0.56\n",
      "EPOCH    5\n",
      "learning rate  0.001\n",
      "Training loss 0.55 Training loss list 0.55 Training Acc  0.75\n",
      "TN: 146 FP: 0 FN: 138 TP: 0\n",
      "Sensitivity: 0.00 Specificity: 1.00 Precision: 0.00 F1: 0.00 MACC 0.50\n",
      "Validation loss 0.76 Validation Acc  0.55\n",
      "EPOCH    6\n",
      "learning rate  0.001\n",
      "Training loss 0.53 Training loss list 0.53 Training Acc  0.78\n",
      "TN: 140 FP: 6 FN: 123 TP: 15\n",
      "Sensitivity: 0.11 Specificity: 0.96 Precision: 0.71 F1: 0.19 MACC 0.53\n",
      "Validation loss 0.75 Validation Acc  0.53\n",
      "EPOCH    7\n",
      "learning rate  0.001\n",
      "Training loss 0.51 Training loss list 0.51 Training Acc  0.79\n",
      "TN: 146 FP: 0 FN: 138 TP: 0\n",
      "Sensitivity: 0.00 Specificity: 1.00 Precision: 0.00 F1: 0.00 MACC 0.50\n",
      "Validation loss 0.75 Validation Acc  0.55\n",
      "EPOCH    8\n",
      "learning rate  0.001\n",
      "Training loss 0.50 Training loss list 0.50 Training Acc  0.80\n",
      "TN: 146 FP: 0 FN: 138 TP: 0\n",
      "Sensitivity: 0.00 Specificity: 1.00 Precision: 0.00 F1: 0.00 MACC 0.50\n",
      "Validation loss 0.75 Validation Acc  0.55\n",
      "EPOCH    9\n",
      "learning rate  0.001\n",
      "Training loss 0.49 Training loss list 0.49 Training Acc  0.81\n",
      "TN: 146 FP: 0 FN: 138 TP: 0\n",
      "Sensitivity: 0.00 Specificity: 1.00 Precision: 0.00 F1: 0.00 MACC 0.50\n",
      "Validation loss 0.75 Validation Acc  0.55\n",
      "EPOCH    10\n",
      "learning rate  0.001\n",
      "Training loss 0.48 Training loss list 0.48 Training Acc  0.82\n",
      "TN: 146 FP: 0 FN: 138 TP: 0\n",
      "Sensitivity: 0.00 Specificity: 1.00 Precision: 0.00 F1: 0.00 MACC 0.50\n",
      "Validation loss 0.76 Validation Acc  0.55\n",
      "EPOCH    11\n",
      "learning rate  0.001\n",
      "Training loss 0.48 Training loss list 0.48 Training Acc  0.83\n",
      "TN: 146 FP: 0 FN: 138 TP: 0\n",
      "Sensitivity: 0.00 Specificity: 1.00 Precision: 0.00 F1: 0.00 MACC 0.50\n",
      "Validation loss 0.75 Validation Acc  0.55\n",
      "EPOCH    12\n",
      "learning rate  0.001\n",
      "Training loss 0.47 Training loss list 0.47 Training Acc  0.84\n",
      "TN: 146 FP: 0 FN: 138 TP: 0\n",
      "Sensitivity: 0.00 Specificity: 1.00 Precision: 0.00 F1: 0.00 MACC 0.50\n",
      "Validation loss 0.75 Validation Acc  0.55\n",
      "EPOCH    13\n",
      "learning rate  0.001\n",
      "Training loss 0.46 Training loss list 0.46 Training Acc  0.84\n",
      "TN: 146 FP: 0 FN: 138 TP: 0\n",
      "Sensitivity: 0.00 Specificity: 1.00 Precision: 0.00 F1: 0.00 MACC 0.50\n",
      "Validation loss 0.75 Validation Acc  0.55\n",
      "EPOCH    14\n",
      "learning rate  0.001\n",
      "Training loss 0.46 Training loss list 0.46 Training Acc  0.85\n",
      "TN: 146 FP: 0 FN: 138 TP: 0\n",
      "Sensitivity: 0.00 Specificity: 1.00 Precision: 0.00 F1: 0.00 MACC 0.50\n",
      "Validation loss 0.75 Validation Acc  0.55\n",
      "EPOCH    15\n",
      "learning rate  0.001\n",
      "Training loss 0.45 Training loss list 0.45 Training Acc  0.86\n",
      "TN: 146 FP: 0 FN: 138 TP: 0\n",
      "Sensitivity: 0.00 Specificity: 1.00 Precision: 0.00 F1: 0.00 MACC 0.50\n",
      "Validation loss 0.75 Validation Acc  0.55\n",
      "EPOCH    16\n",
      "learning rate  0.001\n",
      "Training loss 0.45 Training loss list 0.45 Training Acc  0.86\n",
      "TN: 146 FP: 0 FN: 138 TP: 0\n",
      "Sensitivity: 0.00 Specificity: 1.00 Precision: 0.00 F1: 0.00 MACC 0.50\n",
      "Validation loss 0.75 Validation Acc  0.55\n",
      "EPOCH    17\n",
      "learning rate  0.001\n",
      "Training loss 0.44 Training loss list 0.44 Training Acc  0.87\n",
      "TN: 146 FP: 0 FN: 138 TP: 0\n",
      "Sensitivity: 0.00 Specificity: 1.00 Precision: 0.00 F1: 0.00 MACC 0.50\n",
      "Validation loss 0.76 Validation Acc  0.54\n",
      "EPOCH    18\n",
      "learning rate  0.001\n",
      "Training loss 0.44 Training loss list 0.44 Training Acc  0.87\n",
      "TN: 131 FP: 15 FN: 113 TP: 25\n",
      "Sensitivity: 0.18 Specificity: 0.90 Precision: 0.62 F1: 0.28 MACC 0.54\n",
      "Validation loss 0.75 Validation Acc  0.53\n",
      "EPOCH    19\n",
      "learning rate  0.001\n",
      "Training loss 0.43 Training loss list 0.43 Training Acc  0.88\n",
      "TN: 146 FP: 0 FN: 138 TP: 0\n",
      "Sensitivity: 0.00 Specificity: 1.00 Precision: 0.00 F1: 0.00 MACC 0.50\n",
      "Validation loss 0.75 Validation Acc  0.55\n",
      "EPOCH    20\n",
      "learning rate  0.001\n",
      "Training loss 0.43 Training loss list 0.43 Training Acc  0.88\n",
      "TN: 146 FP: 0 FN: 138 TP: 0\n",
      "Sensitivity: 0.00 Specificity: 1.00 Precision: 0.00 F1: 0.00 MACC 0.50\n",
      "Validation loss 0.75 Validation Acc  0.55\n",
      "EPOCH    21\n",
      "learning rate  0.001\n",
      "Training loss 0.43 Training loss list 0.43 Training Acc  0.88\n",
      "TN: 146 FP: 0 FN: 138 TP: 0\n",
      "Sensitivity: 0.00 Specificity: 1.00 Precision: 0.00 F1: 0.00 MACC 0.50\n",
      "Validation loss 0.75 Validation Acc  0.55\n",
      "EPOCH    22\n",
      "learning rate  0.001\n",
      "Training loss 0.42 Training loss list 0.42 Training Acc  0.89\n",
      "TN: 146 FP: 0 FN: 138 TP: 0\n",
      "Sensitivity: 0.00 Specificity: 1.00 Precision: 0.00 F1: 0.00 MACC 0.50\n",
      "Validation loss 0.75 Validation Acc  0.55\n",
      "EPOCH    23\n",
      "learning rate  0.001\n",
      "Training loss 0.42 Training loss list 0.42 Training Acc  0.89\n",
      "TN: 146 FP: 0 FN: 138 TP: 0\n",
      "Sensitivity: 0.00 Specificity: 1.00 Precision: 0.00 F1: 0.00 MACC 0.50\n",
      "Validation loss 0.76 Validation Acc  0.55\n",
      "EPOCH    24\n",
      "learning rate  0.001\n",
      "Training loss 0.42 Training loss list 0.42 Training Acc  0.89\n",
      "TN: 146 FP: 0 FN: 138 TP: 0\n",
      "Sensitivity: 0.00 Specificity: 1.00 Precision: 0.00 F1: 0.00 MACC 0.50\n",
      "Validation loss 0.75 Validation Acc  0.55\n",
      "EPOCH    25\n",
      "learning rate  0.001\n",
      "Training loss 0.41 Training loss list 0.41 Training Acc  0.90\n",
      "TN: 5 FP: 141 FN: 11 TP: 127\n",
      "Sensitivity: 0.92 Specificity: 0.03 Precision: 0.47 F1: 0.63 MACC 0.48\n",
      "Validation loss 0.78 Validation Acc  0.48\n",
      "EPOCH    26\n",
      "learning rate  0.001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-44dc617d6102>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflow_source\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "epochs = 1200\n",
    "\n",
    "logger\n",
    "logger.on_train_begin()\n",
    "optimizer.zero_grad()\n",
    "print(\"steps \", flow_source.steps_per_epoch)\n",
    "for e in range(start_epoch,epochs):\n",
    "    print(\"EPOCH   \",e+1)\n",
    "    print(\"learning rate \",optimizer.param_groups[0]['lr'])\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_loss_list = []\n",
    "    acc = 0\n",
    "    N = 0\n",
    "    optimizer.zero_grad()\n",
    "    for i in range(flow_source.steps_per_epoch+1):\n",
    "\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "        x,y = flow_source.next()\n",
    "        x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
    "        x = x.type(torch.FloatTensor).cuda()\n",
    "        x,y = Variable(x),Variable(y)\n",
    "        y = y.long().cuda()        \n",
    "        cls = model(x)\n",
    "#         print(\"passed \",i+1)\n",
    "        # class_loss = class_criterion(cls,torch.argmax(y,axis=1))        \n",
    "        class_loss = class_criterion(cls,y)\n",
    "        loss = class_loss\n",
    "        epoch_loss = epoch_loss + loss\n",
    "        epoch_loss_list.append(loss)\n",
    "        acc = acc + torch.sum(y==torch.argmax(cls,axis=1))\n",
    "        N = N+len(y)\n",
    "\n",
    "        loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "\n",
    "        if(i%50==0 or i==flow_source.steps_per_epoch):\n",
    "#             print(\"     backward at \",i+1)\n",
    "#             loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "#             print(\"         backward complete \",i+1)\n",
    "\n",
    "    print(\"Training loss\", \"%.2f\"%(epoch_loss.item()/flow_source.steps_per_epoch),end=' ')\n",
    "    print(\"Training loss list\", \"%.2f\"%(torch.mean(torch.tensor(epoch_loss_list)).item()),end=' ')\n",
    "    print(\"Training Acc \", \"%.2f\"%(acc.item()/N))\n",
    "    logger.logs['train_loss'] = (epoch_loss.item()/flow_source.steps_per_epoch)\n",
    "    logger.logs['train_acc'] = (acc.item()/N)\n",
    "    # Validate \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    acc = 0\n",
    "    N = 0\n",
    "    y_pred = None\n",
    "    y_true = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        start_idx = 0\n",
    "        for i,s in enumerate(val_parts):\n",
    "            if(s==0):\n",
    "                continue\n",
    "            x,y = x_val[start_idx:start_idx+s],y_val[start_idx:start_idx+s]\n",
    "            start_idx = start_idx+s\n",
    "            x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
    "            x = x.type(torch.FloatTensor).cuda()\n",
    "            x,y = Variable(x),Variable(y)\n",
    "            y = y.long().cuda()\n",
    "            cls= model(x)\n",
    "            val_class_loss = class_criterion(cls,y)\n",
    "            acc = acc + torch.sum(y==torch.argmax(cls,axis=1))\n",
    "            N = N+len(y)\n",
    "            epoch_loss = epoch_loss + val_class_loss\n",
    "            if(y_pred is None):\n",
    "                y_pred = torch.argmax(cls,axis=1)\n",
    "                y_true = y\n",
    "            else:\n",
    "                y_pred = torch.cat((y_pred,torch.argmax(cls,axis=1)))\n",
    "                y_true = torch.cat((y_true,y))\n",
    "        Macc,sensitivity,specificity,precision,F1 = log_macc(y_pred,y_true,val_parts)\n",
    "        \n",
    "        print(\"Validation loss\", \"%.2f\"%(epoch_loss.item()/len(val_parts)),end=' ')\n",
    "        print(\"Validation Acc \", \"%.2f\"%(acc.item()/N))\n",
    "        logger.logs['val_loss'] = (epoch_loss.item()/len(val_parts))\n",
    "        logger.logs['val_acc'] = (acc.item()/N)\n",
    "        acc = (acc.item()/N)\n",
    "        logger.logs['val_macc'] = Macc\n",
    "        logger.logs['precision'] = precision\n",
    "        logger.logs['sensitivity'] = sensitivity\n",
    "        logger.logs['specificity'] = specificity\n",
    "        logger.logs['F1'] = F1\n",
    "#     lr_schedule.step()\n",
    "    torch.save(model.state_dict(),checkpoint_name.format(epoch=e,val_acc=acc,macc=Macc))\n",
    "    logger.on_epoch_end(e) \n",
    "    flow_source.reset()\n",
    "logger.on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "x,y = flow_source.next()\n",
    "x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
    "x = x.type(torch.FloatTensor).cuda()\n",
    "x,y = Variable(x),Variable(y)\n",
    "y = y.long().cuda()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = 0\n",
    "s = val_parts[0]\n",
    "x,y = x_val[start_idx:start_idx+s],y_val[start_idx:start_idx+s]\n",
    "start_idx = start_idx+s\n",
    "x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
    "x = x.type(torch.FloatTensor).cuda()\n",
    "x,y = Variable(x),Variable(y)\n",
    "y = y.long().cuda()\n",
    "# model.eval()\n",
    "cls= model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 240])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.mfcc(x)\n",
    "wow = out[0,:,:]\n",
    "wow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0375, -0.2427, -0.4996,  ..., -0.0284, -0.0284, -0.0284],\n",
       "        [-0.0394,  0.0554,  0.1440,  ..., -0.0218, -0.0218, -0.0218],\n",
       "        [-0.0090, -0.1058, -0.2108,  ..., -0.0267, -0.0267, -0.0267],\n",
       "        ...,\n",
       "        [-0.0552, -0.0556, -0.0565,  ..., -0.0564, -0.0564, -0.0564],\n",
       "        [-0.0341, -0.0341, -0.0341,  ..., -0.0340, -0.0340, -0.0340],\n",
       "        [-0.0271, -0.0265, -0.0252,  ..., -0.0253, -0.0253, -0.0253]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa1169de690>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "from matplotlib import cm\n",
    "plt.imshow(to_numpy(wow), interpolation='nearest', origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.6940e-03, 9.9631e-01],\n",
       "        [5.1695e-01, 4.8305e-01],\n",
       "        [2.6168e-13, 1.0000e+00],\n",
       "        [1.1031e-01, 8.8969e-01],\n",
       "        [9.9987e-01, 1.2957e-04],\n",
       "        [4.7420e-05, 9.9995e-01],\n",
       "        [8.8056e-01, 1.1944e-01],\n",
       "        [9.9893e-01, 1.0682e-03],\n",
       "        [9.3144e-01, 6.8557e-02],\n",
       "        [5.1704e-04, 9.9948e-01],\n",
       "        [8.8496e-09, 1.0000e+00],\n",
       "        [7.8377e-01, 2.1623e-01],\n",
       "        [9.9689e-01, 3.1070e-03],\n",
       "        [9.8369e-01, 1.6309e-02],\n",
       "        [6.2327e-13, 1.0000e+00],\n",
       "        [2.3746e-05, 9.9998e-01],\n",
       "        [2.6920e-07, 1.0000e+00],\n",
       "        [1.0000e+00, 6.8745e-08],\n",
       "        [1.0000e+00, 4.9925e-15],\n",
       "        [9.9670e-01, 3.2994e-03],\n",
       "        [1.4664e-09, 1.0000e+00],\n",
       "        [2.8193e-01, 7.1807e-01],\n",
       "        [1.0518e-03, 9.9895e-01],\n",
       "        [8.2756e-01, 1.7244e-01],\n",
       "        [1.6109e-03, 9.9839e-01],\n",
       "        [5.0453e-01, 4.9547e-01],\n",
       "        [9.9991e-01, 8.8796e-05],\n",
       "        [4.1290e-02, 9.5871e-01],\n",
       "        [1.1877e-06, 1.0000e+00],\n",
       "        [1.8631e-08, 1.0000e+00],\n",
       "        [3.7042e-23, 1.0000e+00],\n",
       "        [1.5789e-03, 9.9842e-01],\n",
       "        [5.6327e-02, 9.4367e-01],\n",
       "        [1.0000e+00, 2.8193e-11],\n",
       "        [8.3738e-06, 9.9999e-01],\n",
       "        [7.9532e-03, 9.9205e-01]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0, device='cuda:0'), tensor(0, device='cuda:0'))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.isnan(x)==True),torch.sum(torch.isnan(y)==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max X  tensor(0.6412, device='cuda:0') min X  tensor(-0.8642, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.3189, device='cuda:0', grad_fn=<NllLossBackward>),\n",
       " tensor([[1.0000e+00, 9.2045e-01, 1.0000e+00, 9.8792e-01, 1.0000e+00, 1.0000e+00,\n",
       "          2.5091e-02, 2.6845e-03, 8.1348e-08, 1.5315e-04, 1.2367e-10, 6.9464e-05],\n",
       "         [1.6387e-10, 7.9547e-02, 1.8382e-09, 1.2081e-02, 9.7659e-15, 1.0820e-11,\n",
       "          9.7491e-01, 9.9732e-01, 1.0000e+00, 9.9985e-01, 1.0000e+00, 9.9993e-01]],\n",
       "        device='cuda:0', grad_fn=<TransposeBackward0>))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('max X ',torch.max(x),'min X ',torch.min(x))\n",
    "cls = model(x)\n",
    "#         print(\"passed \",i+1)\n",
    "# class_loss = class_criterion(cls,torch.argmax(y,axis=1))        \n",
    "class_loss = class_criterion(cls,y)\n",
    "class_loss,cls.transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.0092e-02,  8.9175e-02,  7.5927e-02,  ...,  7.7499e-05,\n",
       "         -1.3485e-03, -1.8252e-03],\n",
       "        [ 7.0642e-02,  4.9792e-02,  4.8771e-02,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 2.6995e-02,  1.5088e-02,  2.7829e-02,  ...,  1.1403e-03,\n",
       "          1.5927e-03,  2.1556e-03],\n",
       "        ...,\n",
       "        [ 6.8235e-03,  8.3525e-03,  9.3509e-03,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 1.8575e-03,  2.4877e-03,  8.5851e-04,  ...,  6.3576e-04,\n",
       "          3.8280e-04,  2.0335e-04]], device='cuda:0')"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier.classifier.fc1.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.3252e+00, -6.1906e+00, -2.8883e+00,  ..., -6.2633e+01,\n",
       "          -5.2607e+01, -4.6591e+01]],\n",
       "\n",
       "        [[-6.0454e+00, -3.6201e+00, -7.1856e+00,  ..., -1.7213e+00,\n",
       "          -1.4995e+00,  1.6805e+00]],\n",
       "\n",
       "        [[ 6.9201e+00,  8.4345e-01,  1.2147e+00,  ..., -9.1274e+00,\n",
       "          -2.9250e+01, -4.1638e+01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 7.4721e+02,  4.4033e+02,  2.4653e+02,  ..., -2.1821e+03,\n",
       "          -2.4131e+03, -2.1528e+03]],\n",
       "\n",
       "        [[-3.3726e+02, -5.7196e+02, -5.6931e+02,  ..., -2.8876e+02,\n",
       "          -1.6951e+02,  1.3557e+02]],\n",
       "\n",
       "        [[-2.6684e+03, -2.5945e+03, -2.6421e+03,  ...,  1.9785e+02,\n",
       "          -1.6901e+03, -2.8535e+03]]], device='cuda:0')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.mfcc.gamma.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.mfcc.mfcc.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WHOLE_MODEL(\n",
       "  (mfcc): MFCC_Gen_coeff(\n",
       "    (gamma): Conv1d(1, 64, kernel_size=(81,), stride=(1,))\n",
       "    (gammanorm): BatchNorm1d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "    (mfcc): Conv1d(64, 64, kernel_size=(25,), stride=(10,), bias=False)\n",
       "    (normmfcc): BatchNorm1d(64, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "    (normmfcc2D): BatchNorm2d(1, eps=1e-05, momentum=0.99, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (classifier): Network(\n",
       "    (extractor): Extractor(\n",
       "      (conv0): Conv2d(1, 16, kernel_size=(3, 2), stride=(1, 1), padding=(3, 1))\n",
       "      (bn0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv11): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "      (bn11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv21): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (bn21): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv31): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(2, 1))\n",
       "      (bn31): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv41): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (bn41): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop): Dropout2d(p=0.5, inplace=False)\n",
       "    )\n",
       "    (classifier): Class_classifier(\n",
       "      (fc1): Linear(in_features=7168, out_features=100, bias=True)\n",
       "      (fc2): Linear(in_features=100, out_features=2, bias=True)\n",
       "      (relu): ReLU()\n",
       "      (soft): Softmax(dim=1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###   MFCC\n",
    "\n",
    "model.cuda()\n",
    "mfcc_gen.cuda()\n",
    "mfcc_gen.eval()\n",
    "epochs = 1500\n",
    "# torch.save(model,os.path.join(path,'model.pt'))\n",
    "logger.on_train_begin()\n",
    "print(\"steps \", flow_source.steps_per_epoch)\n",
    "\n",
    "# with open(path+fold+'/model1.json', 'w') as outfile:\n",
    "#     outfile.write(str(mod))\n",
    "for e in range(epochs):\n",
    "    print(\"EPOCH   \",e+1)\n",
    "    print(\"learning rate \",optimizer.param_groups[0]['lr'])\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    acc = 0\n",
    "    y_pred = None\n",
    "    y_true = None\n",
    "    N = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    for i in range(flow_source.steps_per_epoch+1):\n",
    "        \n",
    "        \n",
    "        x,y = flow_source.next()\n",
    "        if(str(class_criterion) in ['MSELoss()','BCELoss()']):\n",
    "            y = to_categorical(y,2)\n",
    "            y = y.astype(np.float32)\n",
    "        x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
    "        x = x.type(torch.FloatTensor).cuda()\n",
    "        holdx = x\n",
    "        x = mfcc_gen(x)\n",
    "#         if(len(x)>1):\n",
    "#             x = x[0]\n",
    "        hold = x\n",
    "        x = x.transpose(2,1)\n",
    "        x = x.unsqueeze(1)\n",
    "        # print(x.shape)\n",
    "        x,y = Variable(x),Variable(y)\n",
    "        \n",
    "#         y = y.cuda()\n",
    "        y = y.long().cuda()        \n",
    "        cls = model(x)\n",
    "        # class_loss = class_criterion(cls,torch.argmax(y,axis=1))        \n",
    "        loss = class_criterion(cls,y)\n",
    "        loss.backward()\n",
    "        epoch_loss = epoch_loss + loss\n",
    "        if(i%50==0 or i==flow_source.steps_per_epoch):            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "#         if(str(class_criterion)in ['MSELoss()','BCELoss()']):\n",
    "#             y = torch.argmax(y,axis=1)\n",
    "        if(y_pred is None):\n",
    "            y_pred = torch.argmax(cls,axis=1)\n",
    "            y_true = y\n",
    "        else:\n",
    "            y_pred = torch.cat((y_pred,torch.argmax(cls,axis=1)))\n",
    "            y_true = torch.cat((y_true,y))\n",
    "    \n",
    "        acc = acc + torch.sum(y==torch.argmax(cls,axis=1))\n",
    "        N = N+len(y)\n",
    "    epoch_loss_print = (epoch_loss.item()) if (type(epoch_loss)==torch.Tensor) else epoch_loss\n",
    "    acc_print = (acc.item()) if (type(acc)==torch.Tensor) else acc\n",
    "    print(\"Training loss\", \"%.2f\"%(epoch_loss_print/flow_source.steps_per_epoch),end=' ')\n",
    "    print(\"Training Acc \", \"%.2f\"%(acc_print/N),end=' ')\n",
    "    trainLog(y_true,y_pred)\n",
    "    logger.logs['train_loss'] = (epoch_loss_print/flow_source.steps_per_epoch)\n",
    "    logger.logs['train_acc'] = (acc_print/N)\n",
    "    # Validate \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    acc = 0\n",
    "    N = 0\n",
    "    y_pred = None\n",
    "    y_true = None\n",
    "    with torch.no_grad():\n",
    "        start_idx = 0\n",
    "        for i,s in enumerate(val_parts):\n",
    "            if(s==0):\n",
    "                continue\n",
    "            x,y = x_val[start_idx:start_idx+s],y_val[start_idx:start_idx+s]\n",
    "            start_idx = start_idx+s\n",
    "            \n",
    "            if(str(class_criterion) in ['MSELoss()','BCELoss()']):\n",
    "                y = to_categorical(y,2)\n",
    "                y = y.astype(np.float32)\n",
    "            \n",
    "            x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
    "            x = x.type(torch.FloatTensor).cuda()\n",
    "            holdvalx = x\n",
    "            x = mfcc_gen(x)\n",
    "#             if(len(x)>1):\n",
    "#                 x = x[0]\n",
    "            holdval = x\n",
    "            x = x.transpose(2,1)\n",
    "            x = x.unsqueeze(1)\n",
    "            x,y = Variable(x),Variable(y)\n",
    "            #x = x.reshape(x.shape[0],1,x.shape[1],x.shape[2])\n",
    "            \n",
    "#             y = y.cuda()\n",
    "            y = y.long().cuda()\n",
    "            cls= model(x)\n",
    "            # val_class_loss = class_criterion(cls,torch.argmax(y,axis=1))\n",
    "            val_class_loss = class_criterion(cls,y)\n",
    "            \n",
    "            if(str(class_criterion) in ['MSELoss()','BCELoss()']):\n",
    "                y = torch.argmax(y,axis=1)\n",
    "            \n",
    "            acc = acc + torch.sum(y==torch.argmax(cls,axis=1))\n",
    "            N = N+len(y)\n",
    "            epoch_loss = epoch_loss + val_class_loss\n",
    "            if(y_pred is None):\n",
    "                y_pred = torch.argmax(cls,axis=1)\n",
    "                y_true = y\n",
    "            else:\n",
    "                y_pred = torch.cat((y_pred,torch.argmax(cls,axis=1)))\n",
    "                y_true = torch.cat((y_true,y))\n",
    "        Macc,sensitivity,specificity,precision,F1 = log_macc(y_pred,y_true,val_parts)\n",
    "        \n",
    "        print(\"Validation loss\", \"%.2f\"%(epoch_loss.item()/len(val_parts)),end=' ')\n",
    "        print(\"Validation Acc \", \"%.2f\"%(acc.item()/N))\n",
    "        logger.logs['val_loss'] = (epoch_loss.item()/len(val_parts))\n",
    "        logger.logs['val_acc'] = (acc.item()/N)\n",
    "        acc = (acc.item()/N)\n",
    "        logger.logs['val_macc'] = Macc\n",
    "        logger.logs['precision'] = precision\n",
    "        logger.logs['sensitivity'] = sensitivity\n",
    "        logger.logs['specificity'] = specificity\n",
    "        logger.logs['F1'] = F1\n",
    "    lr_schedule.step()\n",
    "    torch.save(model.state_dict(),checkpoint_name.format(epoch=e,val_acc=acc,macc=Macc))\n",
    "    logger.on_epoch_end(e)\n",
    "    flow_source.reset()\n",
    "logger.on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HEartnet\n",
    "\n",
    "epochs = 1500\n",
    "# torch.save(model,os.path.join(path,'model.pt'))\n",
    "logger.on_train_begin()\n",
    "print(\"steps \", flow_source.steps_per_epoch)\n",
    "\n",
    "# with open(path+fold+'/model1.json', 'w') as outfile:\n",
    "#     outfile.write(str(mod))\n",
    "for e in range(epochs):\n",
    "    print(\"EPOCH   \",e+1)\n",
    "    print(\"learning rate \",optimizer.param_groups[0]['lr'])\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    acc = 0\n",
    "    y_pred = None\n",
    "    y_true = None\n",
    "    N = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    for i in range(flow_source.steps_per_epoch+1):\n",
    "        \n",
    "        \n",
    "        x,y = flow_source.next()\n",
    "        if(str(class_criterion) in ['MSELoss()','BCELoss()']):\n",
    "            y = to_categorical(y,2)\n",
    "            y = y.astype(np.float32)\n",
    "        x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
    "        x = x.type(torch.FloatTensor).cuda()\n",
    "        # print(x.shape)\n",
    "        x,y = Variable(x),Variable(y)\n",
    "        \n",
    "#         y = y.cuda()\n",
    "        y = y.long().cuda()        \n",
    "        cls = model(x)\n",
    "        # class_loss = class_criterion(cls,torch.argmax(y,axis=1))        \n",
    "        loss = class_criterion(cls,y)\n",
    "        loss.backward()\n",
    "        epoch_loss = epoch_loss + loss\n",
    "        if(i%50==0 or i==flow_source.steps_per_epoch):            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "#         if(str(class_criterion)in ['MSELoss()','BCELoss()']):\n",
    "#             y = torch.argmax(y,axis=1)\n",
    "        if(y_pred is None):\n",
    "            y_pred = torch.argmax(cls,axis=1)\n",
    "            y_true = y\n",
    "        else:\n",
    "            y_pred = torch.cat((y_pred,torch.argmax(cls,axis=1)))\n",
    "            y_true = torch.cat((y_true,y))\n",
    "    \n",
    "        acc = acc + torch.sum(y==torch.argmax(cls,axis=1))\n",
    "        N = N+len(y)\n",
    "    epoch_loss_print = (epoch_loss.item()) if (type(epoch_loss)==torch.Tensor) else epoch_loss\n",
    "    acc_print = (acc.item()) if (type(acc)==torch.Tensor) else acc\n",
    "    print(\"Training loss\", \"%.2f\"%(epoch_loss_print/flow_source.steps_per_epoch),end=' ')\n",
    "    print(\"Training Acc \", \"%.2f\"%(acc_print/N),end=' ')\n",
    "    trainLog(y_true,y_pred)\n",
    "    logger.logs['train_loss'] = (epoch_loss_print/flow_source.steps_per_epoch)\n",
    "    logger.logs['train_acc'] = (acc_print/N)\n",
    "    # Validate \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    acc = 0\n",
    "    N = 0\n",
    "    y_pred = None\n",
    "    y_true = None\n",
    "    with torch.no_grad():\n",
    "        start_idx = 0\n",
    "        for i,s in enumerate(val_parts):\n",
    "            if(s==0):\n",
    "                continue\n",
    "            x,y = x_val[start_idx:start_idx+s],y_val[start_idx:start_idx+s]\n",
    "            start_idx = start_idx+s\n",
    "            \n",
    "            if(str(class_criterion) in ['MSELoss()','BCELoss()']):\n",
    "                y = to_categorical(y,2)\n",
    "                y = y.astype(np.float32)\n",
    "            \n",
    "            x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
    "            x = x.type(torch.FloatTensor).cuda()\n",
    "            \n",
    "            x,y = Variable(x),Variable(y)\n",
    "            #x = x.reshape(x.shape[0],1,x.shape[1],x.shape[2])\n",
    "            \n",
    "#             y = y.cuda()\n",
    "            y = y.long().cuda()\n",
    "            cls= model(x)\n",
    "            # val_class_loss = class_criterion(cls,torch.argmax(y,axis=1))\n",
    "            val_class_loss = class_criterion(cls,y)\n",
    "            \n",
    "            if(str(class_criterion) in ['MSELoss()','BCELoss()']):\n",
    "                y = torch.argmax(y,axis=1)\n",
    "            \n",
    "            acc = acc + torch.sum(y==torch.argmax(cls,axis=1))\n",
    "            N = N+len(y)\n",
    "            epoch_loss = epoch_loss + val_class_loss\n",
    "            if(y_pred is None):\n",
    "                y_pred = torch.argmax(cls,axis=1)\n",
    "                y_true = y\n",
    "            else:\n",
    "                y_pred = torch.cat((y_pred,torch.argmax(cls,axis=1)))\n",
    "                y_true = torch.cat((y_true,y))\n",
    "        Macc,sensitivity,specificity,precision,F1 = log_macc(y_pred,y_true,val_parts)\n",
    "        \n",
    "        print(\"Validation loss\", \"%.2f\"%(epoch_loss.item()/len(val_parts)),end=' ')\n",
    "        print(\"Validation Acc \", \"%.2f\"%(acc.item()/N))\n",
    "        logger.logs['val_loss'] = (epoch_loss.item()/len(val_parts))\n",
    "        logger.logs['val_acc'] = (acc.item()/N)\n",
    "        acc = (acc.item()/N)\n",
    "        logger.logs['val_macc'] = Macc\n",
    "        logger.logs['precision'] = precision\n",
    "        logger.logs['sensitivity'] = sensitivity\n",
    "        logger.logs['specificity'] = specificity\n",
    "        logger.logs['F1'] = F1\n",
    "    lr_schedule.step()\n",
    "    torch.save(model.state_dict(),checkpoint_name.format(epoch=e,val_acc=acc,macc=Macc))\n",
    "    logger.on_epoch_end(e)\n",
    "    flow_source.reset()\n",
    "logger.on_train_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFCC gen(not learning) -> Classfier(learning) two modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../../Heartnet_Results/logs/gammatone_torch_layer/fold0_noFIR batch1500 continued .005_2020-04-14 16.10.52.323265/weights/weights.0154-acc_0.7942-macc_0.8296.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2]), torch.Size([12]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps  6483\n",
      "EPOCH    1\n",
      "learning rate  0.001\n",
      "Training loss 0.61 Training Acc  0.68 TN: 44755 FP: 20085 FN: 21183 TP: 43657\n",
      "Sensitivity: 0.67 Specificity: 0.69 Precision: 0.68 F1: 0.68 MACC 0.68\n",
      "TN: 119 FP: 27 FN: 78 TP: 60\n",
      "Sensitivity: 0.43 Specificity: 0.82 Precision: 0.69 F1: 0.53 MACC 0.62\n",
      "Validation loss 0.62 Validation Acc  0.67\n",
      "EPOCH    2\n",
      "learning rate  0.00095\n",
      "Training loss 0.54 Training Acc  0.77 TN: 48762 FP: 16078 FN: 14198 TP: 50642\n",
      "Sensitivity: 0.78 Specificity: 0.75 Precision: 0.76 F1: 0.77 MACC 0.77\n",
      "TN: 72 FP: 74 FN: 30 TP: 108\n",
      "Sensitivity: 0.78 Specificity: 0.49 Precision: 0.59 F1: 0.67 MACC 0.64\n",
      "Validation loss 0.60 Validation Acc  0.68\n",
      "EPOCH    3\n",
      "learning rate  0.0009025\n",
      "Training loss 0.50 Training Acc  0.81 TN: 50005 FP: 14835 FN: 10196 TP: 54644\n",
      "Sensitivity: 0.84 Specificity: 0.77 Precision: 0.79 F1: 0.81 MACC 0.81\n",
      "TN: 66 FP: 80 FN: 20 TP: 118\n",
      "Sensitivity: 0.86 Specificity: 0.45 Precision: 0.60 F1: 0.70 MACC 0.65\n",
      "Validation loss 0.61 Validation Acc  0.68\n",
      "EPOCH    4\n",
      "learning rate  0.000857375\n",
      "Training loss 0.48 Training Acc  0.83 TN: 50769 FP: 14071 FN: 8435 TP: 56405\n",
      "Sensitivity: 0.87 Specificity: 0.78 Precision: 0.80 F1: 0.83 MACC 0.83\n",
      "TN: 72 FP: 74 FN: 22 TP: 116\n",
      "Sensitivity: 0.84 Specificity: 0.49 Precision: 0.61 F1: 0.71 MACC 0.67\n",
      "Validation loss 0.61 Validation Acc  0.69\n",
      "EPOCH    5\n",
      "learning rate  0.0008145062499999999\n",
      "Training loss 0.47 Training Acc  0.84 TN: 51194 FP: 13646 FN: 7244 TP: 57596\n",
      "Sensitivity: 0.89 Specificity: 0.79 Precision: 0.81 F1: 0.85 MACC 0.84\n",
      "TN: 66 FP: 80 FN: 13 TP: 125\n",
      "Sensitivity: 0.91 Specificity: 0.45 Precision: 0.61 F1: 0.73 MACC 0.68\n",
      "Validation loss 0.60 Validation Acc  0.70\n",
      "EPOCH    6\n",
      "learning rate  0.0007737809374999998\n",
      "Training loss 0.46 Training Acc  0.85 TN: 51611 FP: 13229 FN: 6703 TP: 58137\n",
      "Sensitivity: 0.90 Specificity: 0.80 Precision: 0.81 F1: 0.85 MACC 0.85\n",
      "TN: 59 FP: 87 FN: 6 TP: 132\n",
      "Sensitivity: 0.96 Specificity: 0.40 Precision: 0.60 F1: 0.74 MACC 0.68\n",
      "Validation loss 0.60 Validation Acc  0.70\n",
      "EPOCH    7\n",
      "learning rate  0.0007350918906249997\n",
      "Training loss 0.46 Training Acc  0.85 TN: 51768 FP: 13072 FN: 6083 TP: 58757\n",
      "Sensitivity: 0.91 Specificity: 0.80 Precision: 0.82 F1: 0.86 MACC 0.85\n",
      "TN: 66 FP: 80 FN: 8 TP: 130\n",
      "Sensitivity: 0.94 Specificity: 0.45 Precision: 0.62 F1: 0.75 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    8\n",
      "learning rate  0.0006983372960937497\n",
      "Training loss 0.45 Training Acc  0.86 TN: 52117 FP: 12723 FN: 5735 TP: 59105\n",
      "Sensitivity: 0.91 Specificity: 0.80 Precision: 0.82 F1: 0.86 MACC 0.86\n",
      "TN: 63 FP: 83 FN: 6 TP: 132\n",
      "Sensitivity: 0.96 Specificity: 0.43 Precision: 0.61 F1: 0.75 MACC 0.69\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    9\n",
      "learning rate  0.0006634204312890621\n",
      "Training loss 0.45 Training Acc  0.86 TN: 52148 FP: 12692 FN: 5504 TP: 59336\n",
      "Sensitivity: 0.92 Specificity: 0.80 Precision: 0.82 F1: 0.87 MACC 0.86\n",
      "TN: 61 FP: 85 FN: 6 TP: 132\n",
      "Sensitivity: 0.96 Specificity: 0.42 Precision: 0.61 F1: 0.74 MACC 0.69\n",
      "Validation loss 0.60 Validation Acc  0.71\n",
      "EPOCH    10\n",
      "learning rate  0.000630249409724609\n",
      "Training loss 0.45 Training Acc  0.86 TN: 52345 FP: 12495 FN: 5386 TP: 59454\n",
      "Sensitivity: 0.92 Specificity: 0.81 Precision: 0.83 F1: 0.87 MACC 0.86\n",
      "TN: 63 FP: 83 FN: 5 TP: 133\n",
      "Sensitivity: 0.96 Specificity: 0.43 Precision: 0.62 F1: 0.75 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    11\n",
      "learning rate  0.0005987369392383785\n",
      "Training loss 0.44 Training Acc  0.87 TN: 52440 FP: 12400 FN: 5080 TP: 59760\n",
      "Sensitivity: 0.92 Specificity: 0.81 Precision: 0.83 F1: 0.87 MACC 0.87\n",
      "TN: 69 FP: 77 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.47 Precision: 0.64 F1: 0.77 MACC 0.73\n",
      "Validation loss 0.58 Validation Acc  0.73\n",
      "EPOCH    12\n",
      "learning rate  0.0005688000922764595\n",
      "Training loss 0.44 Training Acc  0.87 TN: 52656 FP: 12184 FN: 4977 TP: 59863\n",
      "Sensitivity: 0.92 Specificity: 0.81 Precision: 0.83 F1: 0.87 MACC 0.87\n",
      "TN: 66 FP: 80 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.45 Precision: 0.63 F1: 0.77 MACC 0.72\n",
      "Validation loss 0.58 Validation Acc  0.73\n",
      "EPOCH    13\n",
      "learning rate  0.0005403600876626365\n",
      "Training loss 0.44 Training Acc  0.87 TN: 52651 FP: 12189 FN: 4744 TP: 60096\n",
      "Sensitivity: 0.93 Specificity: 0.81 Precision: 0.83 F1: 0.88 MACC 0.87\n",
      "TN: 63 FP: 83 FN: 4 TP: 134\n",
      "Sensitivity: 0.97 Specificity: 0.43 Precision: 0.62 F1: 0.75 MACC 0.70\n",
      "Validation loss 0.58 Validation Acc  0.73\n",
      "EPOCH    14\n",
      "learning rate  0.0005133420832795047\n",
      "Training loss 0.44 Training Acc  0.87 TN: 52706 FP: 12134 FN: 4767 TP: 60073\n",
      "Sensitivity: 0.93 Specificity: 0.81 Precision: 0.83 F1: 0.88 MACC 0.87\n",
      "TN: 64 FP: 82 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    15\n",
      "learning rate  0.00048767497911552944\n",
      "Training loss 0.44 Training Acc  0.87 TN: 52893 FP: 11947 FN: 4690 TP: 60150\n",
      "Sensitivity: 0.93 Specificity: 0.82 Precision: 0.83 F1: 0.88 MACC 0.87\n",
      "TN: 64 FP: 82 FN: 4 TP: 134\n",
      "Sensitivity: 0.97 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.58 Validation Acc  0.73\n",
      "EPOCH    16\n",
      "learning rate  0.00046329123015975297\n",
      "Training loss 0.44 Training Acc  0.87 TN: 52908 FP: 11932 FN: 4427 TP: 60413\n",
      "Sensitivity: 0.93 Specificity: 0.82 Precision: 0.84 F1: 0.88 MACC 0.87\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    17\n",
      "learning rate  0.0004401266686517653\n",
      "Training loss 0.44 Training Acc  0.87 TN: 52974 FP: 11866 FN: 4454 TP: 60386\n",
      "Sensitivity: 0.93 Specificity: 0.82 Precision: 0.84 F1: 0.88 MACC 0.87\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    18\n",
      "learning rate  0.00041812033521917703\n",
      "Training loss 0.44 Training Acc  0.87 TN: 53053 FP: 11787 FN: 4589 TP: 60251\n",
      "Sensitivity: 0.93 Specificity: 0.82 Precision: 0.84 F1: 0.88 MACC 0.87\n",
      "TN: 63 FP: 83 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    19\n",
      "learning rate  0.00039721431845821814\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53100 FP: 11740 FN: 4339 TP: 60501\n",
      "Sensitivity: 0.93 Specificity: 0.82 Precision: 0.84 F1: 0.88 MACC 0.88\n",
      "TN: 65 FP: 81 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.45 Precision: 0.63 F1: 0.77 MACC 0.72\n",
      "Validation loss 0.58 Validation Acc  0.73\n",
      "EPOCH    20\n",
      "learning rate  0.0003773536025353072\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53141 FP: 11699 FN: 4326 TP: 60514\n",
      "Sensitivity: 0.93 Specificity: 0.82 Precision: 0.84 F1: 0.88 MACC 0.88\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    21\n",
      "learning rate  0.0003584859224085418\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53196 FP: 11644 FN: 4287 TP: 60553\n",
      "Sensitivity: 0.93 Specificity: 0.82 Precision: 0.84 F1: 0.88 MACC 0.88\n",
      "TN: 65 FP: 81 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.45 Precision: 0.63 F1: 0.77 MACC 0.72\n",
      "Validation loss 0.58 Validation Acc  0.73\n",
      "EPOCH    22\n",
      "learning rate  0.0003405616262881147\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53296 FP: 11544 FN: 4240 TP: 60600\n",
      "Sensitivity: 0.93 Specificity: 0.82 Precision: 0.84 F1: 0.88 MACC 0.88\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    23\n",
      "learning rate  0.00032353354497370894\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53294 FP: 11546 FN: 4123 TP: 60717\n",
      "Sensitivity: 0.94 Specificity: 0.82 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 64 FP: 82 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    24\n",
      "learning rate  0.00030735686772502346\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53263 FP: 11577 FN: 4157 TP: 60683\n",
      "Sensitivity: 0.94 Specificity: 0.82 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    25\n",
      "learning rate  0.00029198902433877225\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53354 FP: 11486 FN: 4065 TP: 60775\n",
      "Sensitivity: 0.94 Specificity: 0.82 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 66 FP: 80 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.45 Precision: 0.63 F1: 0.76 MACC 0.72\n",
      "Validation loss 0.58 Validation Acc  0.73\n",
      "EPOCH    26\n",
      "learning rate  0.00027738957312183364\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53303 FP: 11537 FN: 4156 TP: 60684\n",
      "Sensitivity: 0.94 Specificity: 0.82 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 65 FP: 81 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.45 Precision: 0.63 F1: 0.77 MACC 0.72\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    27\n",
      "learning rate  0.0002635200944657419\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53423 FP: 11417 FN: 4070 TP: 60770\n",
      "Sensitivity: 0.94 Specificity: 0.82 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 65 FP: 81 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.45 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    28\n",
      "learning rate  0.0002503440897424548\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53291 FP: 11549 FN: 3898 TP: 60942\n",
      "Sensitivity: 0.94 Specificity: 0.82 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    29\n",
      "learning rate  0.00023782688525533205\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53538 FP: 11302 FN: 3967 TP: 60873\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    30\n",
      "learning rate  0.00022593554099256544\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53404 FP: 11436 FN: 3924 TP: 60916\n",
      "Sensitivity: 0.94 Specificity: 0.82 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 59 FP: 87 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.40 Precision: 0.61 F1: 0.75 MACC 0.69\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    31\n",
      "learning rate  0.00021463876394293716\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53500 FP: 11340 FN: 3906 TP: 60934\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 63 FP: 83 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    32\n",
      "learning rate  0.0002039068257457903\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53447 FP: 11393 FN: 3962 TP: 60878\n",
      "Sensitivity: 0.94 Specificity: 0.82 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    33\n",
      "learning rate  0.00019371148445850077\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53567 FP: 11273 FN: 3907 TP: 60933\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    34\n",
      "learning rate  0.00018402591023557573\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53446 FP: 11394 FN: 3883 TP: 60957\n",
      "Sensitivity: 0.94 Specificity: 0.82 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    35\n",
      "learning rate  0.00017482461472379692\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53544 FP: 11296 FN: 3759 TP: 61081\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    36\n",
      "learning rate  0.00016608338398760707\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53534 FP: 11306 FN: 3741 TP: 61099\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    37\n",
      "learning rate  0.0001577792147882267\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53593 FP: 11247 FN: 3713 TP: 61127\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    38\n",
      "learning rate  0.00014989025404881537\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53599 FP: 11241 FN: 3704 TP: 61136\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 59 FP: 87 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.40 Precision: 0.61 F1: 0.75 MACC 0.69\n",
      "Validation loss 0.60 Validation Acc  0.72\n",
      "EPOCH    39\n",
      "learning rate  0.00014239574134637458\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53582 FP: 11258 FN: 3739 TP: 61101\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    40\n",
      "learning rate  0.00013527595427905584\n",
      "Training loss 0.43 Training Acc  0.89 TN: 53651 FP: 11189 FN: 3695 TP: 61145\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    41\n",
      "learning rate  0.00012851215656510304\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53646 FP: 11194 FN: 3758 TP: 61082\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.88\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    42\n",
      "learning rate  0.00012208654873684788\n",
      "Training loss 0.43 Training Acc  0.89 TN: 53615 FP: 11225 FN: 3670 TP: 61170\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.89\n",
      "TN: 65 FP: 81 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.45 Precision: 0.63 F1: 0.77 MACC 0.72\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    43\n",
      "learning rate  0.00011598222130000548\n",
      "Training loss 0.43 Training Acc  0.89 TN: 53552 FP: 11288 FN: 3607 TP: 61233\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    44\n",
      "learning rate  0.00011018311023500519\n",
      "Training loss 0.43 Training Acc  0.89 TN: 53618 FP: 11222 FN: 3685 TP: 61155\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    45\n",
      "learning rate  0.00010467395472325493\n",
      "Training loss 0.43 Training Acc  0.89 TN: 53687 FP: 11153 FN: 3740 TP: 61100\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    46\n",
      "learning rate  9.944025698709218e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53744 FP: 11096 FN: 3706 TP: 61134\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    47\n",
      "learning rate  9.446824413773756e-05\n",
      "Training loss 0.43 Training Acc  0.88 TN: 53616 FP: 11224 FN: 3698 TP: 61142\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.84 F1: 0.89 MACC 0.88\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    48\n",
      "learning rate  8.974483193085068e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53713 FP: 11127 FN: 3643 TP: 61197\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    49\n",
      "learning rate  8.525759033430814e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53662 FP: 11178 FN: 3629 TP: 61211\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    50\n",
      "learning rate  8.099471081759274e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53720 FP: 11120 FN: 3623 TP: 61217\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    51\n",
      "learning rate  7.69449752767131e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53722 FP: 11118 FN: 3638 TP: 61202\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    52\n",
      "learning rate  7.309772651287744e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53651 FP: 11189 FN: 3622 TP: 61218\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    53\n",
      "learning rate  6.944284018723356e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53724 FP: 11116 FN: 3585 TP: 61255\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    54\n",
      "learning rate  6.597069817787189e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53741 FP: 11099 FN: 3563 TP: 61277\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    55\n",
      "learning rate  6.267216326897829e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53629 FP: 11211 FN: 3612 TP: 61228\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    56\n",
      "learning rate  5.953855510552937e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53769 FP: 11071 FN: 3623 TP: 61217\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    57\n",
      "learning rate  5.65616273502529e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53661 FP: 11179 FN: 3555 TP: 61285\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    58\n",
      "learning rate  5.373354598274025e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53716 FP: 11124 FN: 3553 TP: 61287\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    59\n",
      "learning rate  5.104686868360323e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53706 FP: 11134 FN: 3507 TP: 61333\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    60\n",
      "learning rate  4.849452524942307e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53670 FP: 11170 FN: 3602 TP: 61238\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    61\n",
      "learning rate  4.606979898695191e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53747 FP: 11093 FN: 3587 TP: 61253\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    62\n",
      "learning rate  4.376630903760431e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53688 FP: 11152 FN: 3547 TP: 61293\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    63\n",
      "learning rate  4.157799358572409e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53779 FP: 11061 FN: 3521 TP: 61319\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    64\n",
      "learning rate  3.9499093906437885e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53846 FP: 10994 FN: 3583 TP: 61257\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    65\n",
      "learning rate  3.752413921111599e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53674 FP: 11166 FN: 3472 TP: 61368\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    66\n",
      "learning rate  3.564793225056019e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53722 FP: 11118 FN: 3546 TP: 61294\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    67\n",
      "learning rate  3.3865535638032174e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53726 FP: 11114 FN: 3506 TP: 61334\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    68\n",
      "learning rate  3.2172258856130564e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53727 FP: 11113 FN: 3566 TP: 61274\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    69\n",
      "learning rate  3.056364591332403e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53723 FP: 11117 FN: 3500 TP: 61340\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    70\n",
      "learning rate  2.903546361765783e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53778 FP: 11062 FN: 3486 TP: 61354\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    71\n",
      "learning rate  2.758369043677494e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53824 FP: 11016 FN: 3481 TP: 61359\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    72\n",
      "learning rate  2.620450591493619e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53759 FP: 11081 FN: 3600 TP: 61240\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    73\n",
      "learning rate  2.489428061918938e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53713 FP: 11127 FN: 3512 TP: 61328\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    74\n",
      "learning rate  2.364956658822991e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53776 FP: 11064 FN: 3502 TP: 61338\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    75\n",
      "learning rate  2.2467088258818413e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53791 FP: 11049 FN: 3555 TP: 61285\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    76\n",
      "learning rate  2.134373384587749e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53766 FP: 11074 FN: 3498 TP: 61342\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    77\n",
      "learning rate  2.0276547153583614e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53762 FP: 11078 FN: 3450 TP: 61390\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    78\n",
      "learning rate  1.9262719795904432e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53838 FP: 11002 FN: 3642 TP: 61198\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    79\n",
      "learning rate  1.829958380610921e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53771 FP: 11069 FN: 3468 TP: 61372\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    80\n",
      "learning rate  1.738460461580375e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53803 FP: 11037 FN: 3493 TP: 61347\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    81\n",
      "learning rate  1.6515374385013564e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53821 FP: 11019 FN: 3542 TP: 61298\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    82\n",
      "learning rate  1.5689605665762886e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53823 FP: 11017 FN: 3516 TP: 61324\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    83\n",
      "learning rate  1.490512538247474e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53814 FP: 11026 FN: 3508 TP: 61332\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    84\n",
      "learning rate  1.4159869113351003e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53779 FP: 11061 FN: 3489 TP: 61351\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    85\n",
      "learning rate  1.3451875657683452e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53772 FP: 11068 FN: 3489 TP: 61351\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    86\n",
      "learning rate  1.277928187479928e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53820 FP: 11020 FN: 3493 TP: 61347\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    87\n",
      "learning rate  1.2140317781059316e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53727 FP: 11113 FN: 3606 TP: 61234\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    88\n",
      "learning rate  1.153330189200635e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53759 FP: 11081 FN: 3456 TP: 61384\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    89\n",
      "learning rate  1.0956636797406032e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53810 FP: 11030 FN: 3529 TP: 61311\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    90\n",
      "learning rate  1.0408804957535729e-05\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53776 FP: 11064 FN: 3462 TP: 61378\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    91\n",
      "learning rate  9.888364709658941e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53830 FP: 11010 FN: 3521 TP: 61319\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    92\n",
      "learning rate  9.393946474175994e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53839 FP: 11001 FN: 3422 TP: 61418\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    93\n",
      "learning rate  8.924249150467194e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53798 FP: 11042 FN: 3464 TP: 61376\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    94\n",
      "learning rate  8.478036692943835e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53778 FP: 11062 FN: 3473 TP: 61367\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    95\n",
      "learning rate  8.054134858296643e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53803 FP: 11037 FN: 3419 TP: 61421\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    96\n",
      "learning rate  7.65142811538181e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53787 FP: 11053 FN: 3476 TP: 61364\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    97\n",
      "learning rate  7.26885670961272e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53803 FP: 11037 FN: 3530 TP: 61310\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    98\n",
      "learning rate  6.905413874132084e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53797 FP: 11043 FN: 3444 TP: 61396\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    99\n",
      "learning rate  6.5601431804254795e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53733 FP: 11107 FN: 3456 TP: 61384\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    100\n",
      "learning rate  6.232136021404205e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53842 FP: 10998 FN: 3462 TP: 61378\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    101\n",
      "learning rate  5.920529220333994e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53823 FP: 11017 FN: 3531 TP: 61309\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    102\n",
      "learning rate  5.624502759317295e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53778 FP: 11062 FN: 3428 TP: 61412\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    103\n",
      "learning rate  5.34327762135143e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53799 FP: 11041 FN: 3479 TP: 61361\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    104\n",
      "learning rate  5.076113740283858e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53856 FP: 10984 FN: 3557 TP: 61283\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    105\n",
      "learning rate  4.8223080532696655e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53740 FP: 11100 FN: 3381 TP: 61459\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    106\n",
      "learning rate  4.581192650606182e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53855 FP: 10985 FN: 3402 TP: 61438\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    107\n",
      "learning rate  4.3521330180758725e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53835 FP: 11005 FN: 3414 TP: 61426\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    108\n",
      "learning rate  4.1345263671720786e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53714 FP: 11126 FN: 3457 TP: 61383\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    109\n",
      "learning rate  3.927800048813474e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53807 FP: 11033 FN: 3453 TP: 61387\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    110\n",
      "learning rate  3.7314100463728006e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53725 FP: 11115 FN: 3411 TP: 61429\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    111\n",
      "learning rate  3.5448395440541604e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53838 FP: 11002 FN: 3529 TP: 61311\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    112\n",
      "learning rate  3.3675975668514524e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53795 FP: 11045 FN: 3424 TP: 61416\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    113\n",
      "learning rate  3.1992176885088796e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53707 FP: 11133 FN: 3445 TP: 61395\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    114\n",
      "learning rate  3.0392568040834356e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53756 FP: 11084 FN: 3583 TP: 61257\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    115\n",
      "learning rate  2.8872939638792635e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53785 FP: 11055 FN: 3441 TP: 61399\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    116\n",
      "learning rate  2.7429292656853003e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53753 FP: 11087 FN: 3495 TP: 61345\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    117\n",
      "learning rate  2.605782802401035e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53713 FP: 11127 FN: 3393 TP: 61447\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 59 FP: 87 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.40 Precision: 0.61 F1: 0.75 MACC 0.69\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    118\n",
      "learning rate  2.475493662280983e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53830 FP: 11010 FN: 3515 TP: 61325\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    119\n",
      "learning rate  2.351718979166934e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53894 FP: 10946 FN: 3488 TP: 61352\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    120\n",
      "learning rate  2.234133030208587e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53897 FP: 10943 FN: 3451 TP: 61389\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    121\n",
      "learning rate  2.1224263786981576e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53799 FP: 11041 FN: 3495 TP: 61345\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    122\n",
      "learning rate  2.0163050597632494e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53792 FP: 11048 FN: 3526 TP: 61314\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    123\n",
      "learning rate  1.915489806775087e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53903 FP: 10937 FN: 3424 TP: 61416\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    124\n",
      "learning rate  1.8197153164363325e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53682 FP: 11158 FN: 3488 TP: 61352\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    125\n",
      "learning rate  1.7287295506145157e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53835 FP: 11005 FN: 3497 TP: 61343\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    126\n",
      "learning rate  1.6422930730837899e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53814 FP: 11026 FN: 3417 TP: 61423\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    127\n",
      "learning rate  1.5601784194296004e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53839 FP: 11001 FN: 3462 TP: 61378\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    128\n",
      "learning rate  1.4821694984581202e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53868 FP: 10972 FN: 3476 TP: 61364\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    129\n",
      "learning rate  1.4080610235352142e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53869 FP: 10971 FN: 3499 TP: 61341\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    130\n",
      "learning rate  1.3376579723584535e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53813 FP: 11027 FN: 3519 TP: 61321\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 65 FP: 81 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.45 Precision: 0.63 F1: 0.77 MACC 0.72\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    131\n",
      "learning rate  1.2707750737405307e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53824 FP: 11016 FN: 3516 TP: 61324\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    132\n",
      "learning rate  1.2072363200535042e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53816 FP: 11024 FN: 3493 TP: 61347\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    133\n",
      "learning rate  1.146874504050829e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53871 FP: 10969 FN: 3406 TP: 61434\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    134\n",
      "learning rate  1.0895307788482876e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53781 FP: 11059 FN: 3526 TP: 61314\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    135\n",
      "learning rate  1.0350542399058731e-06\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53803 FP: 11037 FN: 3498 TP: 61342\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    136\n",
      "learning rate  9.833015279105794e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53829 FP: 11011 FN: 3502 TP: 61338\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    137\n",
      "learning rate  9.341364515150503e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53751 FP: 11089 FN: 3467 TP: 61373\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    138\n",
      "learning rate  8.874296289392978e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53869 FP: 10971 FN: 3469 TP: 61371\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    139\n",
      "learning rate  8.430581474923329e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53857 FP: 10983 FN: 3465 TP: 61375\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    140\n",
      "learning rate  8.009052401177162e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53772 FP: 11068 FN: 3497 TP: 61343\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    141\n",
      "learning rate  7.608599781118303e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53902 FP: 10938 FN: 3422 TP: 61418\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    142\n",
      "learning rate  7.228169792062388e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53828 FP: 11012 FN: 3489 TP: 61351\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    143\n",
      "learning rate  6.866761302459269e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53799 FP: 11041 FN: 3489 TP: 61351\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 65 FP: 81 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.45 Precision: 0.63 F1: 0.77 MACC 0.72\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    144\n",
      "learning rate  6.523423237336305e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53795 FP: 11045 FN: 3447 TP: 61393\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 65 FP: 81 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.45 Precision: 0.63 F1: 0.77 MACC 0.72\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    145\n",
      "learning rate  6.197252075469489e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53808 FP: 11032 FN: 3457 TP: 61383\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    146\n",
      "learning rate  5.887389471696014e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53804 FP: 11036 FN: 3526 TP: 61314\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    147\n",
      "learning rate  5.593019998111213e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53786 FP: 11054 FN: 3559 TP: 61281\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    148\n",
      "learning rate  5.313368998205652e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53814 FP: 11026 FN: 3444 TP: 61396\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    149\n",
      "learning rate  5.047700548295369e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53834 FP: 11006 FN: 3470 TP: 61370\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    150\n",
      "learning rate  4.7953155208806e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53770 FP: 11070 FN: 3414 TP: 61426\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    151\n",
      "learning rate  4.55554974483657e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53850 FP: 10990 FN: 3449 TP: 61391\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    152\n",
      "learning rate  4.327772257594741e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53864 FP: 10976 FN: 3528 TP: 61312\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    153\n",
      "learning rate  4.111383644715004e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53805 FP: 11035 FN: 3506 TP: 61334\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    154\n",
      "learning rate  3.905814462479254e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53854 FP: 10986 FN: 3503 TP: 61337\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    155\n",
      "learning rate  3.710523739355291e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53889 FP: 10951 FN: 3433 TP: 61407\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    156\n",
      "learning rate  3.524997552387526e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53806 FP: 11034 FN: 3522 TP: 61318\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    157\n",
      "learning rate  3.34874767476815e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53799 FP: 11041 FN: 3495 TP: 61345\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    158\n",
      "learning rate  3.181310291029742e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53775 FP: 11065 FN: 3449 TP: 61391\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    159\n",
      "learning rate  3.0222447764782547e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53876 FP: 10964 FN: 3547 TP: 61293\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    160\n",
      "learning rate  2.8711325376543416e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53774 FP: 11066 FN: 3497 TP: 61343\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    161\n",
      "learning rate  2.7275759107716244e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53791 FP: 11049 FN: 3488 TP: 61352\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    162\n",
      "learning rate  2.591197115233043e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53827 FP: 11013 FN: 3504 TP: 61336\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    163\n",
      "learning rate  2.4616372594713905e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53843 FP: 10997 FN: 3492 TP: 61348\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    164\n",
      "learning rate  2.3385553964978208e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53873 FP: 10967 FN: 3533 TP: 61307\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    165\n",
      "learning rate  2.2216276266729297e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53883 FP: 10957 FN: 3505 TP: 61335\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    166\n",
      "learning rate  2.110546245339283e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53878 FP: 10962 FN: 3467 TP: 61373\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    167\n",
      "learning rate  2.0050189330723186e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53875 FP: 10965 FN: 3453 TP: 61387\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    168\n",
      "learning rate  1.9047679864187027e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53879 FP: 10961 FN: 3400 TP: 61440\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    169\n",
      "learning rate  1.8095295870977674e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53778 FP: 11062 FN: 3433 TP: 61407\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    170\n",
      "learning rate  1.719053107742879e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53813 FP: 11027 FN: 3442 TP: 61398\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    171\n",
      "learning rate  1.633100452355735e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53850 FP: 10990 FN: 3481 TP: 61359\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    172\n",
      "learning rate  1.5514454297379483e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53778 FP: 11062 FN: 3452 TP: 61388\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    173\n",
      "learning rate  1.473873158251051e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53792 FP: 11048 FN: 3420 TP: 61420\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    174\n",
      "learning rate  1.4001795003384983e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53878 FP: 10962 FN: 3486 TP: 61354\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    175\n",
      "learning rate  1.3301705253215733e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53833 FP: 11007 FN: 3487 TP: 61353\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    176\n",
      "learning rate  1.2636619990554947e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53834 FP: 11006 FN: 3514 TP: 61326\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    177\n",
      "learning rate  1.20047889910272e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53806 FP: 11034 FN: 3468 TP: 61372\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    178\n",
      "learning rate  1.1404549541475839e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53721 FP: 11119 FN: 3499 TP: 61341\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    179\n",
      "learning rate  1.0834322064402047e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53753 FP: 11087 FN: 3489 TP: 61351\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    180\n",
      "learning rate  1.0292605961181944e-07\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53792 FP: 11048 FN: 3489 TP: 61351\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    181\n",
      "learning rate  9.777975663122845e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53789 FP: 11051 FN: 3498 TP: 61342\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    182\n",
      "learning rate  9.289076879966702e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53759 FP: 11081 FN: 3426 TP: 61414\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    183\n",
      "learning rate  8.824623035968367e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53811 FP: 11029 FN: 3414 TP: 61426\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    184\n",
      "learning rate  8.383391884169949e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53818 FP: 11022 FN: 3411 TP: 61429\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    185\n",
      "learning rate  7.964222289961451e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53781 FP: 11059 FN: 3524 TP: 61316\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    186\n",
      "learning rate  7.566011175463377e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53776 FP: 11064 FN: 3447 TP: 61393\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    187\n",
      "learning rate  7.187710616690207e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53787 FP: 11053 FN: 3489 TP: 61351\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    188\n",
      "learning rate  6.828325085855697e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53840 FP: 11000 FN: 3479 TP: 61361\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    189\n",
      "learning rate  6.486908831562912e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53716 FP: 11124 FN: 3460 TP: 61380\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    190\n",
      "learning rate  6.162563389984765e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53839 FP: 11001 FN: 3539 TP: 61301\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.42 Precision: 0.61 F1: 0.75 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    191\n",
      "learning rate  5.854435220485527e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53824 FP: 11016 FN: 3500 TP: 61340\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    192\n",
      "learning rate  5.56171345946125e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53807 FP: 11033 FN: 3447 TP: 61393\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    193\n",
      "learning rate  5.2836277864881873e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53759 FP: 11081 FN: 3359 TP: 61481\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    194\n",
      "learning rate  5.019446397163778e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53814 FP: 11026 FN: 3460 TP: 61380\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    195\n",
      "learning rate  4.7684740773055885e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53832 FP: 11008 FN: 3493 TP: 61347\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    196\n",
      "learning rate  4.530050373440309e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53796 FP: 11044 FN: 3465 TP: 61375\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    197\n",
      "learning rate  4.3035478547682935e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53791 FP: 11049 FN: 3456 TP: 61384\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    198\n",
      "learning rate  4.0883704620298786e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53871 FP: 10969 FN: 3475 TP: 61365\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    199\n",
      "learning rate  3.883951938928385e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53816 FP: 11024 FN: 3506 TP: 61334\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    200\n",
      "learning rate  3.689754341981965e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53828 FP: 11012 FN: 3447 TP: 61393\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    201\n",
      "learning rate  3.505266624882867e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53804 FP: 11036 FN: 3452 TP: 61388\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    202\n",
      "learning rate  3.330003293638723e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53867 FP: 10973 FN: 3461 TP: 61379\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    203\n",
      "learning rate  3.1635031289567866e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53809 FP: 11031 FN: 3484 TP: 61356\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    204\n",
      "learning rate  3.005327972508947e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53788 FP: 11052 FN: 3468 TP: 61372\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    205\n",
      "learning rate  2.8550615738834995e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53818 FP: 11022 FN: 3536 TP: 61304\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    206\n",
      "learning rate  2.7123084951893245e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53740 FP: 11100 FN: 3444 TP: 61396\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    207\n",
      "learning rate  2.576693070429858e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53823 FP: 11017 FN: 3454 TP: 61386\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    208\n",
      "learning rate  2.447858416908365e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53791 FP: 11049 FN: 3481 TP: 61359\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    209\n",
      "learning rate  2.3254654960629467e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53831 FP: 11009 FN: 3482 TP: 61358\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    210\n",
      "learning rate  2.2091922212597992e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53711 FP: 11129 FN: 3431 TP: 61409\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    211\n",
      "learning rate  2.0987326101968092e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53797 FP: 11043 FN: 3486 TP: 61354\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    212\n",
      "learning rate  1.9937959796869687e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53803 FP: 11037 FN: 3507 TP: 61333\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    213\n",
      "learning rate  1.8941061807026202e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53794 FP: 11046 FN: 3414 TP: 61426\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    214\n",
      "learning rate  1.799400871667489e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53778 FP: 11062 FN: 3495 TP: 61345\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    215\n",
      "learning rate  1.7094308280841145e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53812 FP: 11028 FN: 3431 TP: 61409\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    216\n",
      "learning rate  1.6239592866799087e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53835 FP: 11005 FN: 3433 TP: 61407\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    217\n",
      "learning rate  1.5427613223459133e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53782 FP: 11058 FN: 3509 TP: 61331\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    218\n",
      "learning rate  1.4656232562286176e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53846 FP: 10994 FN: 3492 TP: 61348\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    219\n",
      "learning rate  1.3923420934171867e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53763 FP: 11077 FN: 3474 TP: 61366\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    220\n",
      "learning rate  1.3227249887463272e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53849 FP: 10991 FN: 3499 TP: 61341\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    221\n",
      "learning rate  1.2565887393090107e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53786 FP: 11054 FN: 3494 TP: 61346\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    222\n",
      "learning rate  1.1937593023435602e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53782 FP: 11058 FN: 3439 TP: 61401\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    223\n",
      "learning rate  1.1340713372263822e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53801 FP: 11039 FN: 3487 TP: 61353\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    224\n",
      "learning rate  1.077367770365063e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53813 FP: 11027 FN: 3458 TP: 61382\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    225\n",
      "learning rate  1.0234993818468098e-08\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53796 FP: 11044 FN: 3464 TP: 61376\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    226\n",
      "learning rate  9.723244127544693e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53785 FP: 11055 FN: 3452 TP: 61388\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    227\n",
      "learning rate  9.237081921167457e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53789 FP: 11051 FN: 3477 TP: 61363\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    228\n",
      "learning rate  8.775227825109085e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53884 FP: 10956 FN: 3453 TP: 61387\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    229\n",
      "learning rate  8.33646643385363e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53803 FP: 11037 FN: 3429 TP: 61411\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    230\n",
      "learning rate  7.919643112160947e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53809 FP: 11031 FN: 3458 TP: 61382\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    231\n",
      "learning rate  7.5236609565529e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53809 FP: 11031 FN: 3498 TP: 61342\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    232\n",
      "learning rate  7.147477908725255e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53783 FP: 11057 FN: 3458 TP: 61382\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    233\n",
      "learning rate  6.790104013288992e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53804 FP: 11036 FN: 3495 TP: 61345\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    234\n",
      "learning rate  6.450598812624543e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53805 FP: 11035 FN: 3554 TP: 61286\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    235\n",
      "learning rate  6.1280688719933155e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53746 FP: 11094 FN: 3506 TP: 61334\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    236\n",
      "learning rate  5.8216654283936495e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53800 FP: 11040 FN: 3504 TP: 61336\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    237\n",
      "learning rate  5.530582156973966e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53835 FP: 11005 FN: 3423 TP: 61417\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    238\n",
      "learning rate  5.254053049125268e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53824 FP: 11016 FN: 3479 TP: 61361\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    239\n",
      "learning rate  4.991350396669004e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53798 FP: 11042 FN: 3465 TP: 61375\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    240\n",
      "learning rate  4.741782876835554e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53803 FP: 11037 FN: 3460 TP: 61380\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    241\n",
      "learning rate  4.504693732993776e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53820 FP: 11020 FN: 3499 TP: 61341\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    242\n",
      "learning rate  4.279459046344087e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53810 FP: 11030 FN: 3495 TP: 61345\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    243\n",
      "learning rate  4.065486094026883e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53846 FP: 10994 FN: 3476 TP: 61364\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    244\n",
      "learning rate  3.862211789325538e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53848 FP: 10992 FN: 3510 TP: 61330\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    245\n",
      "learning rate  3.6691011998592615e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53795 FP: 11045 FN: 3454 TP: 61386\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    246\n",
      "learning rate  3.4856461398662983e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53857 FP: 10983 FN: 3450 TP: 61390\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    247\n",
      "learning rate  3.311363832872983e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53793 FP: 11047 FN: 3472 TP: 61368\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    248\n",
      "learning rate  3.145795641229334e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53846 FP: 10994 FN: 3527 TP: 61313\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    249\n",
      "learning rate  2.9885058591678673e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53846 FP: 10994 FN: 3498 TP: 61342\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    250\n",
      "learning rate  2.8390805662094737e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53833 FP: 11007 FN: 3506 TP: 61334\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    251\n",
      "learning rate  2.6971265378989998e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53883 FP: 10957 FN: 3502 TP: 61338\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    252\n",
      "learning rate  2.5622702110040496e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53829 FP: 11011 FN: 3543 TP: 61297\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    253\n",
      "learning rate  2.434156700453847e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53853 FP: 10987 FN: 3515 TP: 61325\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    254\n",
      "learning rate  2.3124488654311547e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53769 FP: 11071 FN: 3502 TP: 61338\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    255\n",
      "learning rate  2.196826422159597e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53773 FP: 11067 FN: 3429 TP: 61411\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    256\n",
      "learning rate  2.0869851010516168e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53793 FP: 11047 FN: 3372 TP: 61468\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    257\n",
      "learning rate  1.9826358459990357e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53855 FP: 10985 FN: 3491 TP: 61349\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    258\n",
      "learning rate  1.883504053699084e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53799 FP: 11041 FN: 3455 TP: 61385\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    259\n",
      "learning rate  1.7893288510141295e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53761 FP: 11079 FN: 3449 TP: 61391\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    260\n",
      "learning rate  1.699862408463423e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53784 FP: 11056 FN: 3540 TP: 61300\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    261\n",
      "learning rate  1.6148692880402517e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53878 FP: 10962 FN: 3449 TP: 61391\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    262\n",
      "learning rate  1.534125823638239e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53795 FP: 11045 FN: 3502 TP: 61338\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    263\n",
      "learning rate  1.457419532456327e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53835 FP: 11005 FN: 3535 TP: 61305\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    264\n",
      "learning rate  1.3845485558335106e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53809 FP: 11031 FN: 3469 TP: 61371\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    265\n",
      "learning rate  1.315321128041835e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53793 FP: 11047 FN: 3474 TP: 61366\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    266\n",
      "learning rate  1.2495550716397432e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53841 FP: 10999 FN: 3516 TP: 61324\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    267\n",
      "learning rate  1.187077318057756e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53869 FP: 10971 FN: 3426 TP: 61414\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    268\n",
      "learning rate  1.1277234521548681e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53820 FP: 11020 FN: 3454 TP: 61386\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    269\n",
      "learning rate  1.0713372795471246e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53876 FP: 10964 FN: 3467 TP: 61373\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    270\n",
      "learning rate  1.0177704155697684e-09\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53891 FP: 10949 FN: 3475 TP: 61365\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    271\n",
      "learning rate  9.6688189479128e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53855 FP: 10985 FN: 3508 TP: 61332\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    272\n",
      "learning rate  9.185378000517159e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53839 FP: 11001 FN: 3448 TP: 61392\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    273\n",
      "learning rate  8.7261091004913e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53839 FP: 11001 FN: 3530 TP: 61310\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    274\n",
      "learning rate  8.289803645466735e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53812 FP: 11028 FN: 3497 TP: 61343\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    275\n",
      "learning rate  7.875313463193398e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53796 FP: 11044 FN: 3559 TP: 61281\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 65 FP: 81 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.45 Precision: 0.63 F1: 0.77 MACC 0.72\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    276\n",
      "learning rate  7.481547790033728e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53855 FP: 10985 FN: 3520 TP: 61320\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    277\n",
      "learning rate  7.107470400532042e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53768 FP: 11072 FN: 3467 TP: 61373\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    278\n",
      "learning rate  6.752096880505439e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53763 FP: 11077 FN: 3545 TP: 61295\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    279\n",
      "learning rate  6.414492036480167e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53815 FP: 11025 FN: 3539 TP: 61301\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 65 FP: 81 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.45 Precision: 0.63 F1: 0.77 MACC 0.72\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    280\n",
      "learning rate  6.093767434656158e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53778 FP: 11062 FN: 3520 TP: 61320\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    281\n",
      "learning rate  5.78907906292335e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53873 FP: 10967 FN: 3418 TP: 61422\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    282\n",
      "learning rate  5.499625109777182e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53797 FP: 11043 FN: 3527 TP: 61313\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    283\n",
      "learning rate  5.224643854288323e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53794 FP: 11046 FN: 3499 TP: 61341\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    284\n",
      "learning rate  4.963411661573907e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53846 FP: 10994 FN: 3438 TP: 61402\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    285\n",
      "learning rate  4.715241078495211e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53771 FP: 11069 FN: 3430 TP: 61410\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    286\n",
      "learning rate  4.4794790245704503e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53798 FP: 11042 FN: 3452 TP: 61388\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    287\n",
      "learning rate  4.2555050733419276e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53807 FP: 11033 FN: 3404 TP: 61436\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    288\n",
      "learning rate  4.042729819674831e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53765 FP: 11075 FN: 3397 TP: 61443\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    289\n",
      "learning rate  3.840593328691089e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53809 FP: 11031 FN: 3418 TP: 61422\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    290\n",
      "learning rate  3.648563662256535e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53773 FP: 11067 FN: 3509 TP: 61331\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    291\n",
      "learning rate  3.466135479143708e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53829 FP: 11011 FN: 3507 TP: 61333\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    292\n",
      "learning rate  3.2928287051865226e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53778 FP: 11062 FN: 3492 TP: 61348\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    293\n",
      "learning rate  3.1281872699271963e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53853 FP: 10987 FN: 3507 TP: 61333\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    294\n",
      "learning rate  2.9717779064308364e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53774 FP: 11066 FN: 3399 TP: 61441\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    295\n",
      "learning rate  2.8231890111092944e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53773 FP: 11067 FN: 3524 TP: 61316\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    296\n",
      "learning rate  2.6820295605538294e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53799 FP: 11041 FN: 3482 TP: 61358\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    297\n",
      "learning rate  2.547928082526138e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53802 FP: 11038 FN: 3477 TP: 61363\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    298\n",
      "learning rate  2.4205316783998307e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53817 FP: 11023 FN: 3468 TP: 61372\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    299\n",
      "learning rate  2.299505094479839e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53708 FP: 11132 FN: 3472 TP: 61368\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    300\n",
      "learning rate  2.184529839755847e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53796 FP: 11044 FN: 3469 TP: 61371\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    301\n",
      "learning rate  2.0753033477680545e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53803 FP: 11037 FN: 3532 TP: 61308\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    302\n",
      "learning rate  1.9715381803796517e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53779 FP: 11061 FN: 3477 TP: 61363\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    303\n",
      "learning rate  1.872961271360669e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53838 FP: 11002 FN: 3551 TP: 61289\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    304\n",
      "learning rate  1.7793132077926355e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53812 FP: 11028 FN: 3516 TP: 61324\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    305\n",
      "learning rate  1.6903475474030036e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53778 FP: 11062 FN: 3532 TP: 61308\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    306\n",
      "learning rate  1.6058301700328533e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53828 FP: 11012 FN: 3467 TP: 61373\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    307\n",
      "learning rate  1.5255386615312106e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53767 FP: 11073 FN: 3499 TP: 61341\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    308\n",
      "learning rate  1.44926172845465e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53799 FP: 11041 FN: 3527 TP: 61313\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    309\n",
      "learning rate  1.3767986420319173e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53776 FP: 11064 FN: 3454 TP: 61386\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    310\n",
      "learning rate  1.3079587099303215e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53829 FP: 11011 FN: 3482 TP: 61358\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    311\n",
      "learning rate  1.2425607744338054e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53854 FP: 10986 FN: 3527 TP: 61313\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    312\n",
      "learning rate  1.1804327357121152e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53856 FP: 10984 FN: 3482 TP: 61358\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    313\n",
      "learning rate  1.1214110989265094e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53797 FP: 11043 FN: 3480 TP: 61360\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    314\n",
      "learning rate  1.0653405439801838e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53766 FP: 11074 FN: 3448 TP: 61392\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    315\n",
      "learning rate  1.0120735167811745e-10\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53802 FP: 11038 FN: 3514 TP: 61326\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    316\n",
      "learning rate  9.614698409421157e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53880 FP: 10960 FN: 3461 TP: 61379\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    317\n",
      "learning rate  9.133963488950098e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53838 FP: 11002 FN: 3536 TP: 61304\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    318\n",
      "learning rate  8.677265314502593e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53794 FP: 11046 FN: 3452 TP: 61388\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    319\n",
      "learning rate  8.243402048777463e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53818 FP: 11022 FN: 3534 TP: 61306\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    320\n",
      "learning rate  7.83123194633859e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53819 FP: 11021 FN: 3497 TP: 61343\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    321\n",
      "learning rate  7.43967034902166e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53804 FP: 11036 FN: 3457 TP: 61383\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    322\n",
      "learning rate  7.067686831570576e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53838 FP: 11002 FN: 3401 TP: 61439\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    323\n",
      "learning rate  6.714302489992046e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53776 FP: 11064 FN: 3451 TP: 61389\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    324\n",
      "learning rate  6.378587365492443e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53826 FP: 11014 FN: 3444 TP: 61396\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    325\n",
      "learning rate  6.059657997217821e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53823 FP: 11017 FN: 3360 TP: 61480\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    326\n",
      "learning rate  5.75667509735693e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53861 FP: 10979 FN: 3495 TP: 61345\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    327\n",
      "learning rate  5.468841342489083e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53913 FP: 10927 FN: 3507 TP: 61333\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    328\n",
      "learning rate  5.195399275364629e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53867 FP: 10973 FN: 3423 TP: 61417\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    329\n",
      "learning rate  4.935629311596397e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53771 FP: 11069 FN: 3504 TP: 61336\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    330\n",
      "learning rate  4.688847846016577e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53803 FP: 11037 FN: 3411 TP: 61429\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    331\n",
      "learning rate  4.4544054537157477e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53766 FP: 11074 FN: 3531 TP: 61309\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    332\n",
      "learning rate  4.23168518102996e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53867 FP: 10973 FN: 3492 TP: 61348\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    333\n",
      "learning rate  4.020100921978462e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53825 FP: 11015 FN: 3460 TP: 61380\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    334\n",
      "learning rate  3.8190958758795384e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53750 FP: 11090 FN: 3546 TP: 61294\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    335\n",
      "learning rate  3.628141082085561e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53794 FP: 11046 FN: 3529 TP: 61311\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    336\n",
      "learning rate  3.446734027981283e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53849 FP: 10991 FN: 3511 TP: 61329\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    337\n",
      "learning rate  3.274397326582219e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53830 FP: 11010 FN: 3442 TP: 61398\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    338\n",
      "learning rate  3.110677460253108e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53797 FP: 11043 FN: 3478 TP: 61362\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    339\n",
      "learning rate  2.955143587240452e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53942 FP: 10898 FN: 3463 TP: 61377\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    340\n",
      "learning rate  2.807386407878429e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53802 FP: 11038 FN: 3519 TP: 61321\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    341\n",
      "learning rate  2.6670170874845074e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53833 FP: 11007 FN: 3510 TP: 61330\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    342\n",
      "learning rate  2.533666233110282e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53803 FP: 11037 FN: 3509 TP: 61331\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    343\n",
      "learning rate  2.406982921454768e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53812 FP: 11028 FN: 3486 TP: 61354\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 59 FP: 87 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.40 Precision: 0.61 F1: 0.75 MACC 0.69\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    344\n",
      "learning rate  2.2866337753820293e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53784 FP: 11056 FN: 3447 TP: 61393\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    345\n",
      "learning rate  2.1723020866129277e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53813 FP: 11027 FN: 3428 TP: 61412\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    346\n",
      "learning rate  2.0636869822822813e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53849 FP: 10991 FN: 3334 TP: 61506\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.90 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    347\n",
      "learning rate  1.9605026331681672e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53887 FP: 10953 FN: 3505 TP: 61335\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    348\n",
      "learning rate  1.8624775015097587e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53795 FP: 11045 FN: 3471 TP: 61369\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    349\n",
      "learning rate  1.7693536264342708e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53820 FP: 11020 FN: 3474 TP: 61366\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    350\n",
      "learning rate  1.680885945112557e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53783 FP: 11057 FN: 3489 TP: 61351\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    351\n",
      "learning rate  1.596841647856929e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53800 FP: 11040 FN: 3481 TP: 61359\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    352\n",
      "learning rate  1.5169995654640826e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53854 FP: 10986 FN: 3434 TP: 61406\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    353\n",
      "learning rate  1.4411495871908784e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53796 FP: 11044 FN: 3538 TP: 61302\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    354\n",
      "learning rate  1.3690921078313344e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53782 FP: 11058 FN: 3513 TP: 61327\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    355\n",
      "learning rate  1.3006375024397676e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53777 FP: 11063 FN: 3511 TP: 61329\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    356\n",
      "learning rate  1.2356056273177791e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53841 FP: 10999 FN: 3478 TP: 61362\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    357\n",
      "learning rate  1.1738253459518902e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53822 FP: 11018 FN: 3522 TP: 61318\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    358\n",
      "learning rate  1.1151340786542956e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53844 FP: 10996 FN: 3480 TP: 61360\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    359\n",
      "learning rate  1.0593773747215807e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53793 FP: 11047 FN: 3485 TP: 61355\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    360\n",
      "learning rate  1.0064085059855017e-11\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53847 FP: 10993 FN: 3484 TP: 61356\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 3 TP: 135\n",
      "Sensitivity: 0.98 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    361\n",
      "learning rate  9.560880806862266e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53817 FP: 11023 FN: 3451 TP: 61389\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    362\n",
      "learning rate  9.082836766519153e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53868 FP: 10972 FN: 3521 TP: 61319\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    363\n",
      "learning rate  8.628694928193194e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53871 FP: 10969 FN: 3505 TP: 61335\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    364\n",
      "learning rate  8.197260181783534e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53823 FP: 11017 FN: 3466 TP: 61374\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    365\n",
      "learning rate  7.787397172694357e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53750 FP: 11090 FN: 3427 TP: 61413\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    366\n",
      "learning rate  7.398027314059639e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53790 FP: 11050 FN: 3468 TP: 61372\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    367\n",
      "learning rate  7.028125948356657e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53846 FP: 10994 FN: 3524 TP: 61316\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    368\n",
      "learning rate  6.676719650938824e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53843 FP: 10997 FN: 3434 TP: 61406\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    369\n",
      "learning rate  6.3428836683918825e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53840 FP: 11000 FN: 3474 TP: 61366\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    370\n",
      "learning rate  6.025739484972288e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53810 FP: 11030 FN: 3471 TP: 61369\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    371\n",
      "learning rate  5.724452510723673e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53784 FP: 11056 FN: 3442 TP: 61398\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    372\n",
      "learning rate  5.43822988518749e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53807 FP: 11033 FN: 3494 TP: 61346\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    373\n",
      "learning rate  5.166318390928115e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53802 FP: 11038 FN: 3489 TP: 61351\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    374\n",
      "learning rate  4.908002471381709e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53826 FP: 11014 FN: 3483 TP: 61357\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    375\n",
      "learning rate  4.662602347812623e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53753 FP: 11087 FN: 3494 TP: 61346\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    376\n",
      "learning rate  4.429472230421991e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53817 FP: 11023 FN: 3503 TP: 61337\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    377\n",
      "learning rate  4.207998618900892e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53777 FP: 11063 FN: 3387 TP: 61453\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    378\n",
      "learning rate  3.997598687955847e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53798 FP: 11042 FN: 3495 TP: 61345\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    379\n",
      "learning rate  3.797718753558054e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53785 FP: 11055 FN: 3476 TP: 61364\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    380\n",
      "learning rate  3.607832815880151e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53816 FP: 11024 FN: 3475 TP: 61365\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    381\n",
      "learning rate  3.4274411750861436e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53807 FP: 11033 FN: 3497 TP: 61343\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    382\n",
      "learning rate  3.2560691163318364e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53862 FP: 10978 FN: 3477 TP: 61363\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    383\n",
      "learning rate  3.0932656605152445e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53803 FP: 11037 FN: 3500 TP: 61340\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    384\n",
      "learning rate  2.9386023774894822e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53795 FP: 11045 FN: 3449 TP: 61391\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    385\n",
      "learning rate  2.791672258615008e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53753 FP: 11087 FN: 3578 TP: 61262\n",
      "Sensitivity: 0.94 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 64 FP: 82 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.44 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    386\n",
      "learning rate  2.6520886456842577e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53841 FP: 10999 FN: 3462 TP: 61378\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    387\n",
      "learning rate  2.519484213400045e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53823 FP: 11017 FN: 3476 TP: 61364\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    388\n",
      "learning rate  2.3935100027300425e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53822 FP: 11018 FN: 3482 TP: 61358\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    389\n",
      "learning rate  2.2738345025935404e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53754 FP: 11086 FN: 3464 TP: 61376\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    390\n",
      "learning rate  2.1601427774638634e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53807 FP: 11033 FN: 3439 TP: 61401\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    391\n",
      "learning rate  2.05213563859067e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53767 FP: 11073 FN: 3539 TP: 61301\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    392\n",
      "learning rate  1.9495288566611362e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53784 FP: 11056 FN: 3461 TP: 61379\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    393\n",
      "learning rate  1.8520524138280794e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53835 FP: 11005 FN: 3428 TP: 61412\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 60 FP: 86 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.41 Precision: 0.61 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    394\n",
      "learning rate  1.7594497931366753e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53860 FP: 10980 FN: 3483 TP: 61357\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    395\n",
      "learning rate  1.6714773034798415e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53745 FP: 11095 FN: 3442 TP: 61398\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    396\n",
      "learning rate  1.5879034383058493e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53891 FP: 10949 FN: 3549 TP: 61291\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 61 FP: 85 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.70\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    397\n",
      "learning rate  1.5085082663905568e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53832 FP: 11008 FN: 3460 TP: 61380\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    398\n",
      "learning rate  1.4330828530710288e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53741 FP: 11099 FN: 3511 TP: 61329\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n",
      "EPOCH    399\n",
      "learning rate  1.3614287104174773e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53792 FP: 11048 FN: 3394 TP: 61446\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 62 FP: 84 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.42 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.73\n",
      "EPOCH    400\n",
      "learning rate  1.2933572748966033e-12\n",
      "Training loss 0.42 Training Acc  0.89 TN: 53836 FP: 11004 FN: 3488 TP: 61352\n",
      "Sensitivity: 0.95 Specificity: 0.83 Precision: 0.85 F1: 0.89 MACC 0.89\n",
      "TN: 63 FP: 83 FN: 2 TP: 136\n",
      "Sensitivity: 0.99 Specificity: 0.43 Precision: 0.62 F1: 0.76 MACC 0.71\n",
      "Validation loss 0.59 Validation Acc  0.72\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "mfcc_gen.cuda()\n",
    "mfcc_gen.eval()\n",
    "epochs = 400\n",
    "# torch.save(model,os.path.join(path,'model.pt'))\n",
    "logger.on_train_begin()\n",
    "print(\"steps \", flow_source.steps_per_epoch)\n",
    "\n",
    "# with open(path+fold+'/model1.json', 'w') as outfile:\n",
    "#     outfile.write(str(mod))\n",
    "for e in range(epochs):\n",
    "    print(\"EPOCH   \",e+1)\n",
    "    print(\"learning rate \",optimizer.param_groups[0]['lr'])\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    acc = 0\n",
    "    y_pred = None\n",
    "    y_true = None\n",
    "    N = 0\n",
    "    for i in range(flow_source.steps_per_epoch+1):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x,y = flow_source.next()\n",
    "        if(str(class_criterion) in ['MSELoss()','BCELoss()']):\n",
    "            y = to_categorical(y,2)\n",
    "            y = y.astype(np.float32)\n",
    "        x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
    "        x = x.type(torch.FloatTensor).cuda()\n",
    "        holdx = x\n",
    "        x = mfcc_gen(x)\n",
    "        if(len(x)>1):\n",
    "            x = x[0]\n",
    "        hold = x\n",
    "        x = x.transpose(2,1)\n",
    "        x = x.unsqueeze(1)\n",
    "        # print(x.shape)\n",
    "        x,y = Variable(x),Variable(y)\n",
    "        \n",
    "#         y = y.cuda()\n",
    "        y = y.long().cuda()        \n",
    "        cls = model(x)\n",
    "        # class_loss = class_criterion(cls,torch.argmax(y,axis=1))        \n",
    "        loss = class_criterion(cls,y)\n",
    "        epoch_loss = epoch_loss + loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         if(str(class_criterion)in ['MSELoss()','BCELoss()']):\n",
    "#             y = torch.argmax(y,axis=1)\n",
    "        if(y_pred is None):\n",
    "            y_pred = torch.argmax(cls,axis=1)\n",
    "            y_true = y\n",
    "        else:\n",
    "            y_pred = torch.cat((y_pred,torch.argmax(cls,axis=1)))\n",
    "            y_true = torch.cat((y_true,y))\n",
    "    \n",
    "        acc = acc + torch.sum(y==torch.argmax(cls,axis=1))\n",
    "        N = N+len(y)\n",
    "    print(\"Training loss\", \"%.2f\"%(epoch_loss.item()/flow_source.steps_per_epoch),end=' ')\n",
    "    print(\"Training Acc \", \"%.2f\"%(acc.item()/N),end=' ')\n",
    "    trainLog(y_true,y_pred)\n",
    "    logger.logs['train_loss'] = (epoch_loss.item()/flow_source.steps_per_epoch)\n",
    "    logger.logs['train_acc'] = (acc.item()/N)\n",
    "    # Validate \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    acc = 0\n",
    "    N = 0\n",
    "    y_pred = None\n",
    "    y_true = None\n",
    "    with torch.no_grad():\n",
    "        start_idx = 0\n",
    "        for i,s in enumerate(val_parts):\n",
    "            if(s==0):\n",
    "                continue\n",
    "            x,y = x_val[start_idx:start_idx+s],y_val[start_idx:start_idx+s]\n",
    "            start_idx = start_idx+s\n",
    "            \n",
    "            if(str(class_criterion) in ['MSELoss()','BCELoss()']):\n",
    "                y = to_categorical(y,2)\n",
    "                y = y.astype(np.float32)\n",
    "            \n",
    "            x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
    "            x = x.type(torch.FloatTensor).cuda()\n",
    "            holdvalx = x\n",
    "            x = mfcc_gen(x)\n",
    "            if(len(x)>1):\n",
    "                x = x[0]\n",
    "            holdval = x\n",
    "            x = x.transpose(2,1)\n",
    "            x = x.unsqueeze(1)\n",
    "            x,y = Variable(x),Variable(y)\n",
    "            #x = x.reshape(x.shape[0],1,x.shape[1],x.shape[2])\n",
    "            \n",
    "#             y = y.cuda()\n",
    "            y = y.long().cuda()\n",
    "            cls= model(x)\n",
    "            # val_class_loss = class_criterion(cls,torch.argmax(y,axis=1))\n",
    "            val_class_loss = class_criterion(cls,y)\n",
    "            \n",
    "            if(str(class_criterion) in ['MSELoss()','BCELoss()']):\n",
    "                y = torch.argmax(y,axis=1)\n",
    "            \n",
    "            acc = acc + torch.sum(y==torch.argmax(cls,axis=1))\n",
    "            N = N+len(y)\n",
    "            epoch_loss = epoch_loss + val_class_loss\n",
    "            if(y_pred is None):\n",
    "                y_pred = torch.argmax(cls,axis=1)\n",
    "                y_true = y\n",
    "            else:\n",
    "                y_pred = torch.cat((y_pred,torch.argmax(cls,axis=1)))\n",
    "                y_true = torch.cat((y_true,y))\n",
    "        Macc,sensitivity,specificity,precision,F1 = log_macc(y_pred,y_true,val_parts)\n",
    "        \n",
    "        print(\"Validation loss\", \"%.2f\"%(epoch_loss.item()/len(val_parts)),end=' ')\n",
    "        print(\"Validation Acc \", \"%.2f\"%(acc.item()/N))\n",
    "        logger.logs['val_loss'] = (epoch_loss.item()/len(val_parts))\n",
    "        logger.logs['val_acc'] = (acc.item()/N)\n",
    "        acc = (acc.item()/N)\n",
    "        logger.logs['val_macc'] = Macc\n",
    "        logger.logs['precision'] = precision\n",
    "        logger.logs['sensitivity'] = sensitivity\n",
    "        logger.logs['specificity'] = specificity\n",
    "        logger.logs['F1'] = F1\n",
    "    lr_schedule.step()\n",
    "    torch.save(model.state_dict(),checkpoint_name.format(epoch=e,val_acc=acc,macc=Macc))\n",
    "    logger.on_epoch_end(e)\n",
    "    flow_source.reset()\n",
    "logger.on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss_print = (epoch_loss.item()) if (type(epoch_loss)==torch.Tensor) else epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(int, int)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(epoch_loss_print),type(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "torch.argmax(model(torch.ones(x.shape).cuda()),1),torch.argmax(model(torch.zeros(x.shape).cuda()),1),y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(model.parameters())[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc,gm,gmnorm = mfcc_gen(holdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(to_numpy(holdx[idx]).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(to_numpy(holdval[0]),origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(model.parameters())[0]\n",
    "a1 = list(model.parameters())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape,a1,b.shape, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = list(model.parameters())[0]\n",
    "b1 = list(model.parameters())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(model.parameters())[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model(hold.transpose(2,1).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(to_numpy(holdval[0]),origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(to_numpy(hold[0]),origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.equal(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc(x):\n",
    "    x = x.transpose(2,1).unsqueeze(1)\n",
    "    \n",
    "#     x = model.extractor.conv0(x)\n",
    "#     x = model.extractor.bn0(x)\n",
    "#     x = model.extractor.conv1(x)\n",
    "    \n",
    "    x = F.relu(model.extractor.bn0(model.extractor.conv0(x)))\n",
    "    #Res block 1\n",
    "    x1 = model.extractor.drop(F.relu(model.extractor.bn1(model.extractor.conv1(x))))\n",
    "    x1 = F.relu(F.max_pool2d(model.extractor.drop(model.extractor.bn11(model.extractor.conv11(x1))), 2))\n",
    "    x = torch.cat((x,torch.zeros_like(x)), axis=1)\n",
    "    x = F.max_pool2d(x,2)\n",
    "    x = x+x1\n",
    "\n",
    "\n",
    "    #Res block 2\n",
    "    x1 = model.extractor.drop(F.relu(model.extractor.bn2(model.extractor.conv2(x))))\n",
    "    x1 = F.relu(model.extractor.drop(model.extractor.bn21(model.extractor.conv21(x1))))\n",
    "    x = torch.cat((x,torch.zeros_like(x)), axis=1)\n",
    "    x = F.max_pool2d(x,2)\n",
    "    x = x+x1\n",
    "    #Res block 3\n",
    "    x1 = model.extractor.drop(F.relu(model.extractor.bn3(model.extractor.conv3(x))))\n",
    "    x1 = F.relu(model.extractor.drop(model.extractor.bn31(model.extractor.conv31(x1))))\n",
    "    x = torch.cat((x,torch.zeros_like(x)), axis=1)\n",
    "    x = F.max_pool2d(x,2)\n",
    "    x = x+x1\n",
    "    #Res block 4\n",
    "    x1 = model.extractor.drop(F.relu(model.extractor.bn4(model.extractor.conv4(x))))\n",
    "    x1 = F.relu(model.extractor.drop(model.extractor.bn41(model.extractor.conv41(x1))))\n",
    "    x = torch.cat((x,torch.zeros_like(x)), axis=1)\n",
    "    x = F.max_pool2d(x,2)\n",
    "    x = x+x1\n",
    "\n",
    "    #last conv\n",
    "    # x = model.extractor.drop(F.relu(model.extractor.bn5(model.extractor.conv5(x))))\n",
    "    x = F.max_pool2d(x,((2,1) if(model.extractor.form) else (1,2)))  ### change withinput\n",
    "    x = x.view(x.size(0),-1)\n",
    "    \n",
    "    x = model.classifier.relu(model.classifier.fc1(x))\n",
    "    x = model.classifier.fc2(F.dropout(x))\n",
    "    x = model.classifier.soft(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdval1 = holdval[:1]\n",
    "holdval1[:,:,150:] = 0\n",
    "torch.sum(holdval1[:,:,150:])\n",
    "hold1 = hold[:1]\n",
    "hold1[:,:,100:] = 0\n",
    "torch.sum(hold1[:,:,100:])\n",
    "holdval1.shape, hold1.shape\n",
    "wow = torch.rand((1,64,240)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = calc(hold[:1])\n",
    "b = calc(holdval[:1])\n",
    "print(a.shape, b.shape)\n",
    "torch.equal(a,b),a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotf(a.transpose(1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotf(b.transpose(1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotf(holdx[:1].reshape(2500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotf(holdvalx[:1].reshape(2500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOad model \n",
    "path = \"../../Heartnet_Results/logs/gammatone_torch_layer/\"\n",
    "fold = 'a_bcdEf learnable_2020-03-04 17.19.23.260152'\n",
    "weight = sorted(os.listdir(os.path.join(path,fold,'weights')))[-1]\n",
    "model.load_state_dict(torch.load(os.path.join(path,fold,'weights',weight)))\n",
    "start_epoch = int(weight.split('-',maxsplit=1)[0].split('.')[-1])+1\n",
    "logger = CSVLogger(os.path.join(path,fold)+'/'+'training.csv')\n",
    "checkpoint_name = os.path.join(path,fold,'weights') + \"/\" + 'weights.{epoch:04d}-acc_{val_acc:.4f}-macc_{macc:.4f}.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+fold+'/model1.json', 'w') as outfile:\n",
    "    outfile.write(str(mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.on_off(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhealthra2/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type WHOLE_MODEL. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model,os.path.join(path,'model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([0, 1, 2500]), torch.Size([0]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "epochs = 400\n",
    "print(\"steps \", flow_source.steps_per_epoch)\n",
    "for e in range(epochs):\n",
    "    print(\"EPOCH   \",e+1)\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i in range(flow_source.steps_per_epoch+1):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x,[y,yd] = flow_source.next()\n",
    "        x,y,yd = torch.from_numpy(x),torch.from_numpy(y),torch.from_numpy(yd)\n",
    "        x,y,yd = Variable(x),Variable(y),Variable(yd)\n",
    "        x = x.type(torch.FloatTensor).cuda()\n",
    "        #x = x.reshape(x.shape[0],1,x.shape[1],x.shape[2])\n",
    "        y = y.long().cuda()\n",
    "        yd = yd.long().cuda()\n",
    "        cls, dom = model(x)\n",
    "        class_loss = class_criterion(cls,torch.argmax(y,axis=1))\n",
    "        domain_loss_source = domain_criterion(dom,torch.argmax(yd,axis=1))\n",
    "        \n",
    "        if(args.dann):\n",
    "            x,[y,yd] = flow_target.next()\n",
    "            x,y,yd = torch.from_numpy(x),torch.from_numpy(y),torch.from_numpy(yd)\n",
    "            x,y,yd = Variable(x),Variable(y),Variable(yd)\n",
    "            x = x.type(torch.FloatTensor).cuda()\n",
    "            x = x.reshape(x.shape[0],1,x.shape[1],x.shape[2])\n",
    "            y = y.long().cuda()\n",
    "            yd = yd.long().cuda()\n",
    "            cls, dom = model(x)\n",
    "            domain_loss_target = domain_criterion(dom,torch.argmax(yd,axis=1))\n",
    "            loss = class_loss + domain_loss_source+domain_loss_target\n",
    "        else:\n",
    "            loss = class_loss\n",
    "        epoch_loss = epoch_loss + loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Training loss fo\", \"%.2f\"%(epoch_loss.item()/flow_source.steps_per_epoch),end=' ')\n",
    "    # Validate \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x,y,yd = torch.from_numpy(x_val),torch.from_numpy(y_val),torch.from_numpy(val_domain)\n",
    "        x,y,yd = Variable(x),Variable(y),Variable(yd)\n",
    "        x = x.type(torch.FloatTensor).cuda()\n",
    "        #x = x.reshape(x.shape[0],1,x.shape[1],x.shape[2])\n",
    "        y = y.long().cuda()\n",
    "        yd = yd.long().cuda()\n",
    "        cls, dom = model(x)\n",
    "        val_class_loss = class_criterion(cls,torch.argmax(y,axis=1))\n",
    "        val_domain_loss = domain_criterion(dom,torch.argmax(yd,axis=1))\n",
    "        print(\"val_Class_loss  \",\"%.2f\"%val_class_loss.item())\n",
    "        print(\"val_dom_loss    \", \"%.2f\"%val_domain_loss.item())\n",
    "        log_macc(cls,dom,y,val_parts)\n",
    "    flow_source.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_gen = MFCC_Gen(fs=1000,filters=64,momentum=)\n",
    "mfcc_gen.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfcc_gen.eval()\n",
    "out = mfcc_gen(torch.from_numpy(x_train[0]).unsqueeze(0).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "outnp1 = (out.squeeze(0).cpu().detach().numpy())\n",
    "plt.imshow(outnp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(outnp1),np.max(outnp1),np.std(outnp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_gen2 = MFCC_Gen2(fs=1000,filters=64)\n",
    "mfcc_gen2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_gen2.train()\n",
    "mfcc_gen2.eval()\n",
    "out = mfcc_gen2(torch.from_numpy(x_train[0]).unsqueeze(0).cuda())\n",
    "plt.figure(figsize=(20,20))\n",
    "outnp = (out.squeeze(0).squeeze(0).cpu().detach().numpy())\n",
    "plt.imshow(outnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(outnp),np.max(outnp),np.std(outnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HeartCepTorch import Conv_Gammatone,MFCC_Gen\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.utils import _single\n",
    "class MFCC_Gen2(nn.Module):\n",
    "    def __init__(self,kernel_size = 81,filters = 26,fs=1000,winlen=0.025,winstep=0.01,dimension=1,momentum=0.001):\n",
    "        super(MFCC_Gen2,self).__init__()\n",
    "        self.gamma = Conv_Gammatone(in_channels=1,out_channels=filters ,kernel_size=kernel_size,fsHz=fs)\n",
    "#         self.gamma = nn.Conv1d(in_channels=1,out_channels=filters ,kernel_size=81,stride=1)\n",
    "        self.gammanorm = nn.BatchNorm1d(filters,momentum=momentum)\n",
    "        self.mfcc = nn.Conv1d(filters,filters,int(winlen*fs),stride=int(winstep*fs),padding=0,bias=False)\n",
    "        self.normmfcc = nn.BatchNorm1d(filters,momentum=momentum)\n",
    "        self.normmfcc2D = nn.BatchNorm2d(1,momentum=momentum)\n",
    "        with torch.no_grad():\n",
    "            self.mfcc.weight = Parameter(torch.stack([torch.eye(filters) for i in range(int(winlen*fs))],dim=2))\n",
    "        for x in self.mfcc.named_parameters():\n",
    "            x[1].requires_grad = False\n",
    "        for x in self.gamma.named_parameters():\n",
    "            x[1].requires_grad = False\n",
    "    def forward(self,x):\n",
    "        x = self.gamma(x)\n",
    "#         x = self.gammanorm(x)\n",
    "        x = torch.pow(torch.abs(x),2)\n",
    "        x = self.mfcc(x)\n",
    "        x = torch.log(x+0.0000000000000001)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.normmfcc2D(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mfcc_models import Smallnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Smallnet(2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary(net.cuda(),(1,2500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filt = (8, 4)\n",
    "num_dense = 20\n",
    "lr = 0.01\n",
    "bn_momentum = 0.99\n",
    "eps = 1.1e-5\n",
    "bias = False\n",
    "l2_reg = 0.04864911065093751\n",
    "l2_reg_dense = 0.\n",
    "kernel_size = 5\n",
    "maxnorm = 10000.\n",
    "dropout_rate = 0.5\n",
    "dropout_rate_dense = 0.\n",
    "padding = 'valid'\n",
    "activation_function = 'relu'\n",
    "subsam = 2\n",
    "FIR_train= True\n",
    "trainable = True\n",
    "hp_lambda = np.float32(0)\n",
    "lr_decay =0.0001132885\n",
    "random_seed = 1\n",
    "num_class =2\n",
    "num_class_domain = num_class_domain\n",
    "tipe= 1\n",
    "decision = 'majority' \n",
    "channels = '0101'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadpath='../../Adversarial Heart Sound Results/models/bcdefghi_a 2019-10-16 12:45:03.236037/weights.0036-0.5964.hdf5'\n",
    "loadatttrain = '../../Adversarial Heart Sound Results/models/attention/bcdefghi_a 2019-10-20 12:37:52.424551/weights.0010-0.5444.hdf5'\n",
    "loadpath = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = heartnet(loadpath,activation_function, bn_momentum, bias, dropout_rate, dropout_rate_dense,\n",
    "                             eps, kernel_size, l2_reg, l2_reg_dense, lr, lr_decay, maxnorm,\n",
    "                             padding, random_seed, subsam, num_filt, num_dense, FIR_train, trainable, tipe,\n",
    "                             num_class=num_class,num_class_domain=9,hp_lambda=hp_lambda,segments=channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"conv1d_linearphase_type_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(flow,#steps_per_epoch=len(x_train) // batch_size,\n",
    "                    steps_per_epoch=flow.steps_per_epoch,\n",
    "                    # max_queue_size=20,\n",
    "                    use_multiprocessing=False,\n",
    "                    epochs=200,\n",
    "                    verbose=1,\n",
    "                    shuffle=True,\n",
    "                    callbacks=[log_macc(val_parts, decision=decision,verbose=1,val_files=val_files,wav_files=val_wav_files,checkpoint_name = 'ansari')],\n",
    "                    validation_data=(x_val, [y_val,val_domain]),\n",
    "                    initial_epoch=0,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = hmmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = (hm.replace('</pre></div></div><div class=\"output_area\"><div class=\"run_this_cell\"></div><div class=\"prompt\"></div><div class=\"output_subarea output_text output_stream output_stdout\"><pre>',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = {'Training loss':'train_loss','Training Acc':'train_acc','Sensitivity:':'sensitivity',\n",
    "        'Specificity:':'specificity','Precision:':'precision','F1:':'val_F1','MACC':'val_macc',\n",
    "        'Validation loss':'val_loss','Validation Acc':'val_acc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm.split(\"EPOCH\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in keys:\n",
    "    print(k,hm.split('EPOCH')[3].split(k)[-1].split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,x in enumerate(hm.split(\"EPOCH\")):\n",
    "#     print(i,x)\n",
    "    if(len(x)<10):\n",
    "        continue\n",
    "    for k in keys:\n",
    "        print(keys[k],x.split(k)[1].split()[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = logger.filename[:-4]+'2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file, 'w') as f:\n",
    "#     print('epoch',end=',')\n",
    "    f.write('epoch,')\n",
    "    for k in keys:\n",
    "#         print(keys[k],end=',')\n",
    "        f.write(keys[k]+',')\n",
    "    f.write('\\n')\n",
    "#     print()\n",
    "    for i,x in enumerate(hm.split(\"EPOCH\")):\n",
    "        print(i,x)\n",
    "        if(len(x)<10):\n",
    "            continue\n",
    "        f.write(x.split('\\n')[0].split()[0]+',')\n",
    "#         print(x.split('\\n')[0].split()[0],end=',')\n",
    "        for k in keys:\n",
    "#             print(x.split(k)[1].split()[0],end=',')\n",
    "            \n",
    "            if(k[:3] in ['Tra','Val']):\n",
    "                f.write(x.split(k)[1].split()[0]+',')\n",
    "            else:\n",
    "                print(k,x.split(k)[-1].split()[0],end=',')\n",
    "                f.write(x.split(k)[-1].split()[0]+',')\n",
    "#             f.write(keys[k]+' '+x.split(k)[1].split()[0])\n",
    "        f.write('\\n')\n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domainClass = [(cls,dfc) for cls in range(2) for dfc in train_domain]\n",
    "meta_label = [hey.index((cl,df)) for (cl,df) in zip(y,yd)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set('sdff'+'cdc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[e%50>39 for e in range(60)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.xticks([10*x for x in range(21)])\n",
    "plt.plot([e%50>39 for e in range(200)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np , math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_lambda = 0.01\n",
    "epochs = 400\n",
    "def sig_moid(epoch):\n",
    "    minEpoch = 150\n",
    "    if(False):#args.fixed):\n",
    "        return hp_lambda\n",
    "    if hp_lambda == 0:\n",
    "        return hp_lambda\n",
    "    #     if epoch<minEpoch:\n",
    "    #         return np.float32(0.0)\n",
    "    maxx =  8\n",
    "    p = 3*(epoch-(epochs/2)) / (epochs/2)\n",
    "    lam = 8/(1+math.e**(-p))\n",
    "    lam = lam#*(epoch%50<10)\n",
    "    # hp_lambda = hp_lambda * (params['hp_decay_const'] ** global_epoch_counter)\n",
    "    return lam\n",
    "def f_hp_anneal(epoch):\n",
    "    minEpoch = 150\n",
    "    if(False):#args.fixed):\n",
    "        return hp_lambda\n",
    "    if hp_lambda == 0:\n",
    "        return hp_lambda\n",
    "    #     if epoch<minEpoch:\n",
    "    #         return np.float32(0.0)\n",
    "    gamma =  4\n",
    "    p = (epoch) / (epochs)\n",
    "    lam =  (8 / (2 + 3*(math.e ** (- gamma * p)))) - 1+hp_lambda  # 3 porjonto jaabe\n",
    "    lam = lam#*(epoch%50<10)\n",
    "    # hp_lambda = hp_lambda * (params['hp_decay_const'] ** global_epoch_counter)\n",
    "    return lam\n",
    "def sin_up(epoch):\n",
    "    minEpoch = 150\n",
    "    if(False):#args.fixed):\n",
    "        return hp_lambda\n",
    "    if hp_lambda == 0:\n",
    "        return hp_lambda\n",
    "    #     if epoch<minEpoch:\n",
    "    #         return np.float32(0.0)\n",
    "    maxx =  4\n",
    "    p = (epoch) / (epochs)\n",
    "    lam = (1+math.sin(p*(math.pi)+1.5*math.pi))*maxx+hp_lambda\n",
    "    lam = lam#*(epoch%50<10)\n",
    "    # hp_lambda = hp_lambda * (params['hp_decay_const'] ** global_epoch_counter)\n",
    "    return lam\n",
    "def f_hp_decay(epoch):\n",
    "    minEpoch = 150\n",
    "    if hp_lambda == 0:\n",
    "        return hp_lambda\n",
    "    if epoch<minEpoch:\n",
    "        return np.float32(0.0)\n",
    "    gamma =  4\n",
    "\n",
    "    p = (epoch-minEpoch) / (epochs)\n",
    "    lam =  (10 /2* (1 + 1*(math.e ** (- gamma * p)))) - 1+hp_lambda  # 3 porjonto jaabe\n",
    "    # hp_lambda = hp_lambda * (params['hp_decay_const'] ** global_epoch_counter)\n",
    "    return np.float32(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([sig_moid(e) for e in range(epochs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([sin_up(e) for e in range(epochs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f_hp_anneal(e) for e in range(epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):         \n",
    "    lr0 = .00128437\n",
    "    #print(\"learning rate , lr 0 \", lr, lr0)\n",
    "    a = 1\n",
    "    b = 1\n",
    "    p = epoch/epochs\n",
    "    lrate = lr0/math.pow((1+a*p),b)\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([step_decay(e) for e in range(epochs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import, division\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.engine.topology import InputSpec\n",
    "import tensorflow as tf\n",
    "from keras.utils import conv_utils\n",
    "from keras.layers import activations, initializers, regularizers, constraints\n",
    "import numpy as np\n",
    "from scipy.fftpack import dct\n",
    "from keras.backend.common import normalize_data_format\n",
    "from keras.layers.merge import Multiply\n",
    "class Attt(Layer):\n",
    "    '''Custom Layer for ResNet used for BatchNormalization.\n",
    "\n",
    "    Linear learnable weight vector , does dot multiplication on a vector\n",
    "    # Arguments\n",
    "        axis: integer, axis along which to normalize in mode 0. For instance,\n",
    "            if your input tensor has shape (samples, channels, rows, cols),\n",
    "            set axis to 1 to normalize per feature map (channels axis).'''\n",
    "\n",
    "    def __init__(self, weights=None, axis=-1,init='he_normal',**kwargs):\n",
    "        self.axis = axis\n",
    "        self.init = initializers.get(init)\n",
    "        self.kernel = weights\n",
    "        super(Attt, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        if len(input_shape)>2:\n",
    "            raise ValueError(\"Input to attention layer hasn't been flattened\")\n",
    "        self.input_dim = input_shape[-1]            \n",
    "        self.kernel = self.add_weight(shape=(self.input_dim,),\n",
    "                                      initializer=initializers.Ones(),\n",
    "                                      name='kernel',\n",
    "                                      constraint=constraints.NonNeg()\n",
    "                                      #constraint=constraints.min_max_norm(min_value=0.0, max_value=1.0)\n",
    "                                      #constraint=constraints.UnitNorm(axis=self.axis)\n",
    "                                     )\n",
    "        self.input_spec = InputSpec(min_ndim=2, axes={-1: self.input_dim})            \n",
    "        self.built = True\n",
    "    def call(self, inputs):\n",
    "        output = tf.multiply(inputs,self.kernel)\n",
    "        return output\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'axis': self.axis,\n",
    "            'kernel_initializer': initializers.serialize(self.init)\n",
    "        }\n",
    "        base_config = super(Attt, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv1D, MaxPooling1D, Dense, Dropout, Flatten, Activation, AveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print('outer',i)\n",
    "    for j in range(3):\n",
    "        print('        innter',j)\n",
    "        if(j==1):\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_size(size):\n",
    "\t\"\"\"Pretty prints a torch.Size object\"\"\"\n",
    "\tassert(isinstance(size, torch.Size))\n",
    "\treturn \"  \".join(map(str, size))\n",
    "\n",
    "def dump_tensors(gpu_only=True):\n",
    "\t\"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\n",
    "\timport gc\n",
    "\ttotal_size = 0\n",
    "\tfor obj in gc.get_objects():\n",
    "\t\ttry:\n",
    "\t\t\tif torch.is_tensor(obj):\n",
    "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "\t\t\t\t\tprint(\"%s:%s%s %s\" % (type(obj).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t  \" GPU\" if obj.is_cuda else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t  \" pinned\" if obj.is_pinned else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t  pretty_size(obj.size())))\n",
    "\t\t\t\t\ttotal_size += obj.numel()\n",
    "\t\t\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "\t\t\t\t\tprint(\"%s  %s:%s%s%s%s %s\" % (type(obj).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   type(obj.data).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" GPU\" if obj.is_cuda else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" pinned\" if obj.data.is_pinned else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" grad\" if obj.requires_grad else \"\", \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" volatile\" if obj.volatile else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   pretty_size(obj.data.size())))\n",
    "\t\t\t\t\ttotal_size += obj.data.numel()\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tpass        \n",
    "\tprint(\"Total size:\", total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: GPU pinned 256  256  3  3\n",
      "Parameter: GPU pinned 64  1  81\n",
      "Parameter: GPU pinned 64\n",
      "Parameter: GPU pinned 64\n",
      "Parameter: GPU pinned 64\n",
      "Parameter: GPU pinned 64  64  25\n",
      "Parameter: GPU pinned 64\n",
      "Parameter: GPU pinned 64\n",
      "Parameter: GPU pinned 1\n",
      "Parameter: GPU pinned 1\n",
      "Parameter: GPU pinned 16  1  3  2\n",
      "Parameter: GPU pinned 16\n",
      "Parameter: GPU pinned 16\n",
      "Parameter: GPU pinned 16\n",
      "Parameter: GPU pinned 32  16  3  3\n",
      "Parameter: GPU pinned 32\n",
      "Parameter: GPU pinned 32\n",
      "Parameter: GPU pinned 32\n",
      "Parameter: GPU pinned 32  32  3  3\n",
      "Parameter: GPU pinned 32\n",
      "Parameter: GPU pinned 32\n",
      "Parameter: GPU pinned 32\n",
      "Parameter: GPU pinned 64  32  3  3\n",
      "Parameter: GPU pinned 64\n",
      "Parameter: GPU pinned 64\n",
      "Parameter: GPU pinned 64\n",
      "Parameter: GPU pinned 64  64  3  3\n",
      "Parameter: GPU pinned 64\n",
      "Parameter: GPU pinned 64\n",
      "Parameter: GPU pinned 64\n",
      "Parameter: GPU pinned 128  64  3  3\n",
      "Parameter: GPU pinned 128\n",
      "Parameter: GPU pinned 128\n",
      "Parameter: GPU pinned 128\n",
      "Parameter: GPU pinned 128  128  3  3\n",
      "Parameter: GPU pinned 128\n",
      "Parameter: GPU pinned 128\n",
      "Parameter: GPU pinned 128\n",
      "Parameter: GPU pinned 256  128  3  3\n",
      "Parameter: GPU pinned 256\n",
      "Parameter: GPU pinned 256\n",
      "Parameter: GPU pinned 256\n",
      "Parameter: GPU pinned 256\n",
      "Parameter: GPU pinned 2\n",
      "Parameter: GPU pinned 2  100\n",
      "Parameter: GPU pinned 100\n",
      "Parameter: GPU pinned 100  7168\n",
      "Parameter: GPU pinned 256\n",
      "Parameter: GPU pinned 256\n",
      "Tensor: GPU pinned 64\n",
      "Tensor: GPU pinned 64\n",
      "Tensor: GPU pinned 64\n",
      "Tensor: GPU pinned 64\n",
      "Tensor: GPU pinned 1\n",
      "Tensor: GPU pinned 1\n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned 16\n",
      "Tensor: GPU pinned 16\n",
      "Tensor: GPU pinned 32\n",
      "Tensor: GPU pinned 32\n",
      "Tensor: GPU pinned 32\n",
      "Tensor: GPU pinned 32\n",
      "Tensor: GPU pinned 64\n",
      "Tensor: GPU pinned 64\n",
      "Tensor: GPU pinned 64\n",
      "Tensor: GPU pinned 64\n",
      "Tensor: GPU pinned 128\n",
      "Tensor: GPU pinned 128\n",
      "Tensor: GPU pinned 128\n",
      "Tensor: GPU pinned 128\n",
      "Tensor: GPU pinned 256\n",
      "Tensor: GPU pinned 256\n",
      "Tensor: GPU pinned 256\n",
      "Tensor: GPU pinned 256\n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned 12  2\n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned 12  1  2500\n",
      "Tensor: GPU pinned 12\n",
      "Tensor: GPU pinned 12  1  240  64\n",
      "Tensor: GPU pinned 12  16  244  65\n",
      "Tensor: GPU pinned 12  32  242  63\n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned 2  1  240  64\n",
      "Tensor: GPU pinned 2  16  244  65\n",
      "Tensor: GPU pinned 2  32  242  63\n",
      "Tensor: GPU pinned 2  1  2500\n",
      "Total size: 12638261\n"
     ]
    }
   ],
   "source": [
    "dump_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
