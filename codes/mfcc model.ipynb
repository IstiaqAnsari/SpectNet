{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradReverse(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Extension of grad reverse layer\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, constant):\n",
    "        ctx.constant = constant\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.neg() * ctx.constant\n",
    "        return grad_output, None\n",
    "\n",
    "    def grad_reverse(x, constant):\n",
    "        return GradReverse.apply(x, constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self,kernel_size=3):\n",
    "        super(Network, self).__init__()\n",
    "        self.extractor = Extractor()\n",
    "        self.classifier = Class_classifier(num_class=2)\n",
    "        self.domain = Domain_classifier(domain_class=5)\n",
    "                \n",
    "    def forward(self, x, hp_lambda=0):\n",
    "        print(x.shape)\n",
    "        x = self.extractor(x)\n",
    "        print(x.shape)\n",
    "        cls = self.classifier(x)\n",
    "        print(x.shape)\n",
    "        dom = self.domain(x,hp_lambda)\n",
    "        print(x.shape)\n",
    "        \n",
    "        return cls,dom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function summary in module torchsummary.torchsummary:\n",
      "\n",
      "summary(model, input_size, batch_size=-1, device='cuda')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 246, 13])\n",
      "torch.Size([2, 1, 246, 13])\n",
      "torch.Size([2, 2880])\n",
      "torch.Size([2, 2880])\n",
      "torch.Size([2, 2880])\n",
      "domain torch.Size([2, 2880])\n",
      "torch.Size([2, 2880])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 16, 244, 12]             112\n",
      "       BatchNorm2d-2          [-1, 16, 244, 12]              32\n",
      "            Conv2d-3          [-1, 32, 242, 10]           4,640\n",
      "       BatchNorm2d-4          [-1, 32, 242, 10]              64\n",
      "         Dropout2d-5          [-1, 32, 242, 10]               0\n",
      "            Conv2d-6          [-1, 32, 244, 12]           9,248\n",
      "       BatchNorm2d-7          [-1, 32, 244, 12]              64\n",
      "         Dropout2d-8          [-1, 32, 244, 12]               0\n",
      "            Conv2d-9           [-1, 64, 122, 6]          18,496\n",
      "      BatchNorm2d-10           [-1, 64, 122, 6]             128\n",
      "        Dropout2d-11           [-1, 64, 122, 6]               0\n",
      "           Conv2d-12            [-1, 64, 61, 3]          36,928\n",
      "      BatchNorm2d-13            [-1, 64, 61, 3]             128\n",
      "        Dropout2d-14            [-1, 64, 61, 3]               0\n",
      "           Conv2d-15            [-1, 64, 30, 3]          61,504\n",
      "      BatchNorm2d-16            [-1, 64, 30, 3]             128\n",
      "        Dropout2d-17            [-1, 64, 30, 3]               0\n",
      "        Extractor-18                 [-1, 2880]               0\n",
      "           Linear-19                  [-1, 100]         288,100\n",
      "           Linear-20                    [-1, 2]             202\n",
      " Class_classifier-21                    [-1, 2]               0\n",
      "           Linear-22                  [-1, 100]         288,100\n",
      "           Linear-23                    [-1, 5]             505\n",
      "Domain_classifier-24                    [-1, 5]               0\n",
      "================================================================\n",
      "Total params: 708,379\n",
      "Trainable params: 708,379\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 6.13\n",
      "Params size (MB): 2.70\n",
      "Estimated Total Size (MB): 8.84\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhealthra2/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/mhealthra2/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "model = Network()\n",
    "model.cuda()\n",
    "summary(model,(1,246,13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Extractor, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(1, 16, kernel_size=(3,2),stride=1)   ## change with input shape\n",
    "        self.bn0 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        # Res block 1\n",
    "        self.conv1 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv11 = nn.Conv2d(32, 32, kernel_size=(3,3),stride=(1,1),padding=(2,2))\n",
    "        self.bn11 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Res block 2\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3,3), stride=(1,1),padding=(1,1))\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv21 = nn.Conv2d(64, 64, kernel_size=(3,3), stride=(2,2),padding=(1,1))\n",
    "        self.bn21 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=(5,3), stride=(2,1),padding=(1,1)) ### change with input shape\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.drop = nn.Dropout2d(0.5)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.bn0(self.conv0(x)))\n",
    "        \n",
    "        #Res block 1\n",
    "        x1 = self.drop(F.relu(self.bn1(self.conv1(x))))\n",
    "        x1 = F.relu(F.max_pool2d(self.drop(self.bn11(self.conv11(x1))), 2))\n",
    "        x = torch.cat((x,torch.zeros_like(x)), axis=1)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        x = x+x1\n",
    "        \n",
    "        #Res block 2\n",
    "        x1 = self.drop(F.relu(self.bn2(self.conv2(x))))\n",
    "        x1 = F.relu(self.drop(self.bn21(self.conv21(x1))))\n",
    "        x = torch.cat((x,torch.zeros_like(x)), axis=1)\n",
    "        x = F.max_pool2d(x,2)        \n",
    "        x = x+x1\n",
    "        \n",
    "        #last conv\n",
    "        x = self.drop(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = F.max_pool2d(x,(2,1))  ### change withinput\n",
    "        x = x.view(-1, 64*3*15)\n",
    "        print(x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 246, 13])\n",
      "torch.Size([2, 2880])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 16, 244, 12]             112\n",
      "       BatchNorm2d-2          [-1, 16, 244, 12]              32\n",
      "            Conv2d-3          [-1, 32, 242, 10]           4,640\n",
      "       BatchNorm2d-4          [-1, 32, 242, 10]              64\n",
      "         Dropout2d-5          [-1, 32, 242, 10]               0\n",
      "            Conv2d-6          [-1, 32, 244, 12]           9,248\n",
      "       BatchNorm2d-7          [-1, 32, 244, 12]              64\n",
      "         Dropout2d-8          [-1, 32, 244, 12]               0\n",
      "            Conv2d-9           [-1, 64, 122, 6]          18,496\n",
      "      BatchNorm2d-10           [-1, 64, 122, 6]             128\n",
      "        Dropout2d-11           [-1, 64, 122, 6]               0\n",
      "           Conv2d-12            [-1, 64, 61, 3]          36,928\n",
      "      BatchNorm2d-13            [-1, 64, 61, 3]             128\n",
      "        Dropout2d-14            [-1, 64, 61, 3]               0\n",
      "           Conv2d-15            [-1, 64, 30, 3]          61,504\n",
      "      BatchNorm2d-16            [-1, 64, 30, 3]             128\n",
      "        Dropout2d-17            [-1, 64, 30, 3]               0\n",
      "================================================================\n",
      "Total params: 131,472\n",
      "Trainable params: 131,472\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 6.10\n",
      "Params size (MB): 0.50\n",
      "Estimated Total Size (MB): 6.62\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = Extractor()\n",
    "net.cuda()\n",
    "summary(net,(1,246,13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "64*3*15*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Class_classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_class,in_feature=64*3*15,intermediate_nodes=100):\n",
    "        super(Class_classifier, self).__init__()\n",
    "        # self.fc1 = nn.Linear(50 * 4 * 4, 100)\n",
    "        # self.bn1 = nn.BatchNorm1d(100)\n",
    "        # self.fc2 = nn.Linear(100, 100)\n",
    "        # self.bn2 = nn.BatchNorm1d(100)\n",
    "        # self.fc3 = nn.Linear(100, 10)\n",
    "        self.fc1 = nn.Linear(in_feature, intermediate_nodes)\n",
    "        self.fc2 = nn.Linear(intermediate_nodes, num_class)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # logits = F.relu(self.bn1(self.fc1(input)))\n",
    "        # logits = self.fc2(F.dropout(logits))\n",
    "        # logits = F.relu(self.bn2(logits))\n",
    "        # logits = self.fc3(logits)\n",
    "        logits = self.relu(self.fc1(x))\n",
    "        logits = self.fc2(F.dropout(logits))\n",
    "        logits = self.soft(logits)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Domain_classifier(nn.Module):\n",
    "\n",
    "    def __init__(self,domain_class,in_feature=64*3*15,intermediate_nodes=100):\n",
    "        super(Domain_classifier, self).__init__()\n",
    "        # self.fc1 = nn.Linear(50 * 4 * 4, 100)\n",
    "        # self.bn1 = nn.BatchNorm1d(100)\n",
    "        # self.fc2 = nn.Linear(100, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_feature, intermediate_nodes)\n",
    "        self.fc2 = nn.Linear(intermediate_nodes, domain_class)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "    def forward(self, x, constant):\n",
    "        x = GradReverse.grad_reverse(x, constant)\n",
    "        # logits = F.relu(self.bn1(self.fc1(input)))\n",
    "        # logits = F.log_softmax(self.fc2(logits), 1)\n",
    "        print('domain',x.shape)\n",
    "        logits = self.relu(self.fc1(x))\n",
    "        logits = self.soft(self.fc2(logits))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Branch(nn.Module):\n",
    "\n",
    "    def __init__(self,c_in, c_out, kernel_size=5,stride=1,dropout = 0.5):\n",
    "        super(Branch, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(c_in, c_out, kernel_size=kernel_size,stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(c_out)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.conv2 = nn.Conv1d(c_out, 2*c_out, kernel_size=kernel_size,stride=stride)\n",
    "        self.bn2 = nn.BatchNorm1d(c_out*2)\n",
    "    def forward(self, x):\n",
    "        x = self.drop(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.drop(self.relu(self.bn2(self.conv2(x))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res_block(nn.Module):\n",
    "    def __init__(self,c_in,c_out,kernel_size=5,stride=1,dropout=0.5,padding=2):\n",
    "        super(Res_block,self).__init__()\n",
    "        self.conv1 = nn.Conv1d(c_in,c_out,kernel_size=kernel_size,stride=stride,padding=padding)\n",
    "        self.bn1 = nn.BatchNorm1d(c_out)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.conv2 = nn.Conv1d(c_out, c_out, kernel_size=kernel_size,stride=1,padding=padding)\n",
    "        self.bn2 = nn.BatchNorm1d(c_out)\n",
    "        self.pool = nn.MaxPool1d(stride)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x1 = self.drop(self.relu(self.bn1(self.conv1(x))))\n",
    "        x1 = self.drop(self.relu(self.bn2(self.conv2(x1))))\n",
    "        x = self.pool(x)\n",
    "        x = torch.cat((x,torch.zeros_like(x)), axis=1)\n",
    "        x = x+x1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Smallnet(nn.Module):\n",
    "\n",
    "    def __init__(self,num_class,domain_class):\n",
    "        super(Smallnet, self).__init__()\n",
    "        self.branch1 = Branch(1,8)\n",
    "        self.res_block1 = Res_block(16,32,stride=2)\n",
    "        self.res_block2 = Res_block(32,64,stride=2)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.conv = nn.Conv1d(64,128,kernel_size=5,stride=2)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        \n",
    "        self.classifier = Class_classifier(num_class=num_class,in_feature=9856,intermediate_nodes=50)\n",
    "        self.domain = Domain_classifier(domain_class=domain_class,in_feature=9856,intermediate_nodes=80)\n",
    "    def forward(self, x, hp_lambda=0):\n",
    "        print(x.shape)\n",
    "        x = self.branch1(x)\n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.drop(self.pool(self.conv(self.pool(x))))\n",
    "        x = x.view(-1,9856)\n",
    "        print(x.shape)\n",
    "        cls = self.classifier(x)\n",
    "        dom = self.domain(x,hp_lambda)\n",
    "        return cls,dom\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Smallnet(2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 2500])\n",
      "torch.Size([2, 9856])\n",
      "domain torch.Size([2, 9856])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 8, 2496]              48\n",
      "       BatchNorm1d-2              [-1, 8, 2496]              16\n",
      "              ReLU-3              [-1, 8, 2496]               0\n",
      "           Dropout-4              [-1, 8, 2496]               0\n",
      "            Conv1d-5             [-1, 16, 2492]             656\n",
      "       BatchNorm1d-6             [-1, 16, 2492]              32\n",
      "              ReLU-7             [-1, 16, 2492]               0\n",
      "           Dropout-8             [-1, 16, 2492]               0\n",
      "            Branch-9             [-1, 16, 2492]               0\n",
      "           Conv1d-10             [-1, 32, 1246]           2,592\n",
      "      BatchNorm1d-11             [-1, 32, 1246]              64\n",
      "             ReLU-12             [-1, 32, 1246]               0\n",
      "          Dropout-13             [-1, 32, 1246]               0\n",
      "           Conv1d-14             [-1, 32, 1246]           5,152\n",
      "      BatchNorm1d-15             [-1, 32, 1246]              64\n",
      "             ReLU-16             [-1, 32, 1246]               0\n",
      "          Dropout-17             [-1, 32, 1246]               0\n",
      "        MaxPool1d-18             [-1, 16, 1246]               0\n",
      "        Res_block-19             [-1, 32, 1246]               0\n",
      "           Conv1d-20              [-1, 64, 623]          10,304\n",
      "      BatchNorm1d-21              [-1, 64, 623]             128\n",
      "             ReLU-22              [-1, 64, 623]               0\n",
      "          Dropout-23              [-1, 64, 623]               0\n",
      "           Conv1d-24              [-1, 64, 623]          20,544\n",
      "      BatchNorm1d-25              [-1, 64, 623]             128\n",
      "             ReLU-26              [-1, 64, 623]               0\n",
      "          Dropout-27              [-1, 64, 623]               0\n",
      "        MaxPool1d-28              [-1, 32, 623]               0\n",
      "        Res_block-29              [-1, 64, 623]               0\n",
      "        MaxPool1d-30              [-1, 64, 311]               0\n",
      "           Conv1d-31             [-1, 128, 154]          41,088\n",
      "        MaxPool1d-32              [-1, 128, 77]               0\n",
      "          Dropout-33              [-1, 128, 77]               0\n",
      "           Linear-34                   [-1, 50]         492,850\n",
      "             ReLU-35                   [-1, 50]               0\n",
      "           Linear-36                    [-1, 2]             102\n",
      "          Softmax-37                    [-1, 2]               0\n",
      " Class_classifier-38                    [-1, 2]               0\n",
      "           Linear-39                   [-1, 80]         788,560\n",
      "             ReLU-40                   [-1, 80]               0\n",
      "           Linear-41                    [-1, 5]             405\n",
      "          Softmax-42                    [-1, 5]               0\n",
      "Domain_classifier-43                    [-1, 5]               0\n",
      "================================================================\n",
      "Total params: 1,362,733\n",
      "Trainable params: 1,362,733\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 8.36\n",
      "Params size (MB): 5.20\n",
      "Estimated Total Size (MB): 13.57\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(n.cuda(),(1,2500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
