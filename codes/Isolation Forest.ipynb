{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py,numpy as np,os\n",
    "path = '../data/fold0_noFIR.mat'\n",
    "data = h5py.File(path, 'r')\n",
    "\n",
    "x_train = data['trainX'][:].astype('float32')\n",
    "x_train = np.expand_dims(x_train.transpose(),1)\n",
    "\n",
    "x_val = data['valX'][:].astype('float32')\n",
    "x_val = np.expand_dims(x_val.transpose(),1)\n",
    "\n",
    "y_train = data['trainY'][:].astype('int32')\n",
    "y_train = y_train.transpose()\n",
    "y_train = y_train[:,0]\n",
    "y_train[y_train<0] = 0\n",
    "\n",
    "y_val = data['valY'][:].astype('int32')\n",
    "y_val = y_val.transpose()\n",
    "y_val = y_val[:,0]\n",
    "y_val[y_val<0] = 0\n",
    "\n",
    "val_parts = data['val_parts'][:].astype('int32').squeeze(0)\n",
    "domain = np.array([x-97 for x in data['val_files'][:][0]])\n",
    "train_domain = np.array([x-97 for x in data['train_files'][:][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from torchvision import datasets, transforms\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "from HeartCepTorch import MFCC_Gen,Network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init\n",
    "from torchsummary import summary\n",
    "\n",
    "# from utils import log_macc, results_log\n",
    "from dataLoader import reshape_folds\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import Evaluator\n",
    "import dataLoader\n",
    "# from custom_layers import Attention\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import seaborn as sns\n",
    "from collections import Counter\n",
    "def to_numpy(x):\n",
    "    return x.cpu().detach().numpy()\n",
    "def plotf(x):\n",
    "    plt.plot(to_numpy(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Network(2,0)\n",
    "mfcc_gen = MFCC_Gen(fs=1000,filters=64,momentum=0.1)\n",
    "model.cuda()\n",
    "mfcc_gen.cuda()\n",
    "class_criterion = nn.CrossEntropyLoss()\n",
    "domain_criterion = nn.CrossEntropyLoss()\n",
    "# model.load_state_dict(torch.load('../models/weights.0580-acc_0.7824-macc_0.8294.pt'))\n",
    "model_path = '/media/mhealthra2/Data/heart_sound/Heartnet_Results/logs/gammatone_torch_layer/fold0_noFIR batch1500 continued_part4 .005_2020-04-17 03.18.55.957507/weights/'   \n",
    "model.load_state_dict(torch.load(model_path+'/weights.0150-acc_0.7990-macc_0.8562.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HS_dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx],self.y[idx]\n",
    "dataset = HS_dataset(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for validation set \n",
    "dataset = HS_dataset(x_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=500, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, collate_fn=None,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch_loss = 0\n",
    "acc = 0\n",
    "N = 0\n",
    "y_true = None\n",
    "feature = None\n",
    "y_pred = None\n",
    "model.eval()\n",
    "mfcc_gen.eval()\n",
    "with torch.no_grad():\n",
    "    for (x,y) in iter(loader):\n",
    "#         x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
    "        x = x.type(torch.FloatTensor).cuda()\n",
    "        x = mfcc_gen(x)\n",
    "        x = x.transpose(2,1)\n",
    "        x = x.unsqueeze(1)\n",
    "        x,y = Variable(x),Variable(y)\n",
    "        y = y.long().cuda()\n",
    "        cls= model(x).float()\n",
    "        feat = model.extractor(x)\n",
    "        \n",
    "        if(feature is None):\n",
    "            feature = to_numpy(feat)\n",
    "        else:\n",
    "            feature = np.concatenate((feature,to_numpy(feat)))\n",
    "        if(y_pred is None):\n",
    "            y_true = to_numpy(y)\n",
    "        else:\n",
    "            y_true = np.concatenate((y_true,to_numpy(y)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79810, 7168), torch.Size([310, 7168]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature.shape, feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_feature = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [100, 290, 480, 670, 860, 1050, 1240, 1430, 1620, 1810, 2000], 'max_features': [0.7, 0.8, 1], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 11)]\n",
    "\n",
    "max_features = [0.7,0.8,1]\n",
    "\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 2)]\n",
    "max_depth.append(None)\n",
    "\n",
    "min_samples_split = [2, 5]\n",
    "\n",
    "min_samples_leaf = [1, 2]\n",
    "\n",
    "bootstrap = [True, False]\n",
    "max_samples = [0.1, 0.2]\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               #'max_depth': max_depth,\n",
    "               #'min_samples_split': min_samples_split,\n",
    "               #'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = IsolationForest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class IsolationForest in module sklearn.ensemble._iforest:\n",
      "\n",
      "class IsolationForest(sklearn.base.OutlierMixin, sklearn.ensemble._bagging.BaseBagging)\n",
      " |  IsolationForest(n_estimators=100, max_samples='auto', contamination='auto', max_features=1.0, bootstrap=False, n_jobs=None, behaviour='deprecated', random_state=None, verbose=0, warm_start=False)\n",
      " |  \n",
      " |  Isolation Forest Algorithm.\n",
      " |  \n",
      " |  Return the anomaly score of each sample using the IsolationForest algorithm\n",
      " |  \n",
      " |  The IsolationForest 'isolates' observations by randomly selecting a feature\n",
      " |  and then randomly selecting a split value between the maximum and minimum\n",
      " |  values of the selected feature.\n",
      " |  \n",
      " |  Since recursive partitioning can be represented by a tree structure, the\n",
      " |  number of splittings required to isolate a sample is equivalent to the path\n",
      " |  length from the root node to the terminating node.\n",
      " |  \n",
      " |  This path length, averaged over a forest of such random trees, is a\n",
      " |  measure of normality and our decision function.\n",
      " |  \n",
      " |  Random partitioning produces noticeably shorter paths for anomalies.\n",
      " |  Hence, when a forest of random trees collectively produce shorter path\n",
      " |  lengths for particular samples, they are highly likely to be anomalies.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <isolation_forest>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.18\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : int, optional (default=100)\n",
      " |      The number of base estimators in the ensemble.\n",
      " |  \n",
      " |  max_samples : int or float, optional (default=\"auto\")\n",
      " |      The number of samples to draw from X to train each base estimator.\n",
      " |          - If int, then draw `max_samples` samples.\n",
      " |          - If float, then draw `max_samples * X.shape[0]` samples.\n",
      " |          - If \"auto\", then `max_samples=min(256, n_samples)`.\n",
      " |  \n",
      " |      If max_samples is larger than the number of samples provided,\n",
      " |      all samples will be used for all trees (no sampling).\n",
      " |  \n",
      " |  contamination : 'auto' or float, optional (default='auto')\n",
      " |      The amount of contamination of the data set, i.e. the proportion\n",
      " |      of outliers in the data set. Used when fitting to define the threshold\n",
      " |      on the scores of the samples.\n",
      " |  \n",
      " |          - If 'auto', the threshold is determined as in the\n",
      " |            original paper.\n",
      " |          - If float, the contamination should be in the range [0, 0.5].\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default value of ``contamination`` changed from 0.1\n",
      " |         to ``'auto'``.\n",
      " |  \n",
      " |  max_features : int or float, optional (default=1.0)\n",
      " |      The number of features to draw from X to train each base estimator.\n",
      " |  \n",
      " |          - If int, then draw `max_features` features.\n",
      " |          - If float, then draw `max_features * X.shape[1]` features.\n",
      " |  \n",
      " |  bootstrap : bool, optional (default=False)\n",
      " |      If True, individual trees are fit on random subsets of the training\n",
      " |      data sampled with replacement. If False, sampling without replacement\n",
      " |      is performed.\n",
      " |  \n",
      " |  n_jobs : int or None, optional (default=None)\n",
      " |      The number of jobs to run in parallel for both :meth:`fit` and\n",
      " |      :meth:`predict`. ``None`` means 1 unless in a\n",
      " |      :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
      " |      processors. See :term:`Glossary <n_jobs>` for more details.\n",
      " |  \n",
      " |  behaviour : str, default='deprecated'\n",
      " |      This parameter has not effect, is deprecated, and will be removed.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |         ``behaviour`` is added in 0.20 for back-compatibility purpose.\n",
      " |  \n",
      " |      .. deprecated:: 0.20\n",
      " |         ``behaviour='old'`` is deprecated in 0.20 and will not be possible\n",
      " |         in 0.22.\n",
      " |  \n",
      " |      .. deprecated:: 0.22\n",
      " |         ``behaviour`` parameter is deprecated in 0.22 and removed in\n",
      " |         0.24.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  verbose : int, optional (default=0)\n",
      " |      Controls the verbosity of the tree building process.\n",
      " |  \n",
      " |  warm_start : bool, optional (default=False)\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      .. versionadded:: 0.21\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimators_ : list of DecisionTreeClassifier\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  estimators_samples_ : list of arrays\n",
      " |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      " |      estimator.\n",
      " |  \n",
      " |  max_samples_ : integer\n",
      " |      The actual number of samples\n",
      " |  \n",
      " |  offset_ : float\n",
      " |      Offset used to define the decision function from the raw scores. We\n",
      " |      have the relation: ``decision_function = score_samples - offset_``.\n",
      " |      ``offset_`` is defined as follows. When the contamination parameter is\n",
      " |      set to \"auto\", the offset is equal to -0.5 as the scores of inliers are\n",
      " |      close to 0 and the scores of outliers are close to -1. When a\n",
      " |      contamination parameter different than \"auto\" is provided, the offset\n",
      " |      is defined in such a way we obtain the expected number of outliers\n",
      " |      (samples with decision function < 0) in training.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The implementation is based on an ensemble of ExtraTreeRegressor. The\n",
      " |  maximum depth of each tree is set to ``ceil(log_2(n))`` where\n",
      " |  :math:`n` is the number of samples used to build the tree\n",
      " |  (see (Liu et al., 2008) for more details).\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"\n",
      " |         Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n",
      " |  .. [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation-based\n",
      " |         anomaly detection.\" ACM Transactions on Knowledge Discovery from\n",
      " |         Data (TKDD) 6.1 (2012): 3.\n",
      " |  \n",
      " |  See Also\n",
      " |  ----------\n",
      " |  sklearn.covariance.EllipticEnvelope : An object for detecting outliers in a\n",
      " |      Gaussian distributed dataset.\n",
      " |  sklearn.svm.OneClassSVM : Unsupervised Outlier Detection.\n",
      " |      Estimate the support of a high-dimensional distribution.\n",
      " |      The implementation is based on libsvm.\n",
      " |  sklearn.neighbors.LocalOutlierFactor : Unsupervised Outlier Detection\n",
      " |      using Local Outlier Factor (LOF).\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import IsolationForest\n",
      " |  >>> X = [[-1.1], [0.3], [0.5], [100]]\n",
      " |  >>> clf = IsolationForest(random_state=0).fit(X)\n",
      " |  >>> clf.predict([[0.1], [0], [90]])\n",
      " |  array([ 1,  1, -1])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      IsolationForest\n",
      " |      sklearn.base.OutlierMixin\n",
      " |      sklearn.ensemble._bagging.BaseBagging\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=100, max_samples='auto', contamination='auto', max_features=1.0, bootstrap=False, n_jobs=None, behaviour='deprecated', random_state=None, verbose=0, warm_start=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Average anomaly score of X of the base classifiers.\n",
      " |      \n",
      " |      The anomaly score of an input sample is computed as\n",
      " |      the mean anomaly score of the trees in the forest.\n",
      " |      \n",
      " |      The measure of normality of an observation given a tree is the depth\n",
      " |      of the leaf containing this observation, which is equivalent to\n",
      " |      the number of splittings required to isolate this point. In case of\n",
      " |      several observations n_left in the leaf, the average path length of\n",
      " |      a n_left samples isolation tree is added.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      scores : array, shape (n_samples,)\n",
      " |          The anomaly score of the input samples.\n",
      " |          The lower, the more abnormal. Negative scores represent outliers,\n",
      " |          positive scores represent inliers.\n",
      " |  \n",
      " |  fit(self, X, y=None, sample_weight=None)\n",
      " |      Fit estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          The input samples. Use ``dtype=np.float32`` for maximum\n",
      " |          efficiency. Sparse matrices are also supported, use sparse\n",
      " |          ``csc_matrix`` for maximum efficiency.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present for API consistency by convention.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict if a particular sample is an outlier or not.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      is_inlier : array, shape (n_samples,)\n",
      " |          For each observation, tells whether or not (+1 or -1) it should\n",
      " |          be considered as an inlier according to the fitted model.\n",
      " |  \n",
      " |  score_samples(self, X)\n",
      " |      Opposite of the anomaly score defined in the original paper.\n",
      " |      \n",
      " |      The anomaly score of an input sample is computed as\n",
      " |      the mean anomaly score of the trees in the forest.\n",
      " |      \n",
      " |      The measure of normality of an observation given a tree is the depth\n",
      " |      of the leaf containing this observation, which is equivalent to\n",
      " |      the number of splittings required to isolate this point. In case of\n",
      " |      several observations n_left in the leaf, the average path length of\n",
      " |      a n_left samples isolation tree is added.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          The input samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      scores : array, shape (n_samples,)\n",
      " |          The anomaly score of the input samples.\n",
      " |          The lower, the more abnormal.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.OutlierMixin:\n",
      " |  \n",
      " |  fit_predict(self, X, y=None)\n",
      " |      Perform fit on X and returns labels for X.\n",
      " |      \n",
      " |      Returns -1 for outliers and 1 for inliers.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : ndarray, shape (n_samples, n_features)\n",
      " |          Input data.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present for API consistency by convention.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray, shape (n_samples,)\n",
      " |          1 for inliers, -1 for outliers.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.OutlierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.ensemble._bagging.BaseBagging:\n",
      " |  \n",
      " |  estimators_samples_\n",
      " |      The subset of drawn samples for each base estimator.\n",
      " |      \n",
      " |      Returns a dynamically generated list of indices identifying\n",
      " |      the samples used for fitting each member of the ensemble, i.e.,\n",
      " |      the in-bag samples.\n",
      " |      \n",
      " |      Note: the list is re-created at each call to the property in order\n",
      " |      to reduce the object memory footprint by not storing the sampling\n",
      " |      data. Thus fetching the property may be slower than expected.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(IsolationForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 2,\n",
    "                               cv = 2, verbose=1, random_state=42, n_jobs = -1\n",
    "                              , scoring = {'Accuracy': make_scorer(accuracy_score)},\n",
    "                              refit='Accuracy',return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed: 204.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed: 204.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=2, error_score=nan,\n",
       "                   estimator=IsolationForest(behaviour='deprecated',\n",
       "                                             bootstrap=False,\n",
       "                                             contamination='auto',\n",
       "                                             max_features=1.0,\n",
       "                                             max_samples='auto',\n",
       "                                             n_estimators=100, n_jobs=None,\n",
       "                                             random_state=None, verbose=0,\n",
       "                                             warm_start=False),\n",
       "                   iid='deprecated', n_iter=2, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_features': [0.7, 0.8, 1],\n",
       "                                        'n_estimators': [100, 290, 480, 670,\n",
       "                                                         860, 1050, 1240, 1430,\n",
       "                                                         1620, 1810, 2000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit='Accuracy',\n",
       "                   return_train_score=True,\n",
       "                   scoring={'Accuracy': make_scorer(accuracy_score)},\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.fit(train_feature,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "rff = rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "rff = IsolationForest(n_estimators=1500,max_features=0.7,n_jobs=-1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed: 37.7min remaining: 113.0min\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed: 37.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IsolationForest(behaviour='deprecated', bootstrap=False, contamination='auto',\n",
       "                max_features=0.7, max_samples='auto', n_estimators=1500,\n",
       "                n_jobs=-1, random_state=None, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rff.fit(train_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6710,) (6710,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56.944858420268254"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_val = rff.predict(val_feature)\n",
    "print(pred_val.shape, y_val.shape)\n",
    "acc = 100*np.sum(pred_val==y_val)/len(y_val)\n",
    "max(acc,100-acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79810,) (79810,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "82.1087583009648"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train = rff.predict(train_feature)\n",
    "print(pred_train.shape, y_train.shape)\n",
    "acc = 100*np.sum(pred_train==y_train)/len(y_train)\n",
    "max(acc,100-acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2890"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pred_val[:]==y_val[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79810,) (79810,)\n"
     ]
    }
   ],
   "source": [
    "print(pred_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "# Number of features to consider at every split\n",
    "# Maximum number of levels in tree\n",
    "# Minimum number of samples required to split a node\n",
    "# Minimum number of samples required at each leaf node\n",
    "# Method of selecting samples for training each tree\n",
    "# Create the random grid\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
